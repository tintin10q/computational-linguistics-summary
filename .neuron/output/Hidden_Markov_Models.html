<!DOCTYPE html><html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type" /><meta content="width=device-width, initial-scale=1" name="viewport" /><!--replace-start-0--><!--replace-start-5--><!--replace-start-8--><title>Hidden Markov Models - Neuron Zettelkasten</title><!--replace-end-8--><!--replace-end-5--><!--replace-end-0--><link href="https://cdn.jsdelivr.net/npm/fomantic-ui@2.8.7/dist/semantic.min.css" rel="stylesheet" /><link href="https://fonts.googleapis.com/css?family=Merriweather|Libre+Franklin|Roboto+Mono&amp;display=swap" rel="stylesheet" /><!--replace-start-1--><!--replace-start-4--><!--replace-start-7--><link href="https://raw.githubusercontent.com/srid/neuron/master/assets/neuron.svg" rel="icon" /><meta content="This is like a markov model but we also care about hidden states. In PoS taggging we don’t observe the states we want to predict as we do in Language Modeling. Basically the PoS tags are hidden. We observe a sequence of words and want to find the best sequence of tags for that particular sequence of" name="description" /><link href="https://neuron.zettel.page/Hidden_Markov_Models.html" rel="canonical" /><meta content="Hidden Markov Models" property="og:title" /><meta content="Neuron Zettelkasten" property="og:site_name" /><meta content="article" property="og:type" /><meta content="Hidden Markov Models" property="neuron:zettel-id" /><meta content="Hidden_Markov_Models" property="neuron:zettel-slug" /><meta content="root/Prediction" property="neuron:zettel-tag" /><script type="application/ld+json">[{"@context":"https://schema.org","itemListElement":[{"name":"Prediction","item":"https://neuron.zettel.page/Prediction.html","@type":"ListItem","position":1}],"@type":"BreadcrumbList"}]</script><style type="text/css">body{background-color:#eeeeee !important;font-family:"Libre Franklin", serif !important}body .ui.container{font-family:"Libre Franklin", serif !important}body h1, h2, h3, h4, h5, h6, .ui.header, .headerFont{font-family:"Merriweather", sans-serif !important}body code, pre, tt, .monoFont{font-family:"Roboto Mono","SFMono-Regular","Menlo","Monaco","Consolas","Liberation Mono","Courier New", monospace !important}body div.z-index p.info{color:#808080}body div.z-index ul{list-style-type:square;padding-left:1.5em}body div.z-index .uplinks{margin-left:0.29999em}body .zettel-content h1#title-h1{background-color:rgba(33,133,208,0.1)}body nav.bottomPane{background-color:rgba(33,133,208,2.0e-2)}body div#footnotes{border-top-color:#2185d0}body p{line-height:150%}body img{max-width:100%}body .deemphasized{font-size:0.94999em}body .deemphasized:hover{opacity:1}body .deemphasized:not(:hover){opacity:0.69999}body .deemphasized:not(:hover) a{color:#808080 !important}body div.container.universe{padding-top:1em}body div.zettel-view ul{padding-left:1.5em;list-style-type:square}body div.zettel-view .pandoc .highlight{background-color:#ffff00}body div.zettel-view .pandoc .ui.disabled.fitted.checkbox{margin-right:0.29999em;vertical-align:middle}body div.zettel-view .zettel-content .metadata{margin-top:1em}body div.zettel-view .zettel-content .metadata div.date{text-align:center;color:#808080}body div.zettel-view .zettel-content h1{padding-top:0.2em;padding-bottom:0.2em;text-align:center}body div.zettel-view .zettel-content h2{border-bottom:solid 1px #4682b4;margin-bottom:0.5em}body div.zettel-view .zettel-content h3{margin:0px 0px 0.4em 0px}body div.zettel-view .zettel-content h4{opacity:0.8}body div.zettel-view .zettel-content div#footnotes{margin-top:4em;border-top-style:groove;border-top-width:2px;font-size:0.9em}body div.zettel-view .zettel-content div#footnotes ol > li > p:only-of-type{display:inline;margin-right:0.5em}body div.zettel-view .zettel-content aside.footnote-inline{width:30%;padding-left:15px;margin-left:15px;float:right;background-color:#d3d3d3}body div.zettel-view .zettel-content .overflows{overflow:auto}body div.zettel-view .zettel-content code{margin:auto auto auto auto;font-size:100%}body div.zettel-view .zettel-content p code, li code, ol code{padding:0.2em 0.2em 0.2em 0.2em;background-color:#f5f2f0}body div.zettel-view .zettel-content pre{overflow:auto}body div.zettel-view .zettel-content dl dt{font-weight:bold}body div.zettel-view .zettel-content blockquote{background-color:#f9f9f9;border-left:solid 10px #cccccc;margin:1.5em 0px 1.5em 0px;padding:0.5em 10px 0.5em 10px}body div.zettel-view .zettel-content.raw{background-color:#dddddd}body .ui.label.zettel-tag{color:#000000}body .ui.label.zettel-tag a{color:#000000}body nav.bottomPane ul.backlinks > li{padding-bottom:0.4em;list-style-type:disc}body nav.bottomPane ul.context-list > li{list-style-type:lower-roman}body .footer-version img{-webkit-filter:grayscale(100%);-moz-filter:grayscale(100%);-ms-filter:grayscale(100%);-o-filter:grayscale(100%);filter:grayscale(100%)}body .footer-version img:hover{-webkit-filter:grayscale(0%);-moz-filter:grayscale(0%);-ms-filter:grayscale(0%);-o-filter:grayscale(0%);filter:grayscale(0%)}body .footer-version, .footer-version a, .footer-version a:visited{color:#808080}body .footer-version a{font-weight:bold}body .footer-version{margin-top:1em !important;font-size:0.69999em}@media only screen and (max-width: 768px){body div#zettel-container{margin-left:0.4em !important;margin-right:0.4em !important}}body span.zettel-link-container span.zettel-link a{color:#2185d0;font-weight:bold;text-decoration:none}body span.zettel-link-container span.zettel-link a:hover{background-color:rgba(33,133,208,0.1)}body span.zettel-link-container span.extra{color:auto}body span.zettel-link-container.errors{border:solid 1px #ff0000}body span.zettel-link-container.errors span.zettel-link a:hover{text-decoration:none !important;cursor:not-allowed}body [data-tooltip]:after{font-size:0.69999em}body div.tag-tree div.node{font-weight:bold}body div.tag-tree div.node a.inactive{color:#555555}body .tree.flipped{-webkit-transform:rotate(180deg);-moz-transform:rotate(180deg);-ms-transform:rotate(180deg);-o-transform:rotate(180deg);transform:rotate(180deg)}body .tree{overflow:auto}body .tree ul.root{padding-top:0px;margin-top:0px}body .tree ul{position:relative;padding:1em 0px 0px 0px;white-space:nowrap;margin:0px auto 0px auto;text-align:center}body .tree ul::after{content:"";display:table;clear:both}body .tree ul:last-child{padding-bottom:0.1em}body .tree li{display:inline-block;vertical-align:top;text-align:center;list-style-type:none;position:relative;padding:1em 0.5em 0em 0.5em}body .tree li::before{content:"";position:absolute;top:0px;right:50%;border-top:solid 2px #cccccc;width:50%;height:1.19999em}body .tree li::after{content:"";position:absolute;top:0px;right:50%;border-top:solid 2px #cccccc;width:50%;height:1.19999em}body .tree li::after{right:auto;left:50%;border-left:solid 2px #cccccc}body .tree li:only-child{padding-top:0em}body .tree li:only-child::after{display:none}body .tree li:only-child::before{display:none}body .tree li:first-child::before{border-style:none;border-width:0px}body .tree li:first-child::after{border-radius:5px 0px 0px 0px}body .tree li:last-child::after{border-style:none;border-width:0px}body .tree li:last-child::before{border-right:solid 2px #cccccc;border-radius:0px 5px 0px 0px}body .tree ul ul::before{content:"";position:absolute;top:0px;left:50%;border-left:solid 2px #cccccc;width:0px;height:1.19999em}body .tree li div.forest-link{border:solid 2px #cccccc;padding:0.2em 0.29999em 0.2em 0.29999em;text-decoration:none;display:inline-block;border-radius:5px 5px 5px 5px;color:#333333;position:relative;top:2px}body .tree.flipped li div.forest-link{-webkit-transform:rotate(180deg);-moz-transform:rotate(180deg);-ms-transform:rotate(180deg);-o-transform:rotate(180deg);transform:rotate(180deg)}</style><script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/themes/prism.min.css" rel="stylesheet" /><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/components/prism-core.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/plugins/autoloader/prism-autoloader.min.js"></script><!--replace-end-7--><!--replace-end-4--><!--replace-end-1--></head><body><div class="ui fluid container universe"><!--replace-start-2--><!--replace-start-3--><!--replace-start-6--><nav class="flipped tree deemphasized" id="zettel-uptree" style="transform-origin: 50%"><ul class="root"><li><ul><li><div class="forest-link"><span class="zettel-link-container"><span class="zettel-link"><a href="Prediction.html">Prediction</a></span></span></div></li></ul></li></ul></nav><div class="ui text container" id="zettel-container" style="position: relative"><div class="zettel-view"><article class="ui raised attached segment zettel-content"><div class="pandoc"><h1 id="title-h1">Hidden Markov Models</h1><p>This is like a <a href="Markov%20models.md">markov model</a> but we also care about hidden states. In PoS taggging we don’t observe the states we want to predict as we do in <a href="Language%20Modeling.md">Language Modeling</a>. Basically the <a href="Parts%20of%20Speech.md">PoS</a> tags are hidden. We observe a sequence of words and want to find the best sequence of tags for that particular sequence of words out of all possible sequence of tags.</p><p>A Hidden Markov Model (HMM) consists of the following components:</p><ul><li>Q - A finite set of N states.</li><li>A - A state transition probability matrix.</li><li><span class="math inline">\(\pi\)</span> - An initial probability distrobution.</li><li>O - A sequence of T observations.</li><li>B - An overservation likelihood matrix. Probability for a specific observation.</li></ul><p>The last 2 of these are what is different from <a href="Markov%20models.md">Markov models</a>. But the difference is that we use Q, A and <span class="math inline">\(\pi\)</span> for the hidden states. So the states in Q are not visible to anymore. With O and B we encode what we know or what is visible. We do know that Q contains BoS and EoS.</p><h2 id="components">Components</h2><h3 id="q">Q</h3><p>Rather than consisting of words or engram, Q consits of PoS tags. So, Q contains the states of the HMM. This also includes BoS and EoS.</p><h3 id="a">A</h3><p>A is a |Q|-by-|Q| matrix, where each cell <span class="math inline">\(A_{ij}\)</span> inidicates the probability of moving from state <span class="math inline">\(i \in Q\)</span> to state <span class="math inline">\(j \in Q\)</span>. This means that we need some corpus with annotated data where we can observe PoS tags.</p><p>We can do the estimation using maximum likelyhood estimages. The formula of that is: <span class="math display">$$p(t_{i}, t_{i-n:i-1}) = \frac{c(t_{i-n:i-1}, t_{i})}{c(t_{i-n:i-1})}$$</span> c is a count function. So you devide the cocurancy frequency of the current tags by the frequency of the preceding tags. That gives you the probabilaty the tag given the preceding tags.</p><h3 id="pi"><span class="math inline">\(\pi\)</span></h3><p>Similarly to the <a href="Markov%20Models.md">Markov Chain</a> <span class="math inline">\(\pi\)</span> encodes the probability that each state <span class="math inline">\(q\in Q\)</span> follows the BoS symbol. <span class="math inline">\(\pi\)</span> can be fixed in advance if you want to put constraints on what can come at the start, or you can estimate it from a corpus. So instead of words is about tags.</p><h3 id="o">O</h3><p>The set of observed events: In PoS tagging, O contains words. This set has to be finite otherwise we can not finish. Every observation in O has to be able to be tagged from a state in Q.</p><h3 id="b">B</h3><p>B is another matrix. It is of size |Q|-by-|O|, where each cell <span class="math inline">\(b_{qo}\)</span> indicates the probability that a word <span class="math inline">\(o \in O\)</span> is generated by a state <span class="math inline">\(q \in Q\)</span>.</p><p>To compute B, we need to find out how often each word occurs tagged with a particular PoS tag in a corpus. Again this needs annotated data.</p><p>So B encodes the probability that a certain word occurs since we observed a certain tag. Given that we observed a noun, how likely is it that this noun is exaclty dog for instance and not aardvark.</p><p>This is also calculated with ML: <span class="math display">$$p(w_{i}|t_{i}) = \frac{c(t_{i},w_{i})}{c(t_{i})}$$</span></p><p>This devides the number of times a specific word is tagged as a certain tag.</p><p>So we want to know the likeliest tag for a word, but we compute the likeliest word after observing a tag. This is like <a href="Bayes%20rule.md">Bayes rule</a>. We aim for the posterior but compute the likelihood and the prior, and then estimate the posterior.</p><h2 id="assumptions">Assumptions</h2><p>Both of these assumptions simplify the problem to make it possible to compute.</p><h3 id="markov-assumption">Markov Assumption</h3><p>The probility of the next tag is only determined by the local history, not the whole sequence.</p><h3 id="output-independence">Output independence</h3><p>The probability of a word only depends on the state that produced the corresponding tag, not on any other state or word.</p></div></article><nav class="ui attached segment deemphasized backlinksPane" id="neuron-backlinks-pane"><h3 class="ui header">Backlinks</h3><ul class="backlinks"><li><span class="zettel-link-container cf"><span class="zettel-link"><a href="Decoding.html">Decoding</a></span></span><ul class="context-list" style="zoom: 85%;"><li class="item"><div class="pandoc"><p>In the case of this course decoding has to do with <span class="zettel-link-container cf"><span class="zettel-link" title="Zettel: Hidden Markov Models"><a href="Hidden_Markov_Models.html">Hidden Markov Models.md</a></span></span>. It is the task of <strong>determining the sequence of hidden variables given a sequence of observed events.</strong></p></div></li><li class="item"><div class="pandoc"><p>So the apoximated tag sequence is the sequence of words with the highest posterior probabilty. If you actually calculate this with <span class="zettel-link-container cf"><span class="zettel-link" title="Zettel: Bayes Rule"><a href="Bayes_rule.html">Bayes rule.md</a></span></span> you get <span class="math inline">\(\hat{t}_{1:n} =\text{argmax}_{t_{1:n}} p(t_{1:n}) \cdot p(W_{1:n})\)</span>. We plug in the formulas to estimage both terms under the <span class="zettel-link-container cf"><span class="zettel-link" title="Zettel: Hidden Markov Models"><a href="Hidden_Markov_Models.html">Markov assumption and the output indepedence assumption</a></span></span>. So for a whole sequence:</p></div></li></ul></li></ul><h3 class="ui header"><span title="Backlinks from folgezettel parents">Uplinks</span></h3><ul class="backlinks"><li><span class="zettel-link-container folge"><span class="zettel-link"><a href="Prediction.html">Prediction</a><span data-nosnippet="" style="user-select: none; color: gray" title="Folgezettel">#</span></span></span><ul class="context-list" style="zoom: 85%;"><li class="item"><div class="pandoc"><em>Parent directory zettel</em></div></li></ul></li></ul></nav><nav class="ui attached segment deemphasized bottomPane" id="neuron-tags-pane"><div><span class="ui basic label zettel-tag" title="Tag">root/Prediction</span></div></nav><nav class="ui bottom attached icon compact inverted menu blue" id="neuron-nav-bar"><!--replace-start-9--><!--replace-end-9--><a class="item" href="https://github.com/srid/neuron/edit/master/doc/./Prediction/Hidden Markov Models.md" title="Edit this page"><i class="edit icon"></i></a><a class="right item" href="impulse.html" title="Open Impulse"><i class="wave square icon"></i></a></nav></div></div><!--replace-end-6--><!--replace-end-3--><!--replace-end-2--><div class="ui center aligned container footer-version"><div class="ui tiny image"><a href="https://neuron.zettel.page"><img alt="logo" src="https://raw.githubusercontent.com/srid/neuron/master/assets/neuron.svg" title="Generated by Neuron 1.9.35.3" /></a></div></div></div></body></html>