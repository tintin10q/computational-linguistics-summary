<!DOCTYPE html><html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type" /><meta content="width=device-width, initial-scale=1" name="viewport" /><!--replace-start-0--><!--replace-start-5--><!--replace-start-8--><title>N-Grams - Neuron Zettelkasten</title><!--replace-end-8--><!--replace-end-5--><!--replace-end-0--><link href="https://cdn.jsdelivr.net/npm/fomantic-ui@2.8.7/dist/semantic.min.css" rel="stylesheet" /><link href="https://fonts.googleapis.com/css?family=Merriweather|Libre+Franklin|Roboto+Mono&amp;display=swap" rel="stylesheet" /><!--replace-start-1--><!--replace-start-4--><!--replace-start-7--><link href="https://raw.githubusercontent.com/srid/neuron/master/assets/neuron.svg" rel="icon" /><meta content="When doing Data/Normalization of a sequence your first intuition when working with a language that has spaces might be to split the words along the spaces, but often having shorter splits of maybe 2-3 symbols gives better results. These remaining symbols are N-grams. So when splitting this sentence:" name="description" /><link href="https://neuron.zettel.page/N-grams.html" rel="canonical" /><meta content="N-Grams" property="og:title" /><meta content="Neuron Zettelkasten" property="og:site_name" /><meta content="article" property="og:type" /><meta content="N-grams" property="neuron:zettel-id" /><meta content="N-grams" property="neuron:zettel-slug" /><meta content="root/Languages" property="neuron:zettel-tag" /><script type="application/ld+json">[{"@context":"https://schema.org","itemListElement":[{"name":"Languages","item":"https://neuron.zettel.page/Languages.html","@type":"ListItem","position":1}],"@type":"BreadcrumbList"}]</script><style type="text/css">body{background-color:#eeeeee !important;font-family:"Libre Franklin", serif !important}body .ui.container{font-family:"Libre Franklin", serif !important}body h1, h2, h3, h4, h5, h6, .ui.header, .headerFont{font-family:"Merriweather", sans-serif !important}body code, pre, tt, .monoFont{font-family:"Roboto Mono","SFMono-Regular","Menlo","Monaco","Consolas","Liberation Mono","Courier New", monospace !important}body div.z-index p.info{color:#808080}body div.z-index ul{list-style-type:square;padding-left:1.5em}body div.z-index .uplinks{margin-left:0.29999em}body .zettel-content h1#title-h1{background-color:rgba(33,133,208,0.1)}body nav.bottomPane{background-color:rgba(33,133,208,2.0e-2)}body div#footnotes{border-top-color:#2185d0}body p{line-height:150%}body img{max-width:100%}body .deemphasized{font-size:0.94999em}body .deemphasized:hover{opacity:1}body .deemphasized:not(:hover){opacity:0.69999}body .deemphasized:not(:hover) a{color:#808080 !important}body div.container.universe{padding-top:1em}body div.zettel-view ul{padding-left:1.5em;list-style-type:square}body div.zettel-view .pandoc .highlight{background-color:#ffff00}body div.zettel-view .pandoc .ui.disabled.fitted.checkbox{margin-right:0.29999em;vertical-align:middle}body div.zettel-view .zettel-content .metadata{margin-top:1em}body div.zettel-view .zettel-content .metadata div.date{text-align:center;color:#808080}body div.zettel-view .zettel-content h1{padding-top:0.2em;padding-bottom:0.2em;text-align:center}body div.zettel-view .zettel-content h2{border-bottom:solid 1px #4682b4;margin-bottom:0.5em}body div.zettel-view .zettel-content h3{margin:0px 0px 0.4em 0px}body div.zettel-view .zettel-content h4{opacity:0.8}body div.zettel-view .zettel-content div#footnotes{margin-top:4em;border-top-style:groove;border-top-width:2px;font-size:0.9em}body div.zettel-view .zettel-content div#footnotes ol > li > p:only-of-type{display:inline;margin-right:0.5em}body div.zettel-view .zettel-content aside.footnote-inline{width:30%;padding-left:15px;margin-left:15px;float:right;background-color:#d3d3d3}body div.zettel-view .zettel-content .overflows{overflow:auto}body div.zettel-view .zettel-content code{margin:auto auto auto auto;font-size:100%}body div.zettel-view .zettel-content p code, li code, ol code{padding:0.2em 0.2em 0.2em 0.2em;background-color:#f5f2f0}body div.zettel-view .zettel-content pre{overflow:auto}body div.zettel-view .zettel-content dl dt{font-weight:bold}body div.zettel-view .zettel-content blockquote{background-color:#f9f9f9;border-left:solid 10px #cccccc;margin:1.5em 0px 1.5em 0px;padding:0.5em 10px 0.5em 10px}body div.zettel-view .zettel-content.raw{background-color:#dddddd}body .ui.label.zettel-tag{color:#000000}body .ui.label.zettel-tag a{color:#000000}body nav.bottomPane ul.backlinks > li{padding-bottom:0.4em;list-style-type:disc}body nav.bottomPane ul.context-list > li{list-style-type:lower-roman}body .footer-version img{-webkit-filter:grayscale(100%);-moz-filter:grayscale(100%);-ms-filter:grayscale(100%);-o-filter:grayscale(100%);filter:grayscale(100%)}body .footer-version img:hover{-webkit-filter:grayscale(0%);-moz-filter:grayscale(0%);-ms-filter:grayscale(0%);-o-filter:grayscale(0%);filter:grayscale(0%)}body .footer-version, .footer-version a, .footer-version a:visited{color:#808080}body .footer-version a{font-weight:bold}body .footer-version{margin-top:1em !important;font-size:0.69999em}@media only screen and (max-width: 768px){body div#zettel-container{margin-left:0.4em !important;margin-right:0.4em !important}}body span.zettel-link-container span.zettel-link a{color:#2185d0;font-weight:bold;text-decoration:none}body span.zettel-link-container span.zettel-link a:hover{background-color:rgba(33,133,208,0.1)}body span.zettel-link-container span.extra{color:auto}body span.zettel-link-container.errors{border:solid 1px #ff0000}body span.zettel-link-container.errors span.zettel-link a:hover{text-decoration:none !important;cursor:not-allowed}body [data-tooltip]:after{font-size:0.69999em}body div.tag-tree div.node{font-weight:bold}body div.tag-tree div.node a.inactive{color:#555555}body .tree.flipped{-webkit-transform:rotate(180deg);-moz-transform:rotate(180deg);-ms-transform:rotate(180deg);-o-transform:rotate(180deg);transform:rotate(180deg)}body .tree{overflow:auto}body .tree ul.root{padding-top:0px;margin-top:0px}body .tree ul{position:relative;padding:1em 0px 0px 0px;white-space:nowrap;margin:0px auto 0px auto;text-align:center}body .tree ul::after{content:"";display:table;clear:both}body .tree ul:last-child{padding-bottom:0.1em}body .tree li{display:inline-block;vertical-align:top;text-align:center;list-style-type:none;position:relative;padding:1em 0.5em 0em 0.5em}body .tree li::before{content:"";position:absolute;top:0px;right:50%;border-top:solid 2px #cccccc;width:50%;height:1.19999em}body .tree li::after{content:"";position:absolute;top:0px;right:50%;border-top:solid 2px #cccccc;width:50%;height:1.19999em}body .tree li::after{right:auto;left:50%;border-left:solid 2px #cccccc}body .tree li:only-child{padding-top:0em}body .tree li:only-child::after{display:none}body .tree li:only-child::before{display:none}body .tree li:first-child::before{border-style:none;border-width:0px}body .tree li:first-child::after{border-radius:5px 0px 0px 0px}body .tree li:last-child::after{border-style:none;border-width:0px}body .tree li:last-child::before{border-right:solid 2px #cccccc;border-radius:0px 5px 0px 0px}body .tree ul ul::before{content:"";position:absolute;top:0px;left:50%;border-left:solid 2px #cccccc;width:0px;height:1.19999em}body .tree li div.forest-link{border:solid 2px #cccccc;padding:0.2em 0.29999em 0.2em 0.29999em;text-decoration:none;display:inline-block;border-radius:5px 5px 5px 5px;color:#333333;position:relative;top:2px}body .tree.flipped li div.forest-link{-webkit-transform:rotate(180deg);-moz-transform:rotate(180deg);-ms-transform:rotate(180deg);-o-transform:rotate(180deg);transform:rotate(180deg)}</style><script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/themes/prism.min.css" rel="stylesheet" /><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/components/prism-core.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/plugins/autoloader/prism-autoloader.min.js"></script><!--replace-end-7--><!--replace-end-4--><!--replace-end-1--></head><body><div class="ui fluid container universe"><!--replace-start-2--><!--replace-start-3--><!--replace-start-6--><nav class="flipped tree deemphasized" id="zettel-uptree" style="transform-origin: 50%"><ul class="root"><li><ul><li><div class="forest-link"><span class="zettel-link-container"><span class="zettel-link"><a href="Languages.html">Languages</a></span></span></div></li></ul></li></ul></nav><div class="ui text container" id="zettel-container" style="position: relative"><div class="zettel-view"><article class="ui raised attached segment zettel-content"><div class="pandoc"><h1 id="title-h1">N-Grams</h1><p>When doing <a href="Data/Normalization" title="cf">Data/Normalization</a> of a sequence your first intuition when working with a language that has spaces might be to split the words along the spaces, but often having shorter splits of maybe 2-3 symbols gives better results. These remaining symbols are N-grams. So when splitting this sentence: “Attack at dawn” up into 3-Grams you get: [“Att”, “ack”, “ at“, “Daw”, “n”]</p><h2 id="word-n-gram">Word N-Gram</h2><p>A word N-Gram is a sequence of n words:</p><ul><li>Uni-Gram: Dog</li><li>Uni-Gram: You are</li><li>Tri-Gram: The building blocks</li><li>Tetra-Gram: City of shivering darkness …</li></ul><h2 id="encoding-position">Encoding position</h2><p><a href="Classification/Native baiyes/Naive Bayes Classifier" title="cf">Classification/Native baiyes/Naive Bayes Classifier</a> is bad for making <a href="Prediction/Language Modeling" title="cf">language models</a> because the bag of words assumption but the idea where it splits of the words to normalize the probability can also be used for making language models.</p><h2 id="predicting-n-grams">Predicting N-grams</h2><p>A great usecase for N-grams is using them for language models.</p><h3 id="predicting-next-word">Predicting next word</h3><p>For instance to predict the next word: How likely is it that given words W1 to Wn we observe Wn+1? So given Tree how likely is it we observe a number.</p><h3 id="predicting-the-likelihood-of-whole-sentences">Predicting the likelihood of whole sentences</h3><p>Out of all the sequences of n words, how likely is sequence A? Out of all sequences of 6 words, how likely is “The earth revolves around the sun”.</p><h2 id="getting-probabilities">Getting probabilities</h2><p>We can calculate this with : <span class="math display">$$p(\text{leaves}|\text{A tree has }) = \text{c}\frac{\text{leaves}}{\text{c(a tree has)}}$$</span></p><p>Chance of number given A tree has . Here c is a count function which counts how many times “a tree has” appears (regardless of continuation) in the big corups and how many times it is followed by “leaves” in the corpus. Then you devide them to get a probability. This is always between 0..1 because the continuation is at most all the times, and then you get x/x = 1</p><h3 id="probabilities-for-sequences">Probabilities for sequences</h3><p>To get the probabiliaty for a sequence we do the same but we devide how often the sequence you are intested in appears by the number of all the sequences of the same lenght. So for 4 word sequences that is: <span class="math display">$$p(\text{a tree has leaves}) = \text{c}\frac{\text{c(a tree has leaves)}}{\text{c(sequences of 4 words)}}$$</span></p><p>Because there are probably a lot more sequences of the lenght then the sequence that you are intrested in the probability is going to be very small (sparse). So small even that it could be considered unrelaiable. This is why it is better to calculate the probability of a sentence based on the probability of each word * the probability of the words before it appearing before it. We can just calculate the probabilities like this because of the <strong>probabilities chain rule</strong>. So that looks like this: <span class="math display">$$p(w_{1},...,w_{m})$$</span> <span class="math display">$$= p(w_{1} \cdot p(w_2|w_{1}) \cdot p(w_3|w_{1},w_{2}) \cdot ... p(w_{m}|w_1,...,w_{m-1})$$</span> <span class="math display">$$=\prod_{{i}=1}^m(p(w_i|w_{1:i-1}))$$</span></p><p>So every time you multiply the probability of the word by the probability of the sequence before it. This you repeat for the entire sequence.</p><h3 id="problems">Problems</h3><p>Language is infinite which makes language models age, but it also means that you can come up with sequences that are not in the corpus. The higher the n of an n-gram the lower the chance that the n-gram appears at all in the corpus. Or the larger the n-gram, the higher the chance that we won’t find it anywhere in a fintie corpus. This is solved by assuming the <a href="Prediction/markov assumption" title="cf">Prediction/markov assumption</a>.</p><h3 id="log-space">Log space</h3><p>Whenever you deal with chained probabilites it is best to convert all probabilites to logs so that we can sum insteadof multiplying and avoid having very little nubmers. This is a risk because of underflow. (Computers can’t deal well with very small numbers). Another benifit is that we can sum instead of having to multiply.</p><p><span class="math display">$$\log \text{p}(w_{1},...,w_{m} \approx \sum\limits^{m}_{i=1} (\log p(w_{i}|w_{i-n:i-1}))$$</span></p><h2 id="sequence-boundaries">Sequence boundaries</h2><p>It is very important to put sequence boundaries at the start and end of your sequence. When using N-grams the start boundaries need to be N-symbols long while the end boundaries are N-1. This way there you can still have valid N-grams.</p><h3 id="example">Example</h3><p>^^Hi there^ when using bigrams. This way you can go over the text with ngram lenght. Kind of like a sliding window, but you still know what is at the start because of the ^.</p><h3 id="abbriviations">Abbriviations</h3><p>End of sequence is abrivated so EoS. Beginning of sequence is abrivated so BoS.</p><h2 id="overfitting">Overfitting</h2><p>The longer the N-gram choice the better you can fit the specific training data you have which means the more chance of <a href="Prediction/Overfitting" title="cf">Prediction/Overfitting</a>. The more overfitting the less general your model becomes.</p></div></article><nav class="ui attached segment deemphasized backlinksPane" id="neuron-backlinks-pane"><h3 class="ui header">Backlinks</h3><ul class="backlinks"><li><span class="zettel-link-container cf"><span class="zettel-link"><a href="markov_assumption.html">Markov Assumption</a></span></span><ul class="context-list" style="zoom: 85%;"><li class="item"><div class="pandoc"><p>The Markov assumption says (in the language modeling domain) that the conditional probability of a word apearing next in the sequence can be approximated by looking at its local history instead of the entire history. In this case the preceding history is an <span class="zettel-link-container cf"><span class="zettel-link" title="Zettel: N-Grams"><a href="N-grams.html">n-gram</a></span></span> of words which indicates how far you look back. This can be expresed with math as: <span class="math display">$$p(W_{1},...,W_{m}) `\approx \prod^{m}_{i = 1}(p(W_{i}|W_{i-n:i-1}))$$</span></p></div></li></ul></li><li><span class="zettel-link-container cf"><span class="zettel-link"><a href="Viterbi_Algorithm.html">Viterbi Algorithm</a></span></span><ul class="context-list" style="zoom: 85%;"><li class="item"><div class="pandoc"><p>Using the recursion we get <span class="math inline">\(O(Q^nT)\)</span> where n is the n in <span class="zettel-link-container cf"><span class="zettel-link" title="Zettel: N-Grams"><a href="N-grams.html">N-grams</a></span></span>. This is also called the <strong>order of the model</strong>. So n = 2 is a bigram. So this makes it much better and you need a lot of data to get higher n.</p></div></li></ul></li><li><span class="zettel-link-container cf"><span class="zettel-link"><a href="Smoothing.html">Smoothing</a></span></span><ul class="context-list" style="zoom: 85%;"><li class="item"><div class="pandoc"><p>If the model encounters a word it didn’t see in training it can’t even start looking for transitions from the preceeding <span class="zettel-link-container cf"><span class="zettel-link" title="Zettel: N-Grams"><a href="N-grams.html">N-gram</a></span></span>. This can be quanitified in the <a href="OOV%20rate.md">OOV rate</a> (persentage of never seen before words in the test set). Because langauage is infinite the OOV rate will always be more than 0 when using the model in the wild.</p></div></li></ul></li></ul><h3 class="ui header"><span title="Backlinks from folgezettel parents">Uplinks</span></h3><ul class="backlinks"><li><span class="zettel-link-container folge"><span class="zettel-link"><a href="Languages.html">Languages</a><span data-nosnippet="" style="user-select: none; color: gray" title="Folgezettel">#</span></span></span><ul class="context-list" style="zoom: 85%;"><li class="item"><div class="pandoc"><em>Parent directory zettel</em></div></li></ul></li></ul></nav><nav class="ui attached segment deemphasized bottomPane" id="neuron-tags-pane"><div><span class="ui basic label zettel-tag" title="Tag">root/Languages</span></div></nav><nav class="ui bottom attached icon compact inverted menu blue" id="neuron-nav-bar"><!--replace-start-9--><!--replace-end-9--><a class="item" href="https://github.com/srid/neuron/edit/master/doc/./Languages/N-grams.md" title="Edit this page"><i class="edit icon"></i></a><a class="right item" href="impulse.html" title="Open Impulse"><i class="wave square icon"></i></a></nav></div></div><!--replace-end-6--><!--replace-end-3--><!--replace-end-2--><div class="ui center aligned container footer-version"><div class="ui tiny image"><a href="https://neuron.zettel.page"><img alt="logo" src="https://raw.githubusercontent.com/srid/neuron/master/assets/neuron.svg" title="Generated by Neuron 1.9.35.3" /></a></div></div></div></body></html>