<!DOCTYPE html><html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type" /><meta content="width=device-width, initial-scale=1" name="viewport" /><!--replace-start-0--><!--replace-start-5--><!--replace-start-8--><title>Viterbi Algorithm - Neuron Zettelkasten</title><!--replace-end-8--><!--replace-end-5--><!--replace-end-0--><link href="https://cdn.jsdelivr.net/npm/fomantic-ui@2.8.7/dist/semantic.min.css" rel="stylesheet" /><link href="https://fonts.googleapis.com/css?family=Merriweather|Libre+Franklin|Roboto+Mono&amp;display=swap" rel="stylesheet" /><!--replace-start-1--><!--replace-start-4--><!--replace-start-7--><link href="https://raw.githubusercontent.com/srid/neuron/master/assets/neuron.svg" rel="icon" /><meta content="How do you actually do Decoding in practice? Because there is a large growth in the input. Given a sequence of 4 words and 45 tags there are 45^4 possible sequences. So it’s O(Q^{|T|}). This grows fast. This means we can not calculate all the sequences." name="description" /><link href="https://neuron.zettel.page/Viterbi_Algorithm.html" rel="canonical" /><meta content="Viterbi Algorithm" property="og:title" /><meta content="Neuron Zettelkasten" property="og:site_name" /><meta content="article" property="og:type" /><meta content="https://neuron.zettel.page/Pasted%20image%2020220308194114.webp" property="og:image" /><meta content="Viterbi Algorithm" property="neuron:zettel-id" /><meta content="Viterbi_Algorithm" property="neuron:zettel-slug" /><meta content="root/Prediction" property="neuron:zettel-tag" /><script type="application/ld+json">[{"@context":"https://schema.org","itemListElement":[{"name":"Prediction","item":"https://neuron.zettel.page/Prediction.html","@type":"ListItem","position":1}],"@type":"BreadcrumbList"}]</script><style type="text/css">body{background-color:#eeeeee !important;font-family:"Libre Franklin", serif !important}body .ui.container{font-family:"Libre Franklin", serif !important}body h1, h2, h3, h4, h5, h6, .ui.header, .headerFont{font-family:"Merriweather", sans-serif !important}body code, pre, tt, .monoFont{font-family:"Roboto Mono","SFMono-Regular","Menlo","Monaco","Consolas","Liberation Mono","Courier New", monospace !important}body div.z-index p.info{color:#808080}body div.z-index ul{list-style-type:square;padding-left:1.5em}body div.z-index .uplinks{margin-left:0.29999em}body .zettel-content h1#title-h1{background-color:rgba(33,133,208,0.1)}body nav.bottomPane{background-color:rgba(33,133,208,2.0e-2)}body div#footnotes{border-top-color:#2185d0}body p{line-height:150%}body img{max-width:100%}body .deemphasized{font-size:0.94999em}body .deemphasized:hover{opacity:1}body .deemphasized:not(:hover){opacity:0.69999}body .deemphasized:not(:hover) a{color:#808080 !important}body div.container.universe{padding-top:1em}body div.zettel-view ul{padding-left:1.5em;list-style-type:square}body div.zettel-view .pandoc .highlight{background-color:#ffff00}body div.zettel-view .pandoc .ui.disabled.fitted.checkbox{margin-right:0.29999em;vertical-align:middle}body div.zettel-view .zettel-content .metadata{margin-top:1em}body div.zettel-view .zettel-content .metadata div.date{text-align:center;color:#808080}body div.zettel-view .zettel-content h1{padding-top:0.2em;padding-bottom:0.2em;text-align:center}body div.zettel-view .zettel-content h2{border-bottom:solid 1px #4682b4;margin-bottom:0.5em}body div.zettel-view .zettel-content h3{margin:0px 0px 0.4em 0px}body div.zettel-view .zettel-content h4{opacity:0.8}body div.zettel-view .zettel-content div#footnotes{margin-top:4em;border-top-style:groove;border-top-width:2px;font-size:0.9em}body div.zettel-view .zettel-content div#footnotes ol > li > p:only-of-type{display:inline;margin-right:0.5em}body div.zettel-view .zettel-content aside.footnote-inline{width:30%;padding-left:15px;margin-left:15px;float:right;background-color:#d3d3d3}body div.zettel-view .zettel-content .overflows{overflow:auto}body div.zettel-view .zettel-content code{margin:auto auto auto auto;font-size:100%}body div.zettel-view .zettel-content p code, li code, ol code{padding:0.2em 0.2em 0.2em 0.2em;background-color:#f5f2f0}body div.zettel-view .zettel-content pre{overflow:auto}body div.zettel-view .zettel-content dl dt{font-weight:bold}body div.zettel-view .zettel-content blockquote{background-color:#f9f9f9;border-left:solid 10px #cccccc;margin:1.5em 0px 1.5em 0px;padding:0.5em 10px 0.5em 10px}body div.zettel-view .zettel-content.raw{background-color:#dddddd}body .ui.label.zettel-tag{color:#000000}body .ui.label.zettel-tag a{color:#000000}body nav.bottomPane ul.backlinks > li{padding-bottom:0.4em;list-style-type:disc}body nav.bottomPane ul.context-list > li{list-style-type:lower-roman}body .footer-version img{-webkit-filter:grayscale(100%);-moz-filter:grayscale(100%);-ms-filter:grayscale(100%);-o-filter:grayscale(100%);filter:grayscale(100%)}body .footer-version img:hover{-webkit-filter:grayscale(0%);-moz-filter:grayscale(0%);-ms-filter:grayscale(0%);-o-filter:grayscale(0%);filter:grayscale(0%)}body .footer-version, .footer-version a, .footer-version a:visited{color:#808080}body .footer-version a{font-weight:bold}body .footer-version{margin-top:1em !important;font-size:0.69999em}@media only screen and (max-width: 768px){body div#zettel-container{margin-left:0.4em !important;margin-right:0.4em !important}}body span.zettel-link-container span.zettel-link a{color:#2185d0;font-weight:bold;text-decoration:none}body span.zettel-link-container span.zettel-link a:hover{background-color:rgba(33,133,208,0.1)}body span.zettel-link-container span.extra{color:auto}body span.zettel-link-container.errors{border:solid 1px #ff0000}body span.zettel-link-container.errors span.zettel-link a:hover{text-decoration:none !important;cursor:not-allowed}body [data-tooltip]:after{font-size:0.69999em}body div.tag-tree div.node{font-weight:bold}body div.tag-tree div.node a.inactive{color:#555555}body .tree.flipped{-webkit-transform:rotate(180deg);-moz-transform:rotate(180deg);-ms-transform:rotate(180deg);-o-transform:rotate(180deg);transform:rotate(180deg)}body .tree{overflow:auto}body .tree ul.root{padding-top:0px;margin-top:0px}body .tree ul{position:relative;padding:1em 0px 0px 0px;white-space:nowrap;margin:0px auto 0px auto;text-align:center}body .tree ul::after{content:"";display:table;clear:both}body .tree ul:last-child{padding-bottom:0.1em}body .tree li{display:inline-block;vertical-align:top;text-align:center;list-style-type:none;position:relative;padding:1em 0.5em 0em 0.5em}body .tree li::before{content:"";position:absolute;top:0px;right:50%;border-top:solid 2px #cccccc;width:50%;height:1.19999em}body .tree li::after{content:"";position:absolute;top:0px;right:50%;border-top:solid 2px #cccccc;width:50%;height:1.19999em}body .tree li::after{right:auto;left:50%;border-left:solid 2px #cccccc}body .tree li:only-child{padding-top:0em}body .tree li:only-child::after{display:none}body .tree li:only-child::before{display:none}body .tree li:first-child::before{border-style:none;border-width:0px}body .tree li:first-child::after{border-radius:5px 0px 0px 0px}body .tree li:last-child::after{border-style:none;border-width:0px}body .tree li:last-child::before{border-right:solid 2px #cccccc;border-radius:0px 5px 0px 0px}body .tree ul ul::before{content:"";position:absolute;top:0px;left:50%;border-left:solid 2px #cccccc;width:0px;height:1.19999em}body .tree li div.forest-link{border:solid 2px #cccccc;padding:0.2em 0.29999em 0.2em 0.29999em;text-decoration:none;display:inline-block;border-radius:5px 5px 5px 5px;color:#333333;position:relative;top:2px}body .tree.flipped li div.forest-link{-webkit-transform:rotate(180deg);-moz-transform:rotate(180deg);-ms-transform:rotate(180deg);-o-transform:rotate(180deg);transform:rotate(180deg)}</style><script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/themes/prism.min.css" rel="stylesheet" /><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/components/prism-core.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/plugins/autoloader/prism-autoloader.min.js"></script><!--replace-end-7--><!--replace-end-4--><!--replace-end-1--></head><body><div class="ui fluid container universe"><!--replace-start-2--><!--replace-start-3--><!--replace-start-6--><nav class="flipped tree deemphasized" id="zettel-uptree" style="transform-origin: 50%"><ul class="root"><li><ul><li><div class="forest-link"><span class="zettel-link-container"><span class="zettel-link"><a href="Prediction.html">Prediction</a></span></span></div></li></ul></li></ul></nav><div class="ui text container" id="zettel-container" style="position: relative"><div class="zettel-view"><article class="ui raised attached segment zettel-content"><div class="pandoc"><h1 id="title-h1">Viterbi Algorithm</h1><p>How do you actually do <span class="zettel-link-container cf"><span class="zettel-link" title="Zettel: Decoding"><a href="Decoding.html">Decoding</a></span></span> in practice? Because there is a large growth in the input. Given a sequence of 4 words and 45 tags there are <span class="math inline">\(45^4\)</span> possible sequences. So it’s <span class="math inline">\(O(Q^{|T|})\)</span>. This grows fast. This means we can not calculate all the sequences.</p><p>The basic principle is to use recursion to compute the best path that could lead to a certain point given:</p><ul><li>The HMM</li><li>The overvations up till the point you are</li><li>The most probable state sequence to have generated the observations given the HMM.</li></ul><p>The idea is just to compute the best path for the first part of the sequence. This is not so hard. But then <strong>we know that the best path of the first part of the sequence is the best path up till that point</strong>. This means that we don’t have to recompute the probabilities for that part of the sequence. So if you have word 1 and 2 and 3. You can first compute the maximium likelyhood tag for 1, and then you can use that to calculate the maximum likelyhood for 2 without recomputing 1.</p><p>This means we can start from 3. To calculate the best one we need 2, to calculate the best one we need 1. Then we hit the base condition, so we calculate 1 and return it. Now that it returned we can use it to calcuate 2 and return that now that 2 returned we can calcualte 3. So basically you leave the work until you finished another problem.</p><p>Using the recursion we get <span class="math inline">\(O(Q^nT)\)</span> where n is the n in <span class="zettel-link-container cf"><span class="zettel-link" title="Zettel: N-Grams"><a href="N-grams.html">N-grams</a></span></span>. This is also called the <strong>order of the model</strong>. So n = 2 is a bigram. So this makes it much better and you need a lot of data to get higher n.</p><h2 id="in-practice">In practice</h2><p>Create a matrix M with as many rows as there are states <span class="math inline">\(q \in Q\)</span> and as many collums as there are events <span class="math inline">\(o \in T\)</span> to be decoded (so that is the <span class="zettel-link-container cf"><span class="zettel-link" title="Zettel: Sentence"><a href="Sentences.html">sentence</a></span></span> you want to tag + BoS and EoS).</p><p>The value of each cell <span class="math inline">\(M_{qo}\)</span> is computed as follows: <span class="math inline">\(p(t_{o}|t_{o-n:o-1})p(w_{o}|t_{o})\)</span>. So it’s the prior multiplied by the likelyhood. This means that each cell contains the posterior probability of findinf each tag given the current word after having observed everything that came before.</p><p>Each posterior in this matrix is depeneded on the <strong>emission and the local transition</strong> at each word <span class="math inline">\(w_t\)</span> with <span class="math inline">\(t \in T\)</span>, compute the posteriiors considering</p><h2 id="example">Example</h2><p>For an example we need an already filled A and B matrix. <span class="math inline">\(A_{ij}\)</span> encodes the probability that the tag in column j occurs given that the tag in row i has. <span class="math inline">\(B_{ik}\)</span> encodes the probability that word k is observed given that tag i was. Also rows rum to 1.</p><table class="ui table"><thead><tr><th>A</th><th>Det</th><th>Adj</th><th>Noun</th><th>Verb</th><th>EOS</th></tr></thead><tbody><tr><td>Det</td><td>0</td><td>0.2</td><td>0.8</td><td>0</td><td>0</td></tr><tr><td>Adj</td><td>0</td><td>0.3</td><td>0.6</td><td>0</td><td>0.1</td></tr><tr><td>Noun</td><td>0</td><td>0</td><td>0</td><td>0.5</td><td>0.5</td></tr><tr><td>Verb</td><td>0.5</td><td>0.1</td><td>0.2</td><td>0</td><td>0.2</td></tr><tr><td>BOS</td><td>0.5</td><td>0.2</td><td>0.3</td><td>0</td><td>0</td></tr></tbody></table><p>So for instance if you find and Adj you have a 0.3 chance to find another adjective and a 0.6 chance to find a noun. So given adj (on the side) you have a probability of 0.3 to find another adjective next and 0.6 to find a noun next. So given what’s on the side you have probability to find something at the top. That is why collums sum to 1.</p><p>The BOS vector at the bottom is <span class="math inline">\(\pi\)</span>. The initial distrobution. Given BoS what is the most likely to follow.</p><table class="ui table"><thead><tr><th>B</th><th>dog</th><th>the</th><th>chases</th><th>cat</th><th>fat</th></tr></thead><tbody><tr><td>Det</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>Adj</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>Noun</td><td>0.5</td><td>0</td><td>0</td><td>0.4</td><td>0.1</td></tr><tr><td>Verb</td><td>0.1</td><td>0</td><td>0.8</td><td>0.1</td><td>0</td></tr></tbody></table><p>The same here. Given a noun the chance is 0.5 that the word is dog.</p><p>Now we can start the algoritm.</p><p>First create the table. |Tag| rows by |words| collums + the BoS and EOS|. This table is called the trelis.</p><table class="ui table"><thead><tr><th>Trelis</th><th>BoS</th><th>The</th><th>dog</th><th>chases</th><th>the</th><th>fat</th><th>cat</th><th>EOS</th></tr></thead><tbody><tr><td>Det</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Adj</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Noun</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Verb</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table><p>First we can fill in the probabilities for the BoS row by filling in <span class="math inline">\(\pi\)</span> but you don’t have Bos and Eos on the side.</p><table class="ui table"><thead><tr><th>Trelis</th><th>BoS</th><th>The</th><th>dog</th><th>chases</th><th>the</th><th>fat</th><th>cat</th><th>EOS</th></tr></thead><tbody><tr><td>Det</td><td>0.5</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Adj</td><td>0.2</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Noun</td><td>0.3</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Verb</td><td>0</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table><p>Now we want to put take the next word “the” and take the “the” collum from the B matrix. This is the emission probability.</p><table class="ui table"><thead><tr><th>Trelis</th><th>BoS</th><th>The</th><th>dog</th><th>chases</th><th>the</th><th>fat</th><th>cat</th><th>EOS</th></tr></thead><tbody><tr><td>Det</td><td>0.5</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Adj</td><td>0.2</td><td>0</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Noun</td><td>0.3</td><td>0</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Verb</td><td>0</td><td>0</td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table><p>Now you want to multiply the collum we got (emmision probability) with the previous collum (the transition probability). This gives us:</p><table class="ui table"><thead><tr><th>Trelis</th><th>BoS</th><th>The</th><th>dog</th><th>chases</th><th>the</th><th>fat</th><th>cat</th><th>EOS</th></tr></thead><tbody><tr><td>Det</td><td>0.5</td><td>0.5</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Adj</td><td>0.2</td><td>0</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Noun</td><td>0.3</td><td>0</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Verb</td><td>0</td><td>0</td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table><p>Now this collum indicates the posterpior probability of the determiner given that the first token is “the”. We have now decoded the sequence BoS The. Now we can move on to the next sequence.</p><h2 id="trelis">Trelis</h2><p>The trelis represents the probability that the HMM is in state <span class="math inline">\(q \in Q\)</span> after seeing the previous events <span class="math inline">\(o_{1}...o_{t}\in O\)</span> and passing through the most probable sequence of states. The trelis is the name we give to this table we have been building.</p><p>The value of each cell at sucesive states is computed by <strong>multiplying the current transition and emission probabilities by the most probabable sequence</strong> which could lead to that cell. This means we have to find the probable sequence. We do this by taking the max(transition probability * each collum of the A matrix - BOS).</p><p><img alt="Pasted image 20220308194114" src="Pasted%20image%2020220308194114.webp" /></p><p>Now the max 0.4. Now we can multiply this with the emmision probability of the current word (dog) so we get: 0.4 * [0, 0, 0.5, 0.1] = [0, 0, 0.2, 0.04]. This becomes the new collum in the trelis.</p><table class="ui table"><thead><tr><th>Trelis</th><th>BoS</th><th>The</th><th>dog</th><th>chases</th><th>the</th><th>fat</th><th>cat</th><th>EOS</th></tr></thead><tbody><tr><td>Det</td><td>0.5</td><td>0.5</td><td>0</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Adj</td><td>0.2</td><td>0</td><td>0</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Noun</td><td>0.3</td><td>0</td><td>0.2</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Verb</td><td>0</td><td>0</td><td>0.004</td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table><p>We also want to mark which collum gave the max in the multiplication. In that case it was the first collumn. It is not garenteeth that this is always the highest value from the collum. It depends on A. So now we have the postterior probabilities for The dog. We don’t have to recompute for dog.</p><table class="ui table"><thead><tr><th>Trelis</th><th>BoS</th><th>The</th><th>dog</th><th>chases</th><th>the</th><th>fat</th><th>cat</th><th>EOS</th></tr></thead><tbody><tr><td>Det</td><td>0.5</td><td><strong>0.5</strong></td><td>0</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Adj</td><td>0.2</td><td>0</td><td>0</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Noun</td><td>0.3</td><td>0</td><td>0.2</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Verb</td><td>0</td><td>0</td><td>0.004</td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table><p>So as you can see we need 3 pieces of information each time.</p><ul><li>The posterior probability up to the previous state (the trellis)<ul><li>This makes it dynamic and saves time</li></ul></li><li>The transition probabilities from state <span class="math inline">\(q_i\)</span> to state <span class="math inline">\(q_j\)</span><ul><li>This is the prior</li></ul></li><li>The emission probabilities for observation <span class="math inline">\(o_j\)</span> given state <span class="math inline">\(q_j\)</span><ul><li>This the likelyhood</li></ul></li></ul><p>So let’s continue:</p><p><img alt="Pasted image 20220308195301" src="Pasted%20image%2020220308195301.webp" /></p><p>We found that 0.1 is the highest after multiplication. Then we multiply it with the emmision which is [0, 0, 0, 0.8] so 0.1 * [0, 0, 0, 0, 0.8] which becomes [0, 0, 0, 0.08]. Now we can fill that in, and we should mark the thirth collum.</p><table class="ui table"><thead><tr><th>Trelis</th><th>BoS</th><th>The</th><th>dog</th><th>chases</th><th>the</th><th>fat</th><th>cat</th><th>EOS</th></tr></thead><tbody><tr><td>Det</td><td>0.5</td><td><strong>0.5</strong></td><td>0</td><td>0</td><td></td><td></td><td></td><td></td></tr><tr><td>Adj</td><td>0.2</td><td>0</td><td>0</td><td>0</td><td></td><td></td><td></td><td></td></tr><tr><td>Noun</td><td>0.3</td><td>0</td><td><strong>0.2</strong></td><td>0</td><td></td><td></td><td></td><td></td></tr><tr><td>Verb</td><td>0</td><td>0</td><td>0.004</td><td>0.08</td><td></td><td></td><td></td><td></td></tr></tbody></table><p>When we have filled the entire trelis you can make the sequence of tags by picking the on the side of the collum that has the marked number in each row. So, so far we get [Det, Noun] which is correct.</p><p>Now we can repeat.</p><h2 id="more-context">More context</h2><p>We can also look at the maximum of the previous transitions from each posterior more than 1 previous column. So like n = 2 previous collums. However, it increases complexity but also brings sparcity. We don’t want 0 transition sto hijack the computation. Because often a transition is not actually impossible just a really small chance. To somewhat solve this we can use smoothing just like with <a href="Markov%20models.md">Markov models</a>. We can also use interpolation to guess the transition probability which is estimated by linearly interpolating using smaller n-grams. A guessed probability is better than no probability.</p><p>You can also use pseudo morphology -&gt; words constst of smaller units which influence their PoS tag. We can look at this to add bias to the probability, or we could compute emission probabilities for part words (suffixes of different lenghts, down till single characters). Whenever you hit an unkown word, use the emmision probability for the suffixes instead. Something is better than nothing.</p><h1 id="forward-algoritm">Forward Algoritm</h1><p>This is similar to the Viterbi algorithm, but used to compute the liklihood of a sequence of observed events given a HMM. So it tries to find the likelyhood of a sequence appearing at all in the set of all possible sequences. We want to find the culmative probability at each step.</p><p>All this does is replacing the max with the sum. You do this because you want to take into account the probability of all possible paths trough the hidden states, not find the likeliest path. Also you don’t need backpointers because we don’t try to find a tag.</p></div></article><nav class="ui attached segment deemphasized backlinksPane" id="neuron-backlinks-pane"><h3 class="ui header"><span title="Backlinks from folgezettel parents">Uplinks</span></h3><ul class="backlinks"><li><span class="zettel-link-container folge"><span class="zettel-link"><a href="Prediction.html">Prediction</a><span data-nosnippet="" style="user-select: none; color: gray" title="Folgezettel">#</span></span></span><ul class="context-list" style="zoom: 85%;"><li class="item"><div class="pandoc"><em>Parent directory zettel</em></div></li></ul></li></ul></nav><nav class="ui attached segment deemphasized bottomPane" id="neuron-tags-pane"><div><span class="ui basic label zettel-tag" title="Tag">root/Prediction</span></div></nav><nav class="ui bottom attached icon compact inverted menu blue" id="neuron-nav-bar"><!--replace-start-9--><!--replace-end-9--><a class="item" href="https://github.com/srid/neuron/edit/master/doc/./Prediction/Viterbi Algorithm.md" title="Edit this page"><i class="edit icon"></i></a><a class="right item" href="impulse.html" title="Open Impulse"><i class="wave square icon"></i></a></nav></div></div><!--replace-end-6--><!--replace-end-3--><!--replace-end-2--><div class="ui center aligned container footer-version"><div class="ui tiny image"><a href="https://neuron.zettel.page"><img alt="logo" src="https://raw.githubusercontent.com/srid/neuron/master/assets/neuron.svg" title="Generated by Neuron 1.9.35.3" /></a></div></div></div></body></html>