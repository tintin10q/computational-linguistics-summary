% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  11pt,
  british,
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{setspace}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Computational Linguistics Summary},
  pdfauthor={Quinten Cabo - quintencabo@gmail.com},
  pdflang={en-UK},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\usepackage[normalem]{ulem}
% Avoid problems with \sout in headers with hyperref
\pdfstringdefDisableCommands{\renewcommand{\sout}{}}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifXeTeX
  % Load polyglossia as late as possible: uses bidi with RTL langages (e.g. Hebrew, Arabic)
  \usepackage{polyglossia}
  \setmainlanguage[variant=british]{english}
\else
  \usepackage[main=british]{babel}
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
\fi
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{Computational Linguistics Summary}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Midterm \& Final}
\author{Quinten Cabo - quintencabo@gmail.com}
\date{\today}

\begin{document}
\maketitle

\listoffigures
\listoftables
\setstretch{1.5}
Computational Linguistics

\pagebreak

\hypertarget{topics}{%
\section{Topics}\label{topics}}

Hi, this is the summary for the Computational Linguistics
\href{https://uvt.osiris-student.nl/\#/onderwijscatalogus/extern/examenprogramma/16322/8B218-2021?taal=en}{(800962-B-6)}
course from Tilburg University created by Quinten Cabo.

There are 5 topics:

\begin{itemize}
\tightlist
\item
  \href{Data.md}{Data}
\item
  \href{Classification.md}{Classification}
\item
  \href{Prediction.md}{Prediction}
\item
  \href{Languages.md}{Languages}
\item
  \href{Semantic-Similarity.md}{Semantic-Similarity}
\end{itemize}

There are also some notes that are not in a topic. These are:

\begin{itemize}
\tightlist
\item
  \href{Goals.md}{Goals}
\item
  \href{Learning.md}{Learning}
\item
  \href{Other/Bert\%20Lecture.md}{Bert Lecture}
\item
  \href{Other/Quiz\%20Questions.md}{Quiz Questions}
\item
  \href{Other/Practice\%20Questions.md}{Practice Questions}
\item
  \href{Other/Things\%20that\%20he\%20said\%20come\%20at\%20the\%20exam.md}{Things
  that he said come at the exam}
\end{itemize}

Is this summary based on a web of notes and explanations to wordy for
you? Then there is also
\href{Other/Computational-Linguistics-Aurea.pdf}{Aurea's linear summary}
which uses short paragraphs and bullet points instead.

\begin{figure}
\centering
\includegraphics{network.png}
\caption{network}
\end{figure}

https://user-images.githubusercontent.com/24190849/172182045-0a225ef8-b56c-4190-be9e-6895417ee45e.mp4

\hypertarget{disclaimer}{%
\section{Disclaimer}\label{disclaimer}}

Although I have tried my best to make sure this summary is correct, I
will take no responsibility for mistakes that might lead to you having a
lower grade.

\hypertarget{issues}{%
\section{Issues}\label{issues}}

If you see anything that you think might be wrong then please create an
issue on the
\href{https://github.com/tintin10q/computational-linguistics-summary}{Github
repository} or even better, create a
\href{https://www.dataschool.io/how-to-contribute-on-github/}{pull
request} üòÑ

\hypertarget{support}{%
\section{Support}\label{support}}

If you appreciate my summaries and you want to thank me then you can
support me here:

\begin{itemize}
\tightlist
\item
  \href{https://www.paypal.me/quintencabo}{PayPal}
\item
  \href{https://tikkie.me/pay/c3cp0dcfd8mautn8ud79}{Tikki‚Ç¨}
\end{itemize}

\begin{figure}
\centering
\includegraphics{tikki.png}
\caption{Tikkie qr code valid till around 18 june}
\end{figure}

\begin{quote}
Every model is wrong, but some models are useful. \# Lexicon A lexicon
is a list of words enriched with some attributes. Plural is lexica. It
is much simpler than a \href{Corpus.md}{corpus}. Usually a list of words
within some domain or a list of words with some property, often
represented with numbers.
\end{quote}

Lexica do not contain the semantics of the word. So a dictionary is
\textbf{not} a lexicon.

\hypertarget{words}{%
\section{Words}\label{words}}

What is a word? This is a hard question to answer and is very language
dependant. Today, one might say it is the characters between spaces? But
spaces actually didn't exist yet until 400 years ago. There are also
languages without words. This is why we have to talk about
\href{Morphemes.md}{morphemes} to define it better and to segment words.

This is also why when doing analysis, you want to tokenize your text,
which is basically deciding what the words are from the string.

\hypertarget{representing-words-as-numbers}{%
\subsection{Representing words as
numbers}\label{representing-words-as-numbers}}

We usually think of words and letters as \href{Symbol.md}{symbols}. In
that case, each symbol identical to itself and equally different from
all other symbols of the same \href{Type.md}{type}.

Alternatively, you can also encode words and letters as numbers or
\href{../Semantic-Similarity/Vector\%20semantics.md}{lists of numbers}
called vectors or
\href{../Semantic-Similarity/Embeddings.md}{embeddings}. When you do
this, a wide range of math tools become available to analyse
\textbf{graded relations} between symbols. This allows you to say things
about how similar dog and cat are.

\hypertarget{symbols}{%
\section{Symbols}\label{symbols}}

A symbol is what you decide is your smallest unit. This can be
\href{Token.md}{tokens}, words etc whatever you want.

\hypertarget{token-type-lemma-example}{%
\section{Token \& Type \& Lemma
Example}\label{token-type-lemma-example}}

I will eat what you eat, even if I have never eaten it before.

\begin{itemize}
\tightlist
\item
  14 \href{Token.md}{Token} (16 with punctuation).
\item
  12 \href{Type.md}{Type} (eat and eat are of one type, and the two I as
  well.)
\item
  11 \href{Lemma.md}{Lemma} (eat and eaten are the same lemma (not the
  same type))
\end{itemize}

\hypertarget{treebank}{%
\section{Treebank}\label{treebank}}

A treebank stores syntactic trees. Treebanks provide
\href{Corpus.md}{corpora} consisting of syntactically annotated
sentences. The Pann Treebank has annotated the Brown, Switchboard, ATIS,
and WSJ corpora and some Arabic and Chinese. More treebanks exist for
other languages.

The Penn treebank uses bracketing to mark
\href{../Languages/Constituency.md}{constituents}. Some phrases come
with annotations about their grammatical function and semantic function.

You can derive the \href{../Languages/Grammar.md}{grammar} of a language
with a treebank. However,
\href{../Languages/Natural\%20languages.md}{natural languages} are
infinite.

\hypertarget{lemma}{%
\section{Lemma}\label{lemma}}

A lemma is the dictionary entry of a word. So this is always a word, not
just a character. Types and tokens can also be characters. So if you
have ``The ape shared bananas with the other apes'' then ape and apes
here are the same lemma. It is like removing the 's and like the extra
things. These extra modifier things are called
\href{Morphemes.md}{Morphemes}. Walking and walk are considered the same
lemma. Is and am are also considered the same lemma.

The citation form is a \href{../Languages/Synonyms.md}{synonym} for
lemma.

\hypertarget{word-forms}{%
\subsection{Word forms}\label{word-forms}}

Word forms are the other possible versions of a lemma, which mean the
same thing. For instance, the verb \emph{sing}. Sing is the lemma for
the word forms sing, sang and sung.

\hypertarget{lemmatization}{%
\subsection{Lemmatization}\label{lemmatization}}

Lemmatization is the process of \textbf{reducing each word-form} found
in a corpus \textbf{to its corresponding lemma}.

\hypertarget{examples}{%
\subsubsection{Examples}\label{examples}}

\begin{itemize}
\tightlist
\item
  Went ‚Üí Go
\item
  Geese ‚Üí Goose
\item
  Best ‚Üí Good
\item
  Talked ‚Üí Talk
\item
  Apples ‚Üí Apple
\item
  Biked ‚Üí Bike
\item
  Biking ‚Üí Bike
\item
  Bike ‚Üí Bike
\end{itemize}

You go to the base form. Sometimes the base is language specific.
Lemmatizers are specific for languages. \href{./data.md}{Here} is an
example of how lemmas compare to tokens and types.

\hypertarget{word-sense}{%
\subsection{Word sense}\label{word-sense}}

The meaning of a word can be considered separately from the word symbol.
\textbf{The meaning of a word is captured in the word sense}. A word
sense can be expressed in text. So the word sense of apple is a mostly
round fruit. Interestingly, in this explanation I relied on other word
senses for instance fruit to explain the word sense of apple.

\hypertarget{polysemous-multiple-meanings}{%
\subsubsection{Polysemous -- Multiple
meanings}\label{polysemous-multiple-meanings}}

Sometimes a lemma can have multiple word senses. For instance, apple can
also refer to fruit or an organization. When this occurs, a lemma is
called \textbf{polysemous} (have multiple senses). These multiple
meanings lead to \href{../Languages/Ambiguity.md}{ambiguity}. In the
book on page 103 the example of mouse is given. A mouse can be a rodent
or an input device for a computer. Each of these separate meanings is
called a \textbf{word sense}.

\hypertarget{morphemes}{%
\section{Morphemes}\label{morphemes}}

Morphemes are the smallest meaning bearing unit of languages. But this
is controversial.

When doing \href{Lemma.md}{lemmatization}, you turn a word into its
lemma form. The lemma of apes is ape. Here, you removed the `s'. The
parts you removed were \textbf{affixes}, and the part that stays behind
is the \textbf{stem}. Both stems and affixes are \textbf{morphemes}.

\begin{itemize}
\tightlist
\item
  \textbf{Stems} is the main meaning bearing unit in a word.
\item
  \textbf{Affixes} are the strings that modify stems in some way.
\end{itemize}

For something to be a morpheme, it has to have meaning. For instance, if
you have walk then if you remove anything of this:

\begin{quote}
{[}`alk', `Tlk', `Tak', `Tal', `lk', `ak', `al', `k', `l', `k', `a',
`l', `a', `lk', `Tk', `Tl', `k', `l', `k', `T', `l', `T', `ak', `Tk',
`Ta', `k', `a', `k', `T', `a', `T', `al', `Tl', `Ta', `l', `a', `l',
`T', `a', `T'{]}
\end{quote}

These all have no meaning.

\hypertarget{inflected}{%
\subsection{Inflected}\label{inflected}}

When affixes are added to a stem, the word becomes \textbf{inflected}.
For example, if you have the stem green then green\textbf{er} is an
\textbf{inflected} version of green. Another example is house and houses
(houses is inflected).

\hypertarget{stemming}{%
\subsection{Stemming}\label{stemming}}

Stemming is when you remove all the affixes of a stem. It doesn't have
to be the same as lemmatization. Because you don't have to go to the
dictionary version. For instance, with irregular verbs. Broke is the
past version of break. However, both are stems.

So if you stem on \emph{broke} you get \emph{broke}. If you stem on
\emph{break} you get \emph{break}. If you stem \emph{breaking} you get
\emph{break}. If you do lemmatization on broke, you get break.

So stemming only removes affixes, lemmatization goes to the lemma.

Sometimes a word with affixes occurs separately in the dictionary. For
instance, the word ``lemmatization''. If you stem this, you get lemma.
If you lemmatize it, you get ``lemmatization'' because lemmatization
also occurs separately in the dictionary (with the affixes).

So sometimes stemming is more disruptive, sometimes lemmatization is
more disruptive, it depends on the dictionary and the kind of affixes.

\hypertarget{kinds-of-affixes}{%
\section{Kinds of affixes}\label{kinds-of-affixes}}

\hypertarget{inflectional}{%
\subsection{Inflectional}\label{inflectional}}

The inflectional affixes don't create new words in the dictionary. This
means that they are removed by lemmatization. They tend to encode
number, tense, aspect, gender, case. Inflectional affixes are fully
productive. This means that with any (regular) verb, noun etc you can
add these to indicate the meaning. You can always add \texttt{+ed} to a
verb to indicate the past. You can always indicate more than one by
adding an -s. One human, multiple humans.

So words of the same class can get the same inflectional affixes. These
affixes are morphine's. Irregular words can not. For instance, one sheep
and multiple sheep.

\hypertarget{derivational}{%
\subsection{Derivational}\label{derivational}}

The derivational affixes create new words in the dictionary. They do
this by either changing the grammatical category of the word, which
changes the meaning of the word. Or by encoding things like negation,
nominalization, reiteration \ldots{} which directly changes the meaning
of the word.

\begin{itemize}
\tightlist
\item
  Happy ‚Üí unhappy

  \begin{itemize}
  \tightlist
  \item
    Stem: happy
  \item
    Lemmatization: unhappy
  \end{itemize}
\item
  Happy ‚Üí happy + ness ‚Üí happiness

  \begin{itemize}
  \tightlist
  \item
    Stem: happy
  \item
    Lemmatization: happiness
  \end{itemize}
\item
  Happy ‚Üí un + happy + ness ‚Üí unhappiness

  \begin{itemize}
  \tightlist
  \item
    Stem: happy
  \item
    Lemmatization: unhappiness
  \end{itemize}
\end{itemize}

\hypertarget{compounding}{%
\section{Compounding}\label{compounding}}

Sometimes you can also combine stems together to produce new meaning.
For instance:

\begin{itemize}
\tightlist
\item
  fire + fighter = firefighter
\item
  tele + vision = television
\item
  motor + bike = motorbike
\end{itemize}

These types of words are called compounds. In Dutch, it's called dubbel
worden, double words.

The meaning of a compound can be very clear and easy to guess to very
opaque where the meaning is not very much if at all related to the
constituents.

\hypertarget{morphological-complexity}{%
\section{Morphological complexity}\label{morphological-complexity}}

Words can have different morphological complexity. A word is
Monomorphemic when they only consist of a single morpheme (plus possible
inflectional morphemes)

Polymorphemic is when the word includes derivational affixes or are
compounds.

\hypertarget{token}{%
\section{Token}\label{token}}

A token is every entry in a \href{Corpus.md}{corpus}. The idea is that
no token is the \emph{same}. If you have the sentence ``The boys like
boys who like apples'' then the word boys occurs multiple times, but we
consider it a different token.

\hypertarget{corpus}{%
\section{Corpus}\label{corpus}}

The main resource for data are corpora. A corpus is a \textbf{computer
readable} collection of linguistic productions. Text, speech, gestures.
You have \textbf{one corpus} and there are \textbf{multiple corpora}.

\hypertarget{variants-of-corpus}{%
\subsection{Variants of Corpus}\label{variants-of-corpus}}

So structured data. Corpora are made by different people, this also
makes them vary a lot.

\begin{itemize}
\tightlist
\item
  Who produced it?

  \begin{itemize}
  \tightlist
  \item
    Age, gender, education, ethnicity\ldots.
  \end{itemize}
\item
  What language(s) is it?
\item
  When was the text produced?

  \begin{itemize}
  \tightlist
  \item
    Synchronic corpus is if no time period information is in the corpus.
  \item
    Diachronic is when there is a time dimension.
  \end{itemize}
\item
  For which goal or function was the corpus produced?

  \begin{itemize}
  \tightlist
  \item
    Genre, medium
  \end{itemize}
\end{itemize}

These questions make corpora vary.

\hypertarget{choosing-a-corpus-based-on-this-is-important.}{%
\subsection{Choosing a corpus based on this is
important.}\label{choosing-a-corpus-based-on-this-is-important.}}

Sentences in languages are mostly infinite. This is why selecting a
corpus that is \textbf{representative} of the phenomenon you want to
model is very important. Your model will only be as good as the data you
base it on.

\hypertarget{properties-of-a-corpus}{%
\subsection{Properties of a corpus}\label{properties-of-a-corpus}}

\begin{itemize}
\tightlist
\item
  Crawled/manually curated

  \begin{itemize}
  \tightlist
  \item
    Crawled is fetched from the internet
  \end{itemize}
\item
  Balanced/imbalanced
\item
  Single author/more authors
\item
  Diachronic/Synchronic

  \begin{itemize}
  \tightlist
  \item
    Diachronic is organized along the time dimension
  \item
    Synchronic is not organized along the time dimension
  \end{itemize}
\item
  Written/spoken/mixed/video (the modal)
\item
  Single language/multi-language/parallel

  \begin{itemize}
  \tightlist
  \item
    Parallel is when the different languages talk about the same things.
  \end{itemize}
\item
  \ldots{}
\end{itemize}

\hypertarget{sentences}{%
\section{Sentences}\label{sentences}}

A sentence is a sequence of \href{Symbol.md}{symbols}. So sentence =
sequence, they are \href{../Languages/Synonyms.md}{synonyms}. Often when
talking about a sentence, we mean a sequence of \href{Words.md}{words}.
So in that case, we consider words as symbols.

\hypertarget{type}{%
\section{Type}\label{type}}

A type is every distinct word in a \href{Corpus.md}{corpus}. So with
``The apple boys like boys who like apples''. Here the 2 boys tokens are
of the same type. So types can occur more often, so you can connect a
frequency to them. Apple and apples are not the same type!

Types are typically unambiguous. More than 80\% of words in the English
\href{../Languages/Languages.md}{language} are
\href{../Languages/Ambiguity.md}{unambiguous}. But the types that are
\href{../Languages/Ambiguity.md}{ambiguous.} occur frequently. Almost
two thirds of the \href{Token.md}{tokens} in a corpus are ambiguous.

\hypertarget{thesaurus}{%
\section{Thesaurus}\label{thesaurus}}

A thesaurus is a resource that groups \href{Words.md}{words} by how
similar their meaning is, from synonymy to autonomy, sometimes enriched
with other relations and definitions. The difference between a thesaurus
and a dictionary is that a thesaurus also has the relationships between
words, it is like a network instead of a flat list. The plural of
thesaurus is thesauri.

The most famous thesaurus for English is
\href{https://wordnet.princeton.edu}{WordNet}. Words in a thesauri have
\textbf{glosses}, which are an explanation of what a word means. There
are nouns; verbs and adjectives and adverbs. In WordNet, you have
synsets (near-\href{../Languages/Synonyms.md}{synonym} sets) a set of
things that are very similar because nothing is actually exactly the
same. These synsets are connected by hypernym and hyponym (is a)
relations.

For example, a dog is a mammal and a human is also a mammal. These are
hypernym relations. The other way around is hyponym. So an example of a
mammal is a dog.

\textbf{Human is a hyponym of mammal} and \textbf{mammal is a hypernym
of human}. Going down the tree is a hyponym relationship and going up
the tree is a hypernym relationship. Also see the image below. Colour
\textbf{is a} hypernym of blue and blue \textbf{is a} hyponym of colour.

\begin{figure}
\centering
\includegraphics{hypo_hyper_nym.png}
\caption{Hyper and hyper - nym relationships. Blue is a hyponym of
colour and color is a hypernym of blue}
\end{figure}

So you can traverse this tree, and then you can see how many steps it
takes to get to another category. This is called the \textbf{path
length} or maximum similarity. Two synsets are more similar the shorter
the path that connects them.

So here, the path length between colour and Sky blue and plant is 2.

Here is a bigger tree:

\begin{figure}
\centering
\includegraphics{Pasted_image_20220213185844.png}
\caption{Bigger Thesaurus tree}
\end{figure}

So in this picture, finch is more similar to animal than tree because
the path length of finch to animal is 2 while from tree to animal is 3.
This picture is a simplification because remember instead of just one
word there are always synsets of words which almost mean the same thing.

\hypertarget{resnik-similarity}{%
\subsection{Resnik similarity}\label{resnik-similarity}}

The Resnik similarity is a way to calculate
\href{../Semantic-Similarity/Similarity.md}{similarity} between synsets.

Path length by itself is a crude measure because every step is counted
as 1. However, if you think about it, knowing that two things are a bird
is more informative than knowing something is a mammal. You can already
see this in the picture with bird having 2 hyponyms and mammal having 3
hyponyms. The Resnik similarity measure attempts to overcome this by
calculating the information content each synset carries instead. So then
when calculating the path length instead of always taking one you take
the information content of the synset you go through.

You can look at it as the information-content weighted path length: two
words are more similar the \textbf{higher the information content} of
their lowest common subsume.

\begin{quote}
\textbf{A common subsume} is a connection that both words have. So a
common subsume of tree and flower is plant but also living and entity.
Or finch and whale is animal and living and entity. The closest common
subsume is the common subsume, which is the closest to both of your
targets. The closest subsume of finch and flower is living. The closest
common subsume of whale and bike is entity. The closest common subsume
of tree and flower is plant.
\end{quote}

\begin{quote}
\textbf{The information content} is based on how often generally occurs.
The more often something occurs, the lower the information content. Just
like Shannon tells us. The more often something occurs, the lower the
entropy and the lower the information content. \textbf{You can calculate
it with 1/n} where n is how frequently it occurs in the corpus. It makes
sense. For instance, if a sentence starts with ``the'' then almost
anything can follow, but if the sentence starts with ``Rainbows'' then
much fewer things can follow that, so rainbow is more informative.
\end{quote}

So in the example above, the Resnick similarity of whale and lion is
lower than finch and penguin because mammal occurs more than bird. You
would think, at least.

So the less frequent the lowest common subsume is in a corpus, the
higher the information content it has and the higher the Resnik
similarity between two synsets.

\hypertarget{example}{%
\subsubsection{Example}\label{example}}

So lets bird has information content of 0.5 and mammal has the
information content of 0.2 and animal has an information content of 0.1
then the Resnik similarity between whale and finch is
\(0.5 + 0.2 + 0.1 = 0.8\).

Now let's say the information content of vehicle is 0.3 and entity is
0.01 and living is 0.09. This makes the Resnic similarity between car
and plant \(0.3 + 0.01 + 0.09 = 0.4\).

So, according to Resnic, whale and finch are twice as similar as plant
and car. If you had just used path length, then they would have been the
same in similarity.

\hypertarget{lesk-similarity}{%
\subsection{Lesk similarity}\label{lesk-similarity}}

This type of similarity looks at the lexical overlap of the glosses of
words. Two words are more similar the higher the lexical overlap between
the glosses.

\textbf{Lexical overlap} is just the number of overlapping words in the
glosses. So for instance:

Cat: \textbf{Feline} mammal usually having thick soft fur and no ability
to roar. Lion: Large gregarious predatory \textbf{feline} of Africa and
India having a tawny coat with a shaggy mane in the male. Car: A motor
vehicle with four wheels; usually propelled by an internal combustion
engine.

Cat and lion share a word, so their Lesk similarity is higher than cat
and car. But at first glance, cat and car look more similar. Only one
letter difference.

\hypertarget{normalization}{%
\section{Normalization}\label{normalization}}

Normalization is the act of trying to reduce noise, so the signal
becomes stronger, and so it can be picked up. When analysing a text,
normalization can be useful. This can entail:

\begin{itemize}
\tightlist
\item
  Segmenting sentences from text
\item
  Segmenting individual words (tokenization)
\item
  Lemmatization
\item
  Spelling correcting
\item
  Making everything lowercase.
\item
  Eliminating undesirable characteristics
\item
  Remove the most frequent \href{Words.md}{words}.
\end{itemize}

Sometimes you want to create a bag of words after you have done
lemmatization if you don't care about the variability between dog and
dogs.

You can also use \href{../Languages/Regular\%20expression.md}{regular
expressions} to transfer words that you consider the same into the same
word. Or transfer all email address to EMAIL. Or you could say DAYOFWEEK
for each week day.

\hypertarget{losing-information}{%
\subsection{Losing information}\label{losing-information}}

You always lose information when normalizing, so you have to be careful.
Often you want to lose this information, but for instance God and god
can mean different things and things like this you might want to keep.
Or so and soooooo can be considered the same or not, it depends. You
might care about this difference and loose it if you normalize it.

\hypertarget{ambiguous-words}{%
\subsection{Ambiguous words}\label{ambiguous-words}}

\begin{itemize}
\tightlist
\item
  gold-digger, 2 words? 1 word?
\item
  Clitecs: I'm, 2 words? 1 word?
\item
  Abbreviations: e.t.c. 3 words? 1 word?
\end{itemize}

You can keep going.

This is why you have \textbf{tokenizers} that turn the text into tokens.
The tokens don't have this problem.

\hypertarget{lcd}{%
\subsubsection{LCD}\label{lcd}}

LCD is a standard for tokenizing .

For instance, you always consider \texttt{.} a separate token.
Abbreviations are one word. \texttt{Doesn\textquotesingle{}t} should be
tokenized, as \texttt{does} and \texttt{n\textquotesingle{}t}. The
people that made LCD thought hard about what tokenizing decisions should
be made.

\hypertarget{levels-of-analysis}{%
\section{Levels of analysis}\label{levels-of-analysis}}

Computational linguistics is about analysing and producing language with
computers. You can analyse languages on different levels. Here they are:

\hypertarget{phonology}{%
\subsection{Phonology}\label{phonology}}

Studies linguistic sounds to construct \textbf{inventories of sounds
with a linguistic role}. A phonetic is a basic sound. There is a fixed
list of basic sounds that humans can make. From these sounds, words are
formed. The idea of phonology is to analyse the languages by looking at
the basic units. Because switching them up changes the meaning.
\textbf{We won't look at this one much, if at all.}

\hypertarget{segmentation}{%
\subsection{Segmentation}\label{segmentation}}

In the context of computational linguistics (CL) segmentation is the
task of splitting text or speech into \href{Symbol.md}{symbols} of the
appropriate granularity. In a text, words are symbols, but they are made
up of letter symbols etc.

Segmenting speech is harder because it is unclear where the spaces are.
Fun fact, spaces were only invented 4 centuries ago!

\hypertarget{morphology}{%
\subsection{Morphology}\label{morphology}}

This studies how words are built up from smaller meaning-bearing units,
the \href{Morphemes.md}{Morphemes}. The classic view of CL is that a
morpheme is the smallest linguistic unit that can carry meaning. This is
a controversial point, apparently. Morphological complexity varies
between languages more simple or super complex. So the word ``apple'' is
a morphine, which means üçé (that is supposed to be an apple emoji). Then
if you have ``apples'' you also have the morpheme s which means more
than one apple.

The complexity is determined by the amount of morphologic units are in
one symbol.

\hypertarget{syntax}{%
\subsection{Syntax}\label{syntax}}

Studies of the set of rules, principles and processes for combining
symbols according to the structure of the language. This asserts
whenever a sentence is well-formed. \textbf{But it doesn't look at
meaning at all}. So you can have something like the chicken cooked the
ropes from the chimney. It is formed ok, but the meaning does not make
sense. It is quite hard to make sentences which don't mean anything.

\hypertarget{lexical-semantics}{%
\subsection{Lexical semantics}\label{lexical-semantics}}

This describes the meaning of single symbolic units like words,
morphemes and collocations. It looks to classify and decompose lexical
structures cross linguistically, and understand similarities. So this is
about units, analysing and comparing units of syntax.

\hypertarget{compositional-semantics}{%
\subsection{Compositional Semantics}\label{compositional-semantics}}

This studies how atomic meanings are combined into larger meaningful
units, such as sentences, paragraphs etc. Because you can't just keep
adding atomic units together.

\hypertarget{pragmatics}{%
\subsection{Pragmatics}\label{pragmatics}}

This analyses how context influences meaning. This encompasses
semantics, linguistic knowledge of participants, situational context,
shared knowledge, goals and intent. Super hard for computers to do.

\hypertarget{what-to-do-when-analysing-language}{%
\section{What to do when analysing
language}\label{what-to-do-when-analysing-language}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Identify the language we are dealing with.
\item
  Identify the linguistic units of interest (letters, morphemes, words,
  phrases, sentences, paragraphs \ldots)
\item
  Find the structure beneath the service.
\item
  Understand what each symbol means.
\item
  Understand what the whole message aims to convey
\item
  Respond appropriately (by a computer)
\end{enumerate}

Number 6 is the gold jewel of this whole field. That is what it is all
for.

\hypertarget{co-occurrence}{%
\section{Co-occurrence}\label{co-occurrence}}

Co-occurrence is a simple way of comparing the meaning of words in a
corpus without the need for annotation or a
\href{../Data/Thesaurus.md}{thesaurus}. The idea is to create a
co-occurrence sets for the two words from the \href{Context.md}{context}
they appear in.

You do this by specifying the length of your context, for instance a
sentence, and then you just add all words to a set which co-occur in
that context around the target word.

After you have these sets, you can use the
\href{Jaccard's\%20distance.md}{Jaccard's distance} to derive at a
similarity measure. If you want, you can also visualize this as a
\href{Vector\%20semantics.md}{vector} instead of a set where every value
in the vector is either 0 or 1 to indicate if the word appeared in the
context of the target word or not.

\begin{longtable}[]{@{}lllll@{}}
\toprule
& Cat & Food & Music & Beans \\
\midrule
\endhead
Cat & 1 & 1 & 0 & 0 \\
Food & 1 & 1 & 1 & 1 \\
Music & 0 & 1 & 1 & 0 \\
Beans & 0 & 1 & 0 & 1 \\
\bottomrule
\end{longtable}

This method is very flat and coarse. There is no information about
co-occurrence frequency or about context similarity,, and it is prone to
chance co-occurrences. Basically every item only exist once in a set, so
there is no frequency taken into account. To do that, you need to look
at co-occurrence counts.

\hypertarget{co-occurrence-counts}{%
\section{Co-occurrence counts}\label{co-occurrence-counts}}

To improve upon co-occurrence, you can also count how often a word
appears in the context of a target word. Instead of just adding words
which appear in a context of a target word to a set.

I would do this like this:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ collections }\ImportTok{import}\NormalTok{ defaultdict}

\CommentTok{\# Get the corpus data}
\ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(}\StringTok{\textquotesingle{}corpus.txt\textquotesingle{}}\NormalTok{) }\ImportTok{as}\NormalTok{ corpus\_file:}
\NormalTok{    corpus }\OperatorTok{=}\NormalTok{ corpus\_file.read()}

\CommentTok{\# A word will have the default value of a default dict with 0 as default value}
\NormalTok{co\_occurence\_counts }\OperatorTok{=}\NormalTok{ defaultdict(}\KeywordTok{lambda}\NormalTok{ : defaultdict(}\KeywordTok{lambda}\NormalTok{ : }\DecValTok{0}\NormalTok{))}

\NormalTok{words\_in\_corpus }\OperatorTok{=} \BuiltInTok{set}\NormalTok{(corpus.split(}\StringTok{\textquotesingle{} \textquotesingle{}}\NormalTok{))}

\NormalTok{sentences }\OperatorTok{=} \BuiltInTok{set}\NormalTok{(corpus.split(}\StringTok{\textquotesingle{}.\textquotesingle{}}\NormalTok{))}

\ControlFlowTok{for}\NormalTok{ target\_word }\KeywordTok{in}\NormalTok{ words\_in\_corpus: }\CommentTok{\# Make the count for every word }
    \ControlFlowTok{for}\NormalTok{ sentence }\KeywordTok{in}\NormalTok{ sentences:  }\CommentTok{\# Sentence is the context here}
\NormalTok{        sentence }\OperatorTok{=}\NormalTok{ sentence.split(}\StringTok{\textquotesingle{} \textquotesingle{}}\NormalTok{)}
        \ControlFlowTok{if}\NormalTok{ target\_word }\KeywordTok{in}\NormalTok{ sentence:}
\NormalTok{            sentence.remove(target\_word) }\CommentTok{\# Don\textquotesingle{}t co occure with yourself}
            \ControlFlowTok{for}\NormalTok{ word }\KeywordTok{in}\NormalTok{ sentence:}
\NormalTok{                co\_occurrence\_counts[target\_word][word] }\OperatorTok{+=} \DecValTok{1}

\CommentTok{\# Now you have a dict with all the words and the counts of the words in their context}

\CommentTok{\# Because of the default dict all other words will be 0 if accessed.}

\CommentTok{\# Hopefully you can see how the dict relates/can be easily transformed to a matrix/vector space }
\end{Highlighting}
\end{Shaded}

This then gives you a matrix something like this (but then bigger):

\begin{longtable}[]{@{}lllll@{}}
\toprule
& Cat & Food & Music & Beans \\
\midrule
\endhead
Cat & 20 & 16 & 2 & 6 \\
Food & 16 & 4 & 1 & 70 \\
Music & 2 & 1 & 40 & 0 \\
Beans & 6 & 70 & 0 & 100 \\
\bottomrule
\end{longtable}

This matrix represents the \textbf{vector space}. Then you can compare
rows of this vector space (which are the embeddings) by computing the
\href{Cosine.md}{Cosine} angle between two rows (vectors).

\hypertarget{hyperparameters}{%
\subsection{Hyperparameters}\label{hyperparameters}}

Both these methods have hyperparameters which can vary. For instance,
you can pick different context sizes, leave out the x most frequent
words in the corpus. Assign different occurrence counts the further a
word is from the target words, and on and on. You can also use
\href{../Languages/N-grams.md}{N-grams} instead of words, for instance.

\hypertarget{problems-with-count-based-embeddings}{%
\section{Problems with count based
embeddings}\label{problems-with-count-based-embeddings}}

We are just assigning the same number to each word which occurs in the
context. However, some words in the context might provide more
information than others. This method currently does not take advantage
of that. Basically, the raw frequency counts don't take into account the
information provided by the context.

The vectors are \textbf{very space} (because of the
\href{../Languages/Zipfian\%20Distribution.md}{Zipfian Distribution} of
language) as the vectors are the same size as every distinct word in the
corpus, often like 40,000 in size. The most of the vector will just be
0. These things could be solved somewhat by tweaking the hyperparameters
described above. You want to apply
\href{Dimensionality\%20reduction.md}{dimensionality reduction} to make
the embeddings more usable.

\hypertarget{association-measure}{%
\subsection{Association measure}\label{association-measure}}

To solve the not taking into account of information problem is to weight
each count collected for each word which appeared in the context by an
\textbf{\href{Association\%20measure.md}{association measure}} before
you compute the cosine angle between two vectors.

\hypertarget{cosine}{%
\section{Cosine}\label{cosine}}

The \textbf{cosine} \href{Similarity.md}{similarity} can calculate the
semantic similarity between two \href{../Data/Words.md}{words},
\href{../Data/Sentences.md}{sentences} or two documents when represented
as \href{Vector\%20semantics.md}{vectors} or
\href{Embeddings.md}{embeddings}.

The idea of the cosine metric when comparing
\href{Word\%20similarity.md}{embeddings} is to calculate the cosine of
the angle between the vectors (in n-dimensional space) This means that
the two word vectors you want to compare have to have the same
shape/dimensionality.

Let's say we have the words Village and Waiter and those are represented
with the vectors \textbf{v} and \textbf{w} of the same size. Now we want
to use cosine to calculate the similarity of \textbf{v} and \textbf{w}.
First you can take the dot product of the two vectors:
\[\textbf{v} \cdot \textbf{w}\] \textgreater{} \textbf{Reminder}
\textgreater{} Dot product is defined as:
\[\sum\limits_{i=1}^Nv_iw_i=v_1w_1+v_2w_2+...+v_Nw_N\]

The dot product acts as a similarity metric because it will tend to be
high just when the two vectors have large values in the same dimensions
and vectors that have zeros in different dimensions (orthogonal vectors)
will have a dot product of 0, representing their strong dissimilarity.

This raw dot product favours vector length long vectors because the dot
product is higher if a vector is longer (with higher values in each
dimension). More frequent words have longer vectors, since they tend to
co-occur with more words and have higher co-occurrence values with each
of them.

This is not good because it would mean longer vectors are more similar,
which doesn't make sense. Longer here is not the number of items in the
vector, it is the length of the vector from the center of the
n-dimensional space to `final location' of the vector. See below:

\begin{quote}
The vector length is written as \(|\textbf{v}|\) so with those two
\textbar{} \textbar{} The vector length is defined as:
\[|\textbf{v}| = \sqrt{\sum\limits^{N}_{i=1}{v^2_i}}\] So you take the
sum of the elements after you multiplied all the elements of the vector
by themselves, and then you take the square root of that.
\end{quote}

We would like a similarity metric that tells us how similar two words
are regardless of their frequency. To overcome this issue, we modify the
dot product to normalize for the vector length by dividing the dot
product by the lengths of each of the two vectors.
\[\frac{\textbf{v} \cdot \textbf{w}}{|\textbf{v}||\textbf{w}|}\] This
normalized dot product turns out to be the same as the cosine of the
angle between the two vectors .

\[\frac{\textbf{v} \cdot \textbf{w}}{|\textbf{v}||\textbf{w}|} = \cos{\theta}\]
So then this is also
true:\[\textbf{v} \cdot \textbf{w} = |\textbf{v}||\textbf{w}| \cos{\theta}\]
This means we calculate the \textbf{cosine} metric between two vectors
\textbf{v} and \textbf{w} as:
\[\text{cosine}(\textbf{v},\textbf{w}) = \frac{\textbf{v} \cdot \textbf{w}}{|\textbf{v}||\textbf{w}|} = \frac{\sum\limits^N_{i=1}{v_iw_i}}{\sqrt{\sum\limits^N_{i=1}{v_i^2}}\sqrt{\sum\limits^N_{i=1}{w_i^2}}}\]
These are all the same. Even though the most to the right looks
daunting, it is just the dot product on top and the vector length
calculations expressed with the sum sigma notation.

\begin{figure}
\centering
\includegraphics{Pasted_image_20220606220028.png}
\caption{Cosine distance}
\end{figure}

\hypertarget{jaccards-distance}{%
\section{Jaccard's distance}\label{jaccards-distance}}

The Jaccard distance is a simple way to quantify how similar the
\href{Context.md}{context} is of two words is.

The idea is Jaccard's distance is that you divide the intersection of
two sets and the union of two context sets. This gives a value between 1
and 0. With 1 being complete overlap and 0 being no overlap. This is a
more general method to compute the similarity of two sets.

This is idea can be expressed with this math:
\[\text{Jacc}(\textbf{x},\textbf{y}) = \frac{\textbf{x}~\cap~\textbf{y}}{\textbf{x}~\cup~\textbf{y}}\]

You can use Jaccard distance to calculate the similarity between two
\href{Co-occurrence.md}{co-occurrence} sets.

This method is very flat and coarse. There is no information about
co-occurrence frequency or about context similarity,, and it is prone to
chance co-occurrences. Basically every item only exist once in a set, so
there is no frequency taken into account. To do that, you need to look
at \href{Co-occurrence.md}{Co-occurrence counts}.

\hypertarget{word-similarity}{%
\section{Word similarity}\label{word-similarity}}

Sometimes \href{../Data/Words.md}{words} are
\href{../Languages/Synonyms.md}{synonyms} of each other. But they can
also be related in other ways. For instance, coffee and cup are related
but might not seem related to a computer.

\hypertarget{asking-humans}{%
\subsection{Asking humans}\label{asking-humans}}

One way is asking humans to say how related they think word pairs are
and using those scores. This requires a lot of effort.

\hypertarget{semantic-fields}{%
\subsubsection{Semantic fields}\label{semantic-fields}}

One way of asking humans is by asking them if a word belongs to a
semantic field. Words that are in the semantic field \emph{hospital}
could be related like surgeon, scalpel, nurse, aesthetic, hospital,
operation, disease, recovery and so on.

\textbf{Topic models} use semantic fields to discover abstract semantic
fields in documents. A topic model can say which semantic fields are
similar.

Semantic fields and topic models can be used to discover the topical
structure of a document using unsupervised
\href{../Other/Learning.md}{learning}.

\hypertarget{semantic-frames}{%
\subsubsection{Semantic Frames}\label{semantic-frames}}

A semantic frame is a set of words which encapsulate perspectives or
participants in a particular type of event. For instance, a commercial
transaction is an event where one entity trades something (usually
money) to another entity in return for something valuable, like a
service or a some goods. Now verbs like buy, sell or nouns like buyer,
seller, goods, money, service encode this event lexically. Frames have
semantic roles, and words in the sentence take roles in the event.
Knowing about the frame allows paraphrasing a sentence: \emph{Quinten
bought the book from Ling} as \emph{Ling sold the book to Quinten,} for
example.

\hypertarget{using-vectors}{%
\subsection{Using vectors}\label{using-vectors}}

You can get a sense of the meaning of words by
\href{Vector\%20semantics.md}{assigning vectors to words}. This
approximates meaning. Basically, the computer can never grasp the
meaning of a word, but it can know if two words are likely to mean the
same / are \href{../Languages/Synonyms.md}{synonyms} because the vectors
are \href{Similarity.md}{similar}. The closeness is dependent on the way
you decide to calculate distance, usually this is done with
\href{Cosine.md}{cosine}.

\hypertarget{association-measure-1}{%
\section{Association measure}\label{association-measure-1}}

When using \href{Co-occurrence.md}{Co-occurrence} to create a
\href{Vector\%20Space.md}{Vector Space} from a corpus, you are just
assigning the same value to each word which occurs in the
\href{Context.md}{context} along a target word. However, some surface
forms in the context might provide more information than others.
Basically, the raw frequency counts don't take into account the
information provided by the context. To overcome this, it is useful to
apply an association measure.

The idea is to \textbf{warp the \href{Co-occurrence.md}{Co-occurrence}
patterns to weigh the more informative contexts more and the useless
ones less} (or not at all). One way to compute these weights is with the
\href{Point\%20wise\%20mutual\%20information\%20(PMI).md}{Point wise
mutual information (PMI)}.

\hypertarget{distributional-hypothesis}{%
\section{Distributional hypothesis}\label{distributional-hypothesis}}

The \textbf{distributional hypothesis} says that
\href{../Data/Words.md}{words} which are \href{Similarity.md}{similar}
in meaning tent to appear in the same context and places in
\href{../Data/Sentences.md}{sentences}. In other words, similar words
are used with the same other words.

For instance, \emph{eye doctor} and \emph{oculist} are a
\href{Synonyms.md}{synonym}. However, a computer doesn't know this
without looking at for instance a
\href{../Data/Thesaurus.md}{thesaurus}. With the distributional
hypothesis, a computer can also \href{../Other/Learning.md}{learn} which
words are similar semantically by looking if they appear in the same
places and context in sentences.

So the distributional hypothesis says that two words are more similar
when the overlap of their contexts is large.

Models which use the distributional hypothesis are called distributed
semantic models (DSM).

\begin{quote}
\textbf{You shall know a word by the company it keeps} J.R. Firth (1957)
\end{quote}

\hypertarget{example-1}{%
\subsection{Example}\label{example-1}}

Suppose you didn't know the meaning of the word ongchoi (a recent
borrowing from Cantonese) but you see it in the following contexts: -
Ongchoi is delicious saut√©ed with garlic. - Ongchoi is superb over rice.
- \ldots ongchoi leaves with salty sauces\ldots{}

And suppose that you had seen many of these context words in other
contexts: - \ldots spinach saut√©ed with garlic over rice\ldots{} -
\ldots chard stems and leaves are delicious\ldots{} - \ldots collard
greens and other salty leafy greens

The fact that ongchoi occurs with words like rice and garlic and
delicious and salty, as do words like spinach, chard, and collard greens
might suggest that ongchoi is a leafy green similar to these other leafy
greens. The same thing can be done computationally by just counting
words in the context of ongchoi. The computer would not know what
ongchoi means, but it would know that it shares meaning with food words.

\hypertarget{problem-with-the-distributional-hypothesis}{%
\subsection{Problem with the distributional
hypothesis}\label{problem-with-the-distributional-hypothesis}}

One large problem with the distributional hypothesis is that some words
can have multiple \href{../Data/Lemma.md}{senses} (multiple meanings).
For instance, table can be something like this (sense 1):

\begin{figure}
\centering
\includegraphics{Pasted_image_20220603161925.png}
\caption{Table}
\end{figure}

Or something like this (sense 2):

\begin{longtable}[]{@{}llll@{}}
\toprule
Number & A & B & C \\
\midrule
\endhead
1 & Red & 5 & Birds \\
3 & Blue & 5 & Dogs \\
7 & Green & 500 & Green Birds \\
\bottomrule
\end{longtable}

This would mean that a word's \href{Co-occurrence.md}{Co-occurrence}
with different contexts both where table means sense 1 and other where
it means sense 2. So in a way it does capture the fact that table means
2 things, but this is not great because it means that the table vector
will be related with both waiter and cells. But of course in real life
we only mean one for instance of sense 2. So it conflates to two
meanings.

Ultimately this leads to lower \href{Similarity.md}{similarity} scores
because the part of the vector which is high because of sense 1 will
lower the similarity of sense 2. Both cancel each other out and lower
the similarity for the other sense.

So these models don't deal well with
\href{../Languages/Ambiguity.md}{ambiguity}. Or maybe they deal too well
with it?

\href{../Prediction/Transformers.md}{Transformers} are able to deal with
this well. The idea is to compute the embeddings on the fly. Instead of
all beforehand.

\hypertarget{euclidean-distance}{%
\section{Euclidean distance}\label{euclidean-distance}}

The Euclidian distance calculates the distance between two points by:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Calculating the distance between each coordinate of the point and
\item
  Multiplying each distance by itself
\item
  Summing all the distances from every coordinate
\item
  Taking the square root of that.
\end{enumerate}

If we have the two vectors \(\textbf{v}\) and \(\textbf{w}\) this can be
expressed with math like so:
\[d(\textbf{v}, \textbf{w}) = \sqrt{(\textbf{v}_{0}-\textbf{w}_{1})^2+(\textbf{v}_{0}-\textbf{w}_{1})^2+...+(\textbf{v}_{n}-\textbf{w}_{n})^2}\]
Here d stands for distance.

For \href{Embeddings.md}{embeddings} this is \textbf{not} a good measure
of \href{Similarity.md}{similarity} because it is heavily influenced by
absolute large differences. For example, if the distance of only one
coordinate is really high, for instance 300 and all the other distances
are around 3 then the computer still thinks the two embeddings v and w
are far away from each other.

\begin{figure}
\centering
\includegraphics{Pasted_image_20220606220215.png}
\caption{Euclidian vs cosine distance}
\end{figure}

\hypertarget{word2vec}{%
\section{Word2Vec}\label{word2vec}}

The Word2Vec model is a popular way to use
\href{../Prediction/Feed\%20forward\%20neural\%20networks\%20(FFNN).md}{neural
networks} to create \href{Embeddings.md}{word Embeddings}. Using neural
networks, you can immediately get dense vectors instead of the space
vectors you get with \href{Co-occurrence.md}{counting based methods}.

Word2Vec has \textbf{two possible architectures}. One of them is called
\textbf{skip gram with negative sampling (SKNS)} this tries to predict
the words in the context from the target word. The other one is
\textbf{continues bag of words (CBoW)} which tries to predict the target
word from the context. With both methods we don't care about the actual
predictions but rather the hidden layers which are created as a result
of training for good predictions. \textbf{The hidden layers are selected
as the embeddings}. This is called representational learning. Let's now
go in more detail below.

\begin{figure}
\centering
\includegraphics{Pasted_image_20220604014153.png}
\caption{CVoW vs SGNS}
\end{figure}

\hypertarget{skip-gram-with-negative-sampling-skns}{%
\subsection{Skip gram with negative sampling --
SKNS}\label{skip-gram-with-negative-sampling-skns}}

The idea behind SKNS is build around the intuition that you don't need
to count \href{Co-occurrence.md}{Co-occurring} words, but you should
\textbf{predict co-occurring words}. The prediction is not the goal, but
the way to learn the representations. If the prediction gets good
results, you have a good \href{Embeddings.md}{embedding}. We don't care
about the prediction outcome, only about how it influences the
embeddings. The embeddings are taken directly from the weights used in
the neural network.

\hypertarget{how-to-do-this}{%
\subsubsection{How to do this?}\label{how-to-do-this}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Pick a target word.
\item
  Isolate a window of co-occurring context words. Window = context size.
  The window is a bag of words.
\item
  Update the representation (the weights used in the network) of the
  target word such that it \textbf{maximizes the probability of
  predicting its neighbours as neighbours} and minimizes the probability
  of predicting the other words as neighbours.
\end{enumerate}

So we try to train a neural network which will predict high co-occurring
words for a target word while not predicting non co-occurring words for
a target word. The weights which lead to the best model are chosen to
represent the target word. Using the weights of a good performing
network to represent the target word I find really clever.

The reason why this is called skip gram with negative sampling is that
you (would) have to use all the negative example classes (words which
don't co-occur with the target word) to train the model. However, this
would take waayy too long. Plus, not all words are equally informative.
So instead we only \textbf{sample} a few \textbf{negative classes}
(words which don't co-occur also called foils) to train on, and this
also works well. \textbf{Sample the negative words by picking the words
with probability inversely proportional to word frequency.} This makes
sure words which are very frequent don't get used as negative class as
these tent to be words like the or of which are not very informative
because they appear everywhere. When training the model, you do always
use all the positive words (the words which do co-occur) as classes.

This is a form of unsupervised learning. Text supervises itself as using
this method embeddings can be learned without having annotated data. You
don't need annotations to calculate the loss. Everything you need is
already in the text. This is great because annotating data is expensive.

\hypertarget{a-simple-binary-prediction-task}{%
\subsubsection{A simple binary prediction
task}\label{a-simple-binary-prediction-task}}

Word2Vec was designed to be very efficient. It achieves this by framing
what is described above as a binary problem. The system doesn't predict
which word occurs in the context, but \textbf{only if a word does}. This
is binary instead of multinomial, which is much more efficient to
calculate.

In practice, this is done by first paring the target word with a
co-occurring word and updating the network until the probability of
predicting the positive word is high. Then you also take one negative
sample and keep updating the model until the weights don't predict that
word. Then you just repeat these two steps by paring the target word
with another positive word and negative word. Typically, you do this
until you have gone through every positive word. This is many
comparisons, but they are cheap individually. The weight vector in the
end becomes the word embedding.

However, we are still going to use the
\href{../Classification/Logistic\%20Regression.md}{softmax} function
instead of a sigmoid function because we would like to have a
probability for every word in the corpus at every position and not just
a probability for negative or positive words. Naturally, you want the
probability for the words which appear in the context to be bigger and
smaller for the words which don't. The number in each output node is
\textbf{the probability of picking the corresponding word as neighbour
of the target}.

This approximates meaning because if two words have similar meaning the
\href{Distributional\%20hypothesis.md}{Distributional hypothesis} says
there are used around the same words (or generally have the same
neighbours). So the model should generate similar hidden layer
representations (weights) for words with similar meaning, as this
generates similar output. Another way to view this is that the network
with push or put vectors closer together in the
\href{Vector\%20Space.md}{Vector Space} which have similar contexts.

That results in something like this:

\begin{figure}
\centering
\includegraphics{embeddings-from-hidden-layer.png}
\caption{Embeddings from hidden layers. You can see that the hidden
layers for each word are different.}
\end{figure}

\hypertarget{representation-learning}{%
\subsubsection{Representation learning}\label{representation-learning}}

This type of learning described above is called representation learning.
The goal of the training is to get good internal representation of the
words. \textbf{By optimizing for prediction, we learn better
representations.} You don't really care about the predictions. They are
just the way to do unsupervised learning to get the representations. The
loss function, if you will.

Usually good representations are a by product of good predictions, but
in this case we are actually more into the representations as we can use
measures like \href{Cosine.md}{Cosine} to actually compare the meaning
of the resulting word representations. This is quite different to what
we have seen with machine learning up till now. One example of this
mentioned is to compute the representation of sentences and then compare
how similar a paraphrased sentence is.

\hypertarget{two-embeddings}{%
\subsubsection{Two embeddings}\label{two-embeddings}}

In practice we actually get two embeddings per word as each word can be
the target word or the in the context (positive or negative). After
doing Word2Vec you will get an embedding for the word-as-context and the
word-as-target.

\hypertarget{continues-bag-of-word-cbow}{%
\subsection{Continues Bag-Of-Word
(CBoW)}\label{continues-bag-of-word-cbow}}

The other architecture of Word2Vec is continues bag of words (CBoW).
This is similar to SKNS, but you flip the problem. Instead of predicting
the context (neighbouring words) from the target word, you try to
\textbf{predict the target word from the context words}. You also
\textbf{discard the order information} (bag of words).

This is kind of like a fill in exercise like this
\includegraphics{fill-in-exercise.png}

But remember that you discard the order of the words. So from the bag of
words you want to get the target word.

But how do we feed the context to the model? Some contexts might be
different sizes as well. The best way is to combine the vectors of the
words in the bag somehow into one vector. There are infinite ways to do
this, but Word2Vec gets the input to the network with by taking the
average vector of all the word vectors which co-occur in the context bag
of words. This is quite a simple method, but it actually works well.

So if you have \emph{I like banana}, and you want to predict \emph{like}
you get \emph{I \_\_\_ banana} which gives the context bag \{``I'',
``banana''\}. Then you average the vectors of I and banana to get the
input vector like, and then you train the model to predict like.

\hypertarget{comparing-cbow-and-sgns}{%
\subsection{Comparing CBoW and SGNS}\label{comparing-cbow-and-sgns}}

\begin{figure}
\centering
\includegraphics{Pasted_image_20220605011501.png}
\caption{CBOW and Skipgram}
\end{figure}

\hypertarget{rare-words}{%
\subsubsection{Rare words}\label{rare-words}}

\textbf{CBoW} maximizes the probability of the target word given the
context. Hence, given the context \emph{yesterday was a really \_\_\_
day}, CBoW will get rewarded more often for predicting \emph{beautiful}
or \emph{nice}, then for saying delightful. As delightful appears less
in \href{../Languages/Languages.md}{language} than \emph{nice} and
\emph{beautiful}.

\textbf{SGNS} predicts the context from the target. Given the word
\emph{delightful}, it assigns a large probability to the context words
\emph{yesterday}, \emph{was}, \emph{really}, \emph{day}.
\textbf{Independently,} as these are trained separately.

With \textbf{CBoW} the word \emph{nice}, \emph{beautiful} and
\emph{delightful} kind of compete to fill the space. In \textbf{SGNS}
the words \emph{delightful} + context pairs are treated as new
observations which avoid the competition and \textbf{penalizes rare
words}.

\begin{quote}
This sort of gets back to \href{Distributional\%20hypothesis.md}{the
problem with words having multiple senses}. SGNS I think kind of solves
the problem of multiple senses, as the representation of the word is
trained to deal well with all the different contexts it appears in. But
this is just what I think. Let me know if you think this is wrong.
\end{quote}

\begin{quote}
I think CBoW is more in line with what people do when processing
language. You learn words by the contexts they appear in.
\end{quote}

Because Skip-gram rely on single words input, it is less sensitive to
overfit frequent words, because even if frequent words are presented
more times that rare words during training, they still appear
individually, while CBOW is prone to overfit frequent words because they
appear several times along with the same context.

\hypertarget{efficiency}{%
\subsubsection{Efficiency}\label{efficiency}}

The CBoW architecture is computationally more efficient than the SKNS
method because you only use the context once instead to predict the
target word, instead of doing a lot of training for all the positive
context words and samples of negative context words. So there are fewer
comparisons with CBoW. This can also be seen in this image below.

\begin{figure}
\centering
\includegraphics{Pasted_image_20220604014153.png}
\caption{CVoW vs SGNS much more comparisions with SGNS}
\end{figure}

In the original paper of Word2Vec they wrote that CBOW took hours to
train, SGNS 3 days.

\hypertarget{choosing-between-cbow-and-sgns}{%
\subsubsection{Choosing between CBoW and
SGNS}\label{choosing-between-cbow-and-sgns}}

Choose SGNS or CBoW depending on your task and constraints.

\hypertarget{semantic-drift}{%
\subsection{Semantic drift}\label{semantic-drift}}

If you have a diachronic \href{../Data/Corpus.md}{Corpus}, you can also
estimate different embedding spaces for different time periods and look
at \textbf{which words have changed in meaning} and track this change.

\hypertarget{evaluation}{%
\subsection{Evaluation}\label{evaluation}}

\hypertarget{visualizations}{%
\subsubsection{Visualizations}\label{visualizations}}

One way to intrinsically evaluate your model is making sense is by
creating visualizations. This is done by reducing many dimensions to
just 2 or 3 to be able to plot them in our word. This can be done with
PCA or tSNE (in combination with PCA for lage spaces). This is
effectively a form of
\href{Dimensionality\%20reduction.md}{Dimensionality reduction}.

When you plot the vectors that are \href{Similarity.md}{more similar}
should be the ones with similar meaning. While the ones which are far
apart should not have similar meaning. If this is shown in the
visualization, the model works well.

Below is a visualization of a 60 dimensional space visualized in 2
dimensions.

\begin{figure}
\centering
\includegraphics{Pasted_image_20220601145459.png}
\caption{A two-dimensional (t-SNE) projection of embeddings for some
words and phrases, showing that words with similar meanings are nearby
in space}
\end{figure}

\hypertarget{intrinsic-evaluation}{%
\subsubsection{Intrinsic evaluation}\label{intrinsic-evaluation}}

Further intrinsic evaluation assess the goodness of a semantic space by
how well it can capture \textbf{human intuitions about word relations}.
Basically ask a bunch of humans how they think words relate and average
the results and see if the model can perform in this way. There are
datasets for this, for instance TOEFL or SimLex999. You can also say
that some words contain other words. So for instance if you subtract the
vector of paris with France and add the Itally vector you should get
Rome.

\hypertarget{extrinsic-evaluation}{%
\subsection{Extrinsic evaluation}\label{extrinsic-evaluation}}

This is when you use the resulting representations for another
downstream task. If the performance of this task improves, then you know
your model does well. Basically, it seems that the performance of every
task in computational linguistics improves. For this reason, word
embeddings are part of almost every architecture.

\hypertarget{biases}{%
\subsection{Biases}\label{biases}}

The models are not magic and only as good as our data. \textbf{Garbage
in is garbage out}. If your data incorporates societal biases, these
will occur in the results of your model. Choose your data carefully
without biases. This is currently an active problem in the field.

For instance, if in your corpora fathers are more often doctors and
mothers are more often nurses then this will be encoded in the word
encodings. Or when using \href{Connotations.md}{connotations} what if
the names of one nationality are more often closer to positive
sentiment, while other nationality names are closer to negative words.
This can have an unwanted effect on sentiment analysis predictions for
instance.

What can you do about this? This is still an active research field.
There seem to be two ways either change the data to represent society in
the way that we want. For instance you can apply
\href{../Data/Normalization.md}{Normalization} and replace all sex words
like girl or boy with SEX for instance. Or you can try to subtract a
special bias vector. Or you can try to change the model which actively
debias things they find in the data.

\hypertarget{summary-of-word2vec}{%
\subsection{Summary of Word2Vec}\label{summary-of-word2vec}}

With word2vec we use neural networks to learn better word embeddings by
optimizing hidden layers to yield better prediction in a binary
classification task. Given a source and a target word, predict whether
the target is a co-occurring word or a foil. After learning, the hidden
layer for each word encodes its semantic representation as learned from
text. This is called representation learning. You have to be careful
with biases.

\hypertarget{vector-space}{%
\section{Vector Space}\label{vector-space}}

When you have computed \href{Embeddings.md}{Embeddings}, basically
represented a bunch of words into vectors. Usually these vectors are the
same length because you check all the \href{Context.md}{context} for
with all the target words. This means you can put all the embeddings on
top of each other because they have the same columns if you wil.

This then represents the vector space. The n-dimensional space in which
all word embeddings reside.

A vector space is essentially a matrix created by putting all the
vectors on top of each other. Rows are targets, columns are contexts.
Targets and contexts can be anything. You can imagine the vector space
as a large n-dimensional space in which all the vectors have a location.

Tree examples below. The first one is with
\href{Co-occurrence.md}{Co-occurrence} of words in context around
targets words. The second one counts how often words occur in certain
document contexts. You can also derive them using neural networks, which
leads to less interpretable vector spaces. The last example is a vector
space plotted in 3d space to give an idea how you can imagine the
vectors having a position in a high dimensional space.

The cat cell I think means that around the target word cat, cat appeared
again.

\begin{longtable}[]{@{}lllll@{}}
\toprule
& Cat & Food & Music & Beans \\
\midrule
\endhead
Cat & 5 & 16 & 2 & 6 \\
Food & 16 & 10 & 1 & 70 \\
Music & 2 & 1 & 40 & 0 \\
Beans & 6 & 70 & 0 & 4 \\
\bottomrule
\end{longtable}

\begin{longtable}[]{@{}lllll@{}}
\toprule
& Document 1 & Document 2 & Document 3 & Document 4 \\
\midrule
\endhead
Cat & 20 & 16 & 2 & 6 \\
Food & 16 & 3 & 1 & 70 \\
Music & 2 & 1 & 15 & 0 \\
Beans & 6 & 70 & 0 & 60 \\
\bottomrule
\end{longtable}

\begin{figure}
\centering
\includegraphics{Pasted_image_20220606221314.png}
\caption{Vector space example}
\end{figure}

\hypertarget{dimensionality-reduction}{%
\section{Dimensionality reduction}\label{dimensionality-reduction}}

When using \href{Co-occurrence.md}{Co-occurrence} to derive a
\href{Vector\%20Space.md}{Vector Space} becomes very large with each row
being typically having 10\^{}5 dimensions of which most of the values
are 0 because the target words never appearing with other words in the
context. This takes a lot of storage and feeds into the curse of
dimensionality.

\hypertarget{goals}{%
\subsection{Goals}\label{goals}}

Every \href{Context.md}{context} is considered an atomic context. This
basically means that each context is considered separate. So we consider
the context cat and lion to be separate, but they are not.

So ultimately want to

\begin{itemize}
\tightlist
\item
  \textbf{Reduce noise and brittleness}: Some co-occurrences may happen
  by chance or under a very specific, non-conventional use of a word. It
  is a good idea to give these low importance.
\item
  \textbf{Discover latent meaning}: Leverage the fact that some contexts
  are similar and may represent a broader domain.
\end{itemize}

So, how can we reduce the dimensionality of the vector space?

\hypertarget{projection}{%
\subsection{Projection}\label{projection}}

We can project the vector space into a lower dimension. So you turn the
matrix into a new matrix with the same number of rows and a lower number
of columns. Each column is one dimension and also one context. You can
compute which context provides the most information and only keep those
contexts around. The columns with low information will have a lot of
zeros. You have to decide how many columns to keep.

More formally, we can \textbf{project} the distributional matrix of
dimensionality N*M into a lower-dimensional space N*K where K
\textless{} M (and K \textless{} N) \textbf{such that the reduced space
preserves the maximum amount of variance possible}. You only want to get
rid of redundant information. This should result in row vectors which
are real-valued and dense (not a lot of zeros).

There are a lot of techniques for dimensionality reduction trough
projection. There are many ways to project into lower dimensions like:

\begin{itemize}
\tightlist
\item
  Singular Value Decomposition
\item
  Non-negative Matrix Factorization
\item
  Kernel Principal component Analysis
\item
  Latent Dirichlet Allocation (used with
  \href{Word\%20similarity.md}{Topic models}).
\item
  \ldots{} (there are more)
\end{itemize}

The simplest trimming columns are explained below, in this method. The
simplest way to remove not, so useful information from your vector space
is to just trim entire columns from the matrix based on how many 0 there
are.

\hypertarget{trimming-columns}{%
\subsection{Trimming Columns}\label{trimming-columns}}

\textbf{When two targets have 0 in the same dimension, it doesn't
influence the Cosine} because the angle between 0 and 0 is 0. So there
is no reason to keep this around. You can just remove the columns with
the most 0 in them, and then keep a certain number of columns around.

This is quick and transparent (easy to interpret) but it doesn't reduce
noise and doesn't bring latent meaning to the surface. Also, you can
still interpret the remaining columns as the contexts.

\hypertarget{vector-semantics}{%
\section{Vector Semantics}\label{vector-semantics}}

The idea of vector semantics is to represent words (or your surface
form) as a vector of numbers. These vectors represent a word, a point in
a multidimensional semantic space. This allows you to compare the words
on a deeper level because vectors can be more or less
\href{Similarity.md}{Similar} instead of being all equally different or
based on \href{../Languages/Edit\%20distance.md}{Edit distance}.

These vectors can be derived from the distributions of word neighbours /
context of the other words because of the
\href{Distributional\%20hypothesis.md}{distributional hypothesis}. This
allows computers to compare the meaning of words with just a
\href{../Data/Corpus.md}{corpus} and without something like
\href{../Data/Thesaurus.md}{wordnet}. This is deeper than looking at
\href{../Languages/Edit\%20distance.md}{edit distance}.

\hypertarget{getting-the-vectors}{%
\subsection{Getting the vectors}\label{getting-the-vectors}}

An early idea to get the vectors was with
\href{Connotations.md}{connotations}. This requires humans to annotate
data in a 3 dimensional space of valence, arousal and dominance. Later,
linguists started looking at the \href{Context.md}{context} around words
to create the vectors.

\hypertarget{connotations}{%
\subsubsection{Connotations}\label{connotations}}

Early on this was done with \href{Connotations.md}{connotations}. The
idea is to ask a bunch of humans to rate words along the valence arousal
and dominance dimensions and average the results. This then gives you
vectors 3 numbers for each word. An advantage of connotations is that
the resulting vectors can be meaningfully interpreted. If a word has a
vector of high valence, then you know it is probably a positive
sentiment word.

\hypertarget{context}{%
\subsubsection{Context}\label{context}}

Later, linguists started looking at \href{Context.md}{context}. This can
yield vector representations from a corpus without needing humans. One
way of doing this is with counting. You can use an algorithm that scans
a corpus and counts which words are used close to other words. If eye
dokter and oculist are used in the same contexts next to the same words,
then the computer will assign the similar vectors to these words
meaning, as they will be close in the vector space. This is based on the
\href{Distributional\%20hypothesis.md}{distributional hypothesis}. Ways
of getting the vectors include \href{Co-occurrence.md}{Co-occurrence
(counting)} and using hidden layers of neural networks like
\href{../Prediction/Feed\%20forward\%20neural\%20networks\%20(FFNN).md}{Feed
forward neural networks (FFNN)} for instance using
\href{Word2Vec.md}{Word2Vec} or
\href{../Prediction/Recurrent\%20neural\%20network\%20(RNN).md}{Recurrent
neural network (RNN)} .

All the vectors have a place in a \href{Vector\%20Space.md}{vector
space} this allows to compare them.

The vectors derived from context are also called
\href{Embeddings.md}{embeddings} in the more recent literature. This new
name is given because you can't directly interpret the resulting vectors
any more, like you can with connotation vectors.

\hypertarget{the-pipeline}{%
\subsection{The pipeline}\label{the-pipeline}}

So how to you do get the vectors? 1. Choose a \href{Context.md}{Context}
size (documents, sentences, paragraphs etc.) 2. Derive the base vectors
either by using \href{Co-occurrence.md}{Co-occurrence} or
\href{../Classification/Logistic\%20Regression.md}{machine learning
techniques} (Word2Vec). 3. Choose an
\href{Association\%20measure.md}{Association measure} like
\href{Point\%20wise\%20mutual\%20information\%20(PMI).md}{PMI} or don't.
4. Use a \href{Dimensionality\%20reduction.md}{Dimensionality reduction}
technique, or don't. 5. Use a \href{Similarity.md}{Similarity} measure
to compare vectors. Basically always \href{Cosine.md}{Cosine}.

Models that use this pipeline fall under the name of
\textbf{distributional semantic models}. Which all make use of the
distributional hypothesis.

\hypertarget{point-wise-mutual-information-pmi}{%
\section{Point wise mutual information
(PMI)}\label{point-wise-mutual-information-pmi}}

One way to get weighs for an
\href{Association\%20measure.md}{association measure} to the embeddings
with is \textbf{point wise mutual information}. This is an intuitive
method. The idea is to weight the co-occurrences of words that happen
\textbf{more often than expected by chance} more, since these
co-occurrences capture some linguistically relevant phenome.

If \href{../Languages/Languages.md}{language} was random (entropy at the
max) we would have no association weights, everything would be as likely
as everything else. PMI works because language is not random. This means
you can know how often something should appear. When things appear more
often together then you would expect, then you weigh it more.

To do this, you can count the frequencies of the words in the entire
corpus divided by the total number of \href{../Data/Token.md}{tokens} to
get an expected chance. If you then observe co-occurrence counts with
higher chance than you calculated, you weight these counts higher, as
this might encode the linguistic meaning you are after. You can also
weight the counts which are at chance level lower.

Calculating the PMI of a word \href{Embeddings.md}{embedding}
\(\mathbf{w}\) can be expressed in math as:
\[PMI(\textbf{w},\textbf{c}) = \log_2\frac{p(\textbf{w},\textbf{c})}{p(\textbf{w})p(\textbf{c})} = \log_2\frac{\frac{\text{count}(\textbf{w},\textbf{c})}{V}}{\frac{\text{count}(\textbf{w})}{V}*\frac{\text{count}(\textbf{c})}{V}}\]
Up top you have: The probability of the co-occurrence happening.

At the bottom, you have: You have an independent probability. How likely
are the word and the context to occur together. You get this by
multiplying the probability of how likely the target word is and how
likely the context is when they occur alone.

Here \(V\) is the total number of tokens in the corpus. \(\textbf{c}\)
is the context, \(\textbf{w}\) is the target word.

Now if the PMI is high then the co-occurrence count is probably
interesting because it is higher than chance.

Remember, we calculate PMI to be able to get weight for the association
measure. So we multiply the PMI with the embedding. After that,
calculate the cosine. This resolves the problem where you don't take
into account information from the context. So the PMI is the weight.

Also, you use log to smooth things out when the frequencies get large.

\hypertarget{positive-pmi-ppmi}{%
\subsection{Positive PMI (PPMI)}\label{positive-pmi-ppmi}}

Cosine similarity can take the values between -1 and 1. Where 0 means
nothing in common, 1 means same angle. -1 means opposite direction. No
one has been able to interpret -1 cosine values, so to avoid negative
similarity scores (which we can't interpret) we force all cells in the
vector space to be positive. This is done by saying if something
co-occurs less often than expected by chance, we just say its PMI is 0
which then weights by 0 which result in 0 instead of a negative number.

The things that get the highest PMI are things like names, which don't
occur often and always appear together. Like NEW YORK. Also, colours
like blue and red get a high PMI. So things which are rare which occur
together get high PMI.

You could express this in math as \(PPMI(x) = \text{abs}(PMI(x))\)

\hypertarget{problems-with-pmi}{%
\subsection{Problems with PMI}\label{problems-with-pmi}}

Things which co-occur together more expected than chance get a boost and
things which co-occur less often than change get dragged down (matter
less). But if you have two things which only occur once and also
co-occur together will get a PMI score of 1 as in this case its
\(\frac{1}{1*1}\).

So if two rare things occur together, they have the highest PMI, which
in general favours low-frequency co occurrences generated from
low-frequency targets and contexts.

From this the computer thinks that if the first word appears then the
second word also always appears. However, if a word only appears once,
then this it could be an error and not really display a pattern in the
language.

\hypertarget{example-2}{%
\subsubsection{Example}\label{example-2}}

If the words ``entry'' and ``shell'' only occur once in a
\href{../Data/Corpus.md}{corpus} and also co-occur the only time they do
occur, so like \emph{You enter through the entry shell.} This means that
in theory you get all the information of shell by observing entry. This
will get a PMI of 1. But really the corpus just lacks more occurrences
of entry and shell. You can solve this by adding a bias. There are two
examples of bias below.

\hypertarget{local-mutual-information}{%
\subsubsection{Local mutual
information}\label{local-mutual-information}}

To deal with this you multiply the result of the PMI formula which is
the log frequency of target co-occurrence. This is just taking the log
of how often the co-occurrence appears. The log of 1 is 0 so then all
things which only occur once disappear.

Things which often occur still get boosted because the log of the
absolute occurrence will still be high. So now to get a high PMI score
you need to occur frequently and also often together with something else
which is exactly what we want.

\hypertarget{context-exponent}{%
\subsubsection{Context exponent}\label{context-exponent}}

You can also include an exponent when computing the context probability
such that it increases the probability of rare events.

\hypertarget{pmi-tutorial}{%
\subsection{PMI tutorial}\label{pmi-tutorial}}

How to calculate PMI. Let's say you have a
\href{Vector\%20Space.md}{Vector Space} like this:

\begin{longtable}[]{@{}lllll@{}}
\toprule
& of & the & dog & run \\
\midrule
\endhead
of & 5 & 1000 & 100 & 55 \\
the & 1000 & 400 & 500 & 100 \\
dog & 100 & 500 & 10 & 40 \\
run & 55 & 100 & 40 & 20 \\
\bottomrule
\end{longtable}

And we want to calculate PMI between \textbf{dog} and \textbf{the}?

We want \(PMI(\textbf{dog},\textbf{the})\)

We do this by using the formula
\(PMI(\textbf{dog},\textbf{the}) = \log_2\frac{p(\textbf{dog},\textbf{the})}{p(\textbf{dog})p(\textbf{the})} = \log_2\frac{\frac{\text{count}(\textbf{dog},\textbf{the})}{V}}{\frac{\text{count}(\textbf{dog})}{V}*\frac{\text{count}(\textbf{the})}{V}}\)
So we start with:

\[\log_2\frac{\frac{\text{count}(\textbf{dog},\textbf{the})}{V}}{\frac{\text{count}(\textbf{dog})}{V}*\frac{\text{count}(\textbf{the})}{V}}\]
Remember the numerator is the count of the word and the context
co-occurring divided by V. \textbf{Calculate V at the end if you have
to, and it is not given}. The
\(\text{count} ( \textbf{dog}, \textbf{the} )\) in
\(\frac{ \text{count} ( \textbf{dog}, \textbf{the} )}{V}\) in the
nominator is exactly what the table describes, so it can be directly
read from the table (in this case the dog, the cell). This cell is 500
so that gives:
\[\log_2\frac{\frac{500}{V}}{\frac{\text{count}(\textbf{dog})}{V}*\frac{\text{count}(\textbf{the})}{V}}\]

To get \(\text{count}(\textbf{dog})\) you just sum the \textbf{dog} row.
So \(\text{count}(\textbf{dog}) = 100 + 500 + 10 + 40 = 650\).

For \(\text{count}(\textbf{the})\) and you sum the \textbf{the} column.
You sum the column because \textbf{the} in this case is the context. So
\(\text{count}(\textbf{the}) = 1000 + 400 + 500 + 100 = 2000\). We can
now fill that in to the formula:
\[\log_2\frac{\frac{500}{V}}{\frac{650}{V}*\frac{2000}{V}}\] He said he
will ask if you have to take the \(\log_2\) or not. He also said that
the first word will always be the target word (row) and the second word
will always be the context (column).

\hypertarget{embeddings}{%
\section{Embeddings}\label{embeddings}}

The vectors derived by looking at \href{Context.md}{context} are called
\textbf{embeddings}. This new name is given because you can't directly
interpret the resulting vectors any more, like you can with
\href{Connotations.md}{connotation} vectors. Although embeddings derived
by counting are still very interpretable. It more means that to
interpret one column, you need the other columns. But embedding is
basically a \href{../Languages/Synonyms.md}{synonym} for vector part of
a \href{Vector\%20Space.md}{vector space}. Embeddings are used in the
more recent literature.

There are many ways to derive these vectors which all differ and result
in different sizes. Depending on how the vectors are derived the same
word (the same surface from, as in it could also be tokens) can also
have the different embeddings. However, \textbf{the actual values of the
vector don't mean anything by themselves}. You can't interpret the
numbers like you can with connotations, it is just to find the distance
to other embeddings. The dimensions of an embedding space are
\textbf{not symbolic}.

Below is a 60 dimensional embedding space
\href{Dimensionality\%20reduction.md}{projected} down to 2 dimensions.

\begin{figure}
\centering
\includegraphics{Pasted_image_20220601145459.png}
\caption{A two-dimensional (t-SNE) projection of embeddings for some
words and phrases, showing that words with similar meanings are nearby
in space}
\end{figure}

Using word embeddings seems to work really well in
\href{../Languages/Natural\%20languages.md}{Natural Language} Processing
(NLP) applications as it allows comparing words on a deeper level than
where they appear beyond just \href{../Data/Symbol.md}{symbols} which
are all equally different. With embeddings, words can be compared
through where they appear in the n-dimensional space. Embeddings are
used everywhere because they work really well.

For instance, \href{../Classification.md}{classification} models can
assign sentiment as long as some words seem to have
\href{Word\%20similarity.md}{similar} meanings, i.e.~being close to
other positive sentiment words in the embedding n-dimensional space.

\hypertarget{deriving-embeddings}{%
\subsection{Deriving embeddings}\label{deriving-embeddings}}

\hypertarget{one-hot-encoding}{%
\subsubsection{One hot encoding}\label{one-hot-encoding}}

To change the categorical data into numbers, you can use one hot
encoding. The idea is to make a Boolean dimension features for each
possible category, in this case words. This results in something like
this:

\begin{longtable}[]{@{}lllll@{}}
\toprule
& Cat & Food & Music & Beans \\
\midrule
\endhead
Cat & 1 & 0 & 0 & 0 \\
Food & 0 & 1 & 0 & 0 \\
Music & 0 & 0 & 1 & 0 \\
Beans & 0 & 0 & 0 & 1 \\
\bottomrule
\end{longtable}

This does not really encode the meaning though.

\hypertarget{co-occurrence-1}{%
\subsubsection{Co-occurrence}\label{co-occurrence-1}}

One way to derive embeddings is to use
\href{Co-occurrence.md}{co-occurrence} (counts). The idea is to count
and put in a table how often some words occur in a context with other
words. This encodes meaning because according to the
\href{Distributional\%20hypothesis.md}{Distributional hypothesis}, words
with the same meaning are used around the same words. So words which
mean the same should result in similar vectors.

This is much like how our cognition deals with words. There are much
more ways to get embeddings from words, supervised and non supervised,
with different hyperparameters to tweak. Some might work better for the
problem you are trying to solve than others.

This generates \textbf{\href{Vector\%20Space.md}{Vector Space}}. A
vector space is like an all the generated occurrence count vectors
stacked on top of each other to form a matrix from the vectors. We can
put all the embeddings in a matrix because they all occupy the same
space. All the vectors are the same length.

This is how a vector space might look like (real ones are much bigger):

\begin{longtable}[]{@{}lllll@{}}
\toprule
& Cat & Food & Music & Beans \\
\midrule
\endhead
Cat & 20 & 16 & 2 & 6 \\
Food & 16 & 21 & 1 & 70 \\
Music & 2 & 1 & 16 & 0 \\
Beans & 6 & 70 & 0 & 15 \\
\bottomrule
\end{longtable}

Or like below where the contexts are documents. Knowing something about
the document will also tell you something about the co-occurrences.

\begin{longtable}[]{@{}lllll@{}}
\toprule
& Document 1 & Document 2 & Document 3 & Document 4 \\
\midrule
\endhead
Cat & x & 16 & 2 & 6 \\
Food & 16 & x & 1 & 70 \\
Music & 2 & 1 & x & 0 \\
Beans & 6 & 70 & 0 & x \\
\bottomrule
\end{longtable}

The rows (word at the left) are the target words, and each column keeps
track if a word occurred in the context of that target word.

There are a lot of different variations you can apply when deriving
embeddings. You can leave out the top x most frequent words, as these
don't provide a lot of information, for example.

Usually you also apply an \href{Association\%20measure.md}{association
measure} to in an attempt to bring out more useful information and lower
less important information.

\hypertarget{neural-networks}{%
\subsubsection{Neural networks}\label{neural-networks}}

Instead of using counting to derive embeddings, you can also use machine
learning techniques to derive embeddings. In particular, using
\href{../Prediction/Feed\%20forward\%20neural\%20networks\%20(FFNN).md}{Feed
forward neural networks (FFNN)}. This has the advantage of getting dense
vectors immediately instead of first having sparse embeddings and then
projecting them into lower dimensions like with the count based methods.
A popular way to do this is \href{Word2Vec.md}{Word2Vec}.

\hypertarget{connotations-1}{%
\section{Connotations}\label{connotations-1}}

In the fields of computational linguistics, the word connotation means
the aspects of a word's meaning related to the writer or reader's
emotions. Osgood et al.., 1957 quantified this in 3 dimensions:

\begin{itemize}
\tightlist
\item
  **Valence -- The pleasantness of the stimulus
\item
  **Arousal -- The intensity of emotion provoked by the stimulus
\item
  **Dominance -- The degree of control exerted by the stimulus. Slave is
  low in dominance and master is high.
\end{itemize}

When words score the same across 2 or 3 of these dimensions, you can
maybe assume the words are similar.

The actual values of the connotations have to be gathered by asking a
bunch of humans and then averaging the results. This leads to tables of
words like below:

\begin{longtable}[]{@{}llll@{}}
\toprule
\textbf{Word} & \textbf{Valence} & \textbf{Arousal} &
\textbf{Dominance} \\
\midrule
\endhead
courageous & 8.05 & 5.5 & 7.38 \\
music & 7.67 & 5.57 & 6.5 \\
heartbreak & 2.45 & 5.65 & 3.58 \\
cub & 6.71 & 3.95 & 4.24 \\
\bottomrule
\end{longtable}

This now allows you to represent all these words as a vector of 3
scalers across 3 dimensions. So cub can be represented as {[}6.71, 3.95,
4.24{]}. This is a powerful idea, see
\href{Vector\%20semantics.md}{vector semantics}. One way to compare
words when represented as vectors is with the \href{Cosine.md}{Cosine}
angle between the two vectors, or the
\href{Jaccard's\%20distance.md}{Jaccard's distance}. However, arriving
at these vectors with the connotation's method requires human
annotation.

\begin{quote}
Vectors are basically just lists of numbers
\end{quote}

\hypertarget{context-1}{%
\section{Context}\label{context-1}}

The context are the parts of a discourse that surround a word or passage
and can throw light on its meaning. So the
\href{../Data/Words.md}{words} or \href{../Data/Token.md}{tokens} around
the words or tokens you are analysing.

To learn about the meaning of a word, you can look/count the words
surrounding the words you're interested in (the context). If you do this
with a lot of text, you get an idea of what the context around a
particular word usually is. Then you can compare the context of one word
with the context of another word to see how similar it is. If the
context seems similar, the word probably means something similar. This
can be said based on the
\href{Distributional\%20hypothesis.md}{distributional hypothesis}. From
these counts of words appearing in the context you can also construct a
\href{Vector\%20semantics.md}{vector} which words really well as you can
then compare words based on how \href{Similarity.md}{similar} the
vectors are.

This is very much the same as the \href{../Data/Thesaurus.md}{Lesk
similarity}. There you check the overlap in the glosses of two words in
the \href{../Data/Thesaurus.md}{thesaurus}. But with context, you just
do it with the contexts of words in a \href{../Data/Corpus.md}{corpus}.
Arguably this is better, than for this no human annotation is required.

This approximates meaning. Basically, the computer can never grasp the
meaning of a word, but it can know if two words are likely to mean the
same / are \href{../Languages/Synonyms.md}{synonyms} because the vectors
are close.

Contexts can be any size you want. You can take an entire document as a
context or for instance a sentence.

\hypertarget{similarity}{%
\section{Similarity}\label{similarity}}

The similarity refers to how much overlap there is between two things.
Knowing how similar two things are can be very useful for instance
because you can often substitute similar things with each other.

\hypertarget{word-similarity-1}{%
\subsection{Word similarity}\label{word-similarity-1}}

Using a \href{../Data/Thesaurus.md}{thesaurus,} you can find out how
similar two words are using path length with path length, Resnik
similarity or Lesk similarity. Another way of doing this is
\href{../Languages/Edit\%20distance.md}{edit distance}.

\hypertarget{vector-similarity}{%
\subsection{Vector similarity}\label{vector-similarity}}

There are a large number of ways to calculate the similarity between two
\href{Vector\%20semantics.md}{vectors} or
\href{Embeddings.md}{embeddings}. Here, similarity is quantified by the
distance between points in space. There are a large number of ways to
calculate distance between points in space. The ones discussed in the
course are \href{Jaccard's\%20distance.md}{Jaccard's distance},
\href{Euclidian\%20distance.md}{Euclidian distance} and
\href{Cosine.md}{cosine}.

Jaccard is used to compare \href{Co-occurrence.md}{Co-occurrence} sets.
\href{Euclidian\%20distance.md}{Euclidian distance}, while being a more
intuitive distance measure, it is very much influenced by just one
coordinate of the vector being far removed from another word in the
\href{Embeddings.md}{embeddings space}. Cosine is much better than
Euclidian, as cosine gives prominence to similarity in relative values.

\begin{figure}
\centering
\includegraphics{Pasted_image_20220602232429.png}
\caption{Cosine vs Euclidian}
\end{figure}

In this figure above, if a vector is high in y it occurs often with pet
and high in x occurs high in road. The absolute distance between possum
and monocycle is very small, but the angle is large. While the angle of
cat and possum is more similar, which is also correct.

Here is a more fun image:

\begin{figure}
\centering
\includegraphics{Pasted_image_20220606220120.png}
\caption{Distance measures}
\end{figure}

\hypertarget{smoothing}{%
\section{Smoothing}\label{smoothing}}

Smoothing (in the context of language models) means increasing all the
probabilities of a probability distribution by a little bit, so you
don't have impossible continuations (zero probability). What it does is
that we never say that a probability of a transition or N-gram occurring
is zero. We just make it a small number above zero but never zero.

\hypertarget{the-infinite-problem}{%
\subsection{The infinite problem}\label{the-infinite-problem}}

When training \href{Language\%20Modeling.md}{language models} you always
have the problem that
\href{../Languages/Natural\%20languages.md}{Natural languages} are
infinite. This means that any sample that you train your model on is
actually just a small sample of the entire data or population. It is a
\textbf{finite number} of \href{../Data/Token.md}{tokens},
\href{../Data/Type.md}{types} and \href{../Data/Lemma.md}{lemmas}. This
also means that the it is guaranteed that a model will encounter things
it has never seen before in the future. This will cause the model to say
that a sequence is ungrammatical. Most often this is true and what we
want, but it could be false.

\hypertarget{unknown-words-oov-words}{%
\subsubsection{Unknown words (OOV
words)}\label{unknown-words-oov-words}}

If the model encounters a word it didn't see in training it can't even
start looking for transitions from the preceding
\href{../Languages/N-grams.md}{N-gram}. This can be quantified in the
\href{OOV\%20rate.md}{OOV rate} (percentage of never seen before words
in the test set). Because language is infinite the OOV rate will always
be more than 0 when using the model in the wild.

\hypertarget{zipfian}{%
\subsubsection{Zipfian}\label{zipfian}}

Because natural language follows a
\href{../Languages/Zipfian\%20Distribution.md}{Zipfian Distribution}
there are a lot of words that only appear once in a
\href{../Data/Corpus.md}{corpus} less but still a lot of words that
appear twice in a corpus but not a lot of words that often appear in a
corpus.

Because there are few words that appear often and a lot of words that
appear once or twice in a corpus the sequences that contain rare words
are frequent. So transitions with rare words are frequent but hard to
predict. Because you can't just have of the same words. So how do you
estimate rare transitions?

\hypertarget{solutions}{%
\subsection{Solutions}\label{solutions}}

How do we deal with this all this?

\hypertarget{make-it-a-close-world}{%
\subsubsection{Make it a close world}\label{make-it-a-close-world}}

We pretend that an open vocabulary situation is a closed vocabulary
problem where we know all the possible words in advance by replacing
some words in training (usually the rare ones) with a single token, for
instance called \textbar UNK\textbar. Then, with this token you make the
maximum likelihood estimates the usual way. Now in the future you can
just replace all the unknown words in a new sentence with
\textbar UNK\textbar{} and you can use your model again with any
sentence. Pretty clever I would say. This is basically normalization by
replacing all the unknown words with the unknown words token. Kind of
like replacing all the emails addresses with the EMAIL token.

Basically we say rare things are the same. We leverage the relationship
between rare things that they are rare.

You can look at this by saying that you reserve some probability mass
for unknown words.

\hypertarget{the-zero-probability-transition-problem}{%
\subsection{The zero probability transition
problem}\label{the-zero-probability-transition-problem}}

Replacing words that the model hasn't seen before with a special token
solves part of the problem but what if the model encounters a transition
that it has not seen before? This means an N-Gram that doesn't appear in
the transition matrix. Then the model will think that the probability of
it occurring is zero. As soon as this happens the model will get stuck
because \href{Markov\%20models.md}{Markov models} uses the Markov chains
which means that if you multiply by 0 or take the log of 0 you get
problems. So we can not afford the probability of a word or transition
to be 0.

\begin{figure}
\centering
\includegraphics{Pasted_image_20220224152619.png}
\caption{Probability distributions before smoothing}
\end{figure}

In this example the continuation painful, horrible and boring is zero
and that is a problem.

To solve this zero probability transition problem we apply
\textbf{smoothing}.

If we were to apply smoothing on the model from the picture from before
we would get:

\begin{figure}
\centering
\includegraphics{Pasted_image_20220224152946.png}
\caption{Probability distributions after smoothing}
\end{figure}

Now everything is above zero.

Of course there are multiple algorithms to do smoothing with different
results.

\hypertarget{laplace}{%
\subsubsection{Laplace}\label{laplace}}

The Laplace smooth method just adds 1 to all frequency counts of words
before normalization. We have to be sure that we can still turn the
frequencies into a probability distribution.

For transitions this means that transitions with a frequency of 0 in the
training corpus get a frequency of 1, transition with a frequency of 1
get a frequency of 2, \ldots{} In maths this looks like
\[c^{*}= (c+1)\frac{N}{N+V}\] Where N is the number of tokens, V is the
number of types and c is a count of how often something occurs in the
training data.

In the lecture Laplace smoothing is called a quick and dirty solution.
Because with large vocabularies and not so high frequencies, smoothed
probability are too different from the non-smoothed ones. This is
because there are only 100 percentage points to give out and even if we
assign a little probability to unobserved transitions, there are still
really a lot of unobserved transitions. This makes it so that the
probability distribution shifts a lot. This is captured in the image
below:

\begin{figure}
\centering
\includegraphics{Pasted_image_20220224154616.png}
\caption{Result of Laplace parsing}
\end{figure}

We want that what we don't observe gets a small probability but because
there are so many things we didn't observe adding 1 to every count makes
what we didn't observe a lot more likely than we would like.

\hypertarget{add-k}{%
\paragraph{Add K}\label{add-k}}

An improvement is to decrease the amount of probability mass which gets
moved around by adding less than 1 to each count. We can just add 0.001
or something in the probability. Now the smoothing looks like this:
\[p^{*}(W_{n}|W_{n-1}) = \frac{c(W_{n-1}\cdot W_{n}) + k}{c(W_{n-1}) + k \cdot V}\]

This means adding less than one. Apparently you have to do k on both
sides otherwise it is not a probability any more.

But really this is just a hack upon a hack. Estimates have too little
variance and are still off.

\hypertarget{interpolation-smoothing}{%
\subsubsection{Interpolation Smoothing}\label{interpolation-smoothing}}

We can try to look at more sources of data and assigns weights to the
data which indicates how sure we are about it. Higher weights to more
reliable sources of knowledge.

\hypertarget{linear-combinations}{%
\paragraph{Linear combinations}\label{linear-combinations}}

This interpolation is done with linear combinations. The idea is to
always also consider transitions for smaller n-grams using a weighted
linear combination of the probabilities. So you look at other N-grams
and assign a weight to each different type of N-gram which indicates how
much you want to rely on the data. The weights are called \(\lambda\)
here. This causes the formula for a probability of a transition in a
language model to look like this (When considering trigrams):
\[\hat{p}=(w_i|w_{n-2}, w_{n-1}) = \lambda_1(w_i|w_{n-2}, w_{n-1}) + \lambda_2(w_{i}|w_{n-1}) + \lambda_3(w_i)\]

This math translates to:

the probability of a word given the previous word and the previous word
= the probability of the word given the previous word and the previous
word * a weight + the probability of the word given the previous word *
a weight + the probability of the word alone * a weight.

Everything on the right comes from the probability distribution.

So you assign weights to each source of information, telling you
something about the probability. This allows you to consider different
N-grams because we can add another constrained:
\(\lambda_{1}+ \lambda_{2} + \lambda_{3} = 1\). Of course, the amount of
lambda's depends on the N you choose for your N-Grams.

Now the trick is that \(\lambda_3(w_i)\) is never zero because we are
still applying the closed word assumption. If we don't know the word, it
turns into \textbar UNK\textbar{} and that (also) has a probability of
occurring. Just like the probability of other words. So as long as
\(\lambda_3\) is not zero, this formula with linear combinations will
never be zero. Even if the transitions are 0.

\hypertarget{back-off}{%
\subsubsection{Back off}\label{back-off}}

Backof is an algorithm that instead of doing weights reclusively uses
the best source of information it can find. So if there is an 4-Gram use
that, but if there is not then go to 3-gram etc until, if you have to,
use probability of the word and disregard the context. This never fails
because of the \textbar UNK\textbar{} token if it is needed.

Anytime a transition from an n-gram to a state found in the test set
didn't occur in training, recursively fall back on to the smaller
n-grams (For example from ``why did you'' to ``did you'' to ``you'')
until a non-zero transition is found.

This sounds really good. But it does mean that you no longer have
probabilities because you no longer have the thing that the lambdas add
up to 1.

\hypertarget{using-ffnn-for-language-modelling}{%
\section{Using FFNN for language
modelling}\label{using-ffnn-for-language-modelling}}

You can also use
\href{Feed\%20forward\%20neural\%20networks\%20(FFNN).md}{Feed forward
neural networks (FFNN)} for language modelling. These models make the
same \href{Markov\%20assumption.md}{Markov assumption} as
\href{Markov\%20models.md}{Markov chain models}. So the neural network
is tasked with finding the preceding word to the n-gram it gets as
input. FFNN seem to work better than models based on a large matrix of
probabilities.

The neural networks are very popular because of the following
advantages:

\hypertarget{advantages-of-ffnn}{%
\subsection{Advantages of FFNN}\label{advantages-of-ffnn}}

\begin{itemize}
\tightlist
\item
  No need to apply \href{Smoothing.md}{Smoothing}
\item
  Can handle longer histories (\href{../Languages/N-grams.md}{N-gram}
  with large n)
\item
  Generalize better over histories consisting of similar but not
  identical words.
\end{itemize}

The main reason why this approach works and the reason for these
advantages is because
\href{../Semantic-Similarity/Embeddings.md}{embeddings} are used to
represent the words. Embeddings are an antidote to sparsity. A better
antidote than smoothing. They also allow the model to deal with n-grams
they have not seen before by just comparing them with vectors which are
\href{../Semantic-Similarity/Similarity.md}{similar} (are close together
in the \href{../Semantic-Similarity/Vector\%20Space.md}{Vector Space}
and thus probably have similar meaning) and just using those vectors.

\hypertarget{example-3}{%
\subsubsection{Example}\label{example-3}}

So for example if the model has not seen the n-gram ``the silver pike''
but it does have the word embeddings for ``the'', ``silver'' and
``pike''. The model can look for similar embeddings for all of these
words for which it has seen an n-gram. So a close embedding to
``silver'' is ``blue'' and a close embedding to ``pike'' is ``fish''.
Then the model has seen the engram ``the blue fish'' before, and the
most likely next token after that was stored as ``swims''. So based on
all that, the model will assign a high probability to ``swims'' as the
next word.

\hypertarget{what-does-it-mean}{%
\subsection{What does it mean}\label{what-does-it-mean}}

So you can see that you can use embeddings to switch out words which you
the model doesn't know much about to other words which the model knows
more about. This removes the need to move probability space around like
with smoothing.

You can use higher n with the n-grams when using embeddings. This
increases the change that you get an N-gram you haven't seen because,
but the neural network models can deal with this by switching the tokens
you have not seen before with the closest embeddings you have seen
before. With \href{Markov\%20models.md}{Markov chains}, when you
encounter an N-gram you have not seen before you are basically out of
luck and you have to use \href{Smoothing.md}{smoothing}. However, you
can still not increase N by a huge amount as you will still need larger
corpora when you do and there is a limit as you will run in to the
language is infinite problem.

\hypertarget{how-to-get-embeddings}{%
\subsection{How to get embeddings}\label{how-to-get-embeddings}}

You can get off the shelves embeddings which have been trained by
someone else. These you can also fine tune to your specific problem. You
can of course use any corpus that you want. The N-grams of some corpus
might work better for your problem. You can also derive your own
embeddings of course. The embeddings derived by the big companies will
be much larger than you can probably ever really make.

\hypertarget{can-we-get-rid-of-the-markov-assumption}{%
\subsection{Can we get rid of the Markov
assumption}\label{can-we-get-rid-of-the-markov-assumption}}

The \href{Markov\%20assumption.md}{Markov assumption} \textbf{limits the
size of context arbitrarily} by specifying only one n in the n-grams.
Sometimes however we care about short windows and sometimes we care
about longer windows. But so far we have always been using the same n
while saying that a higher n will lead to better predictions (if you
have enough data to back it up). Recurrence can be used to overcome this
problem.

FFNN still assume the Markov assumption. We can use
\href{Recurrence.md}{recurrence} to overcome this problem. This leads to
recurrent neural networks.

\hypertarget{perplexity}{%
\section{Perplexity}\label{perplexity}}

Perplexity is the standard way to intrinsically evaluate whether a
predication system is working. Perplexity computes \textbf{how
surprised} the system is of seeing what it is actually sees in the light
of what it expected to see given what it knows.

What the prediction system expects is caused by the training data.

\hypertarget{more-formally}{%
\subsection{More formally}\label{more-formally}}

At any new token in the data, the language model outputs a probability
for every possible type as a continuation given the previously observed
history and the transition probability matrix it
\href{../Other/Learning.md}{learned} on some other data.

\textbf{The higher the probability a model assigns to new valid
sentences, the better the language model, the lower the perplexity.}

Perplexity based cased by the interaction of the model and the test set.
But really it is caused by the test set because the model is a
probability distribution matrix over the test set. This means that you
can say that perplexity is the \textbf{inverse probability} of the test
set under a language model, \href{../Data/Normalization.md}{normalized}
by the number of tokens (the more tokens there are, the lower the final
probability of a sequence). In maths this is:

\[pp(W) = 2^{-l}\] where
\[l = \frac{1}{|w|}\sum\limits^{|W|}_{i=1}\log p(w_i|w_{i-i-n~:~i-1})\]
with \(w_{i} \in W\) where W is a sequence of tokens. We use log, so we
can use sum (\(\Sigma\)) for calculating the probability. This avoids
underflowing, as the probabilities can get really small.

So basically you have to normalize the perplexity to compare language
models. Which means that you can only compare the complexity of models
that use the same test set.

Since we're taking the inverse probability, a~\textbf{lower
perplexity}~indicates a~\textbf{better model}.

\hypertarget{best-practices-for-evolution}{%
\subsection{Best practices for
evolution}\label{best-practices-for-evolution}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Estimate the \href{Language\%20Modeling.md}{language model} (states
  and transition matrix) on some corpus.
\item
  Fine tune the model on different corpus (test data)
\item
  Test the model to check how it fits on new data (validation data).
\end{enumerate}

NEVER test on the same data you trained on and NEVER validate on the
test data! \textbf{\href{../Other/Learning.md}{Learning} is not
remembering.} If it were, it wouldn't be useful.

Again, you can only compare perplexity of different models if you use
the same test set. Because otherwise the probability distribution is not
the same. So the result states have to be the same.

Perplexity is an average, so it depends on the number of tokens in the
vocabulary.

\hypertarget{recurrent-neural-networks-rnn}{%
\section{Recurrent neural networks
(RNN)}\label{recurrent-neural-networks-rnn}}

A recurrent neural network is a neural network which uses
\href{Recurrence.md}{recurrence} to take into account previous states.
Instead of relying on n-grams and the
\href{Prediction/Markov\%20assumption.md}{Markov assumption,} there is a
\textbf{chain of recurrence}. The idea is to update some hidden layers
in the neural network based on values of other (later) hidden layers.

Instead of parsing the input into \href{Languages/N-grams.md}{N-grams}
and giving those as input, you select an input sequence and give the
network one token of the sequence at a time. Each time, the hidden
layers will update and update the previous layers based on the further
layers. This causes previous words to influence the current prediction
through recurrence. The words you feed into the model before you make
the prediction are called the \textbf{chain of recurrence}. The chain of
recurrence usually (almost) goes back to the beginning of a sequence,
which is typically a sentence. We say almost because the most recent
words affect the current state more than the words further away.

\hypertarget{advantages-of-rnn}{%
\subsection{Advantages of RNN}\label{advantages-of-rnn}}

The chain of recurrence allows avoiding the
\href{Prediction/Markov\%20assumption.md}{Markov assumption} as the
hidden layers encodes information from all states (more about the more
recent ones) that you give it. So with RNN you can use sequences of
arbitrary length instead of a fixed n because the RNN don't rely on
n-grams existing in the corpus. It works because you feed parts of the
sequence and the model just updates and updates until you want to make a
prediction. However, if you make the sequences too long, you run into
the \href{Vanishing\%20gradient\%20problem.md}{vanishing gradient
problem}.

This is great because then you can also use different lengths of input
once you have a model.

You don't have to do smoothing with RNN because you can represent the
input words to an RNN as
\href{Semantic-Similarity/Embeddings.md}{embeddings} which project words
into a \href{Semantic-Similarity/Vector\%20Space.md}{continues space}
where we can leverage
\href{Semantic-Similarity/Similarity.md}{similarity} relations to guess
how our new n-gram should behave in case you did not see it yet. Also,
because you input one token at the time, the chance of finding words you
have not seen yet is smaller. So is it not RNN themselves which avoid
the smoothing, but the combination of RNN and embeddings.

\hypertarget{a-new-set-of-weights}{%
\subsection{A new set of weights}\label{a-new-set-of-weights}}

To make recurrent neural networks a new type of weights is needed. A RNN
normally consists out of 3 sets of weights. - The weights which connect
the input to the hidden layer (W) - The weights \textbf{between the
previous hidden state and the current hidden state} (U). - The weights
between the current hidden state and the output (V)

The W and V are the same as with
\href{Feed\%20forward\%20neural\%20networks\%20(FFNN).md}{FFNN} and the
U weights are added.

There is also a fourth set of weights \textbf{of the embedding layer
(E)} which connects the input to the embedding layer, which are then
connected to the hidden layer through the set of weights W. The
embedding layer is optional but almost always added.

The rest is the same as with FFNN with a loss function, gradient descent
and back propagation. However, the back propagation is a bit different,
it is back propagation through time.

\begin{figure}
\centering
\includegraphics{RNN.png}
\caption{RNN}
\end{figure}

\hypertarget{making-predictions}{%
\subsection{Making predictions}\label{making-predictions}}

So the output (the production) depends on two things: - The embeddings
of the current input word - The hidden layer as influenced by the
previous step(s)

This is the
\href{Classification/Native\%20baiyes/Bayes\%20rule.md}{Bayes rule}
intuition, where with the current evidence and the previous evidence you
make up your mind about the future.

This information is coming from the current input (embedding) and the
hidden layer is summed to obtain a new hidden layer which is transformed
using the
\href{Feed\%20forward\%20neural\%20networks\%20(FFNN).md}{softmax}. This
then gives you probabilities for different classes. This could either be
a probability for each word in the language indicating how likely it is
to be next or for instance a
\href{Languages/Parts\%20of\%20Speech.md}{Parts of Speech tag} or any
other label. These probabilities change every time you input another
embedding (you say they change at every time step). When predicting
words, you at some point stop giving input, and you take the class with
the highest probability. Of course, if you sum these probabilities, you
get 1.

When predicting tags or labels, you will need to have the correct label
for training, as the loss function needs to know what the correct answer
is. This requires annotation, which is expensive. With language
prediction you don't need annotation because you can just use an
existing text to have a correct answer. This kind of prediction can also
be done with
\href{Feed\%20forward\%20neural\%20networks\%20(FFNN).md}{FFNN} however
they don't remember the context.

\hypertarget{stacking-layers.}{%
\subsection{Stacking layers.}\label{stacking-layers.}}

We can stack as many recurrent hidden layers as we want (but the more,
the slower the training). You can also stack the neural networks
themselves, just like with
\href{Classification/Logistic\%20Regression.md}{Logistic Regression}. In
this case, you could use the entire sequence of outputs from one RNN as
the input to a next RNN or different type of neural network. The Neural
networks are mostly \textbf{self-contained modules} which can be
combined in an infinite number of ways, however you should motivate
layer choices because the bigger the NN the more expensive running it
gets.

In essence stacking computes a slightly more abstract version of the
input than the last version. This is especially useful if you have noisy
data. We would like the network to learn how to use the abstract feature
bundles from the input. The lower layers are tuned towards something
closer to the signal, while the higher layers are tuned to more and more
abstract features. This is also how your brain does it, with vision and
sound going through multiple layers.

\hypertarget{bidirectional}{%
\subsubsection{Bidirectional}\label{bidirectional}}

RNN do not only go forward (left to right) but they can also go in the
reverse. You can decide in which order you process the input sequence.
You can go left to right or right to left. You could even have read
middle out if you want. This will result in different dependencies.

Nothing prevents us from having \textbf{two RNNs which read the input
sequence in different directions and then combine their higher hidden
layer} to encode a single representation of a sequence. This combines
reading left to right and right to left, for instance, into a single
representation. There are many ways to combine the representation of
hidden layers. For instance, concatenation or element wise
sum/multiplication. RNN that do this called \textbf{bidirectional}.

\hypertarget{back-propagation-through-time}{%
\subsection{Back propagation through
time}\label{back-propagation-through-time}}

A different back propagation is needed as to make a prediction we have
to run the model multiple times. Normally with backpropagation it
happens after you have just run the model once.

It works by in the forward pass, going through the whole sequence and
keeping track of the loss. Only at the end of the input sequence do you
start the backwards pass and \textbf{go all the way back, processing the
sequence in reverse}. At every step, you compute the required error
terms gradients (with derivatives) and save the error to compute the
gradients at the next (which is the previous) step.

So it sort of like a step up from normal back propagation because you
have to include make an improvement while the model ran multiple times.

\hypertarget{vanishing-gradient}{%
\subsection{Vanishing gradient}\label{vanishing-gradient}}

One problem with RNN is the
\href{Vanishing\%20gradient\%20problem.md}{vanishing gradient problem}.
If the sequence which is processed by the model step for step is very
long, the gradients in backpropagation through time are multiplied
several times (as many times as words in the sequence) and may end up
being zero. The smaller the gradients, the smaller the change in the
weights. \textbf{If the gradients hit zero, the changes to the weights
to stop}.

There are variants of RNN like LSTMs and GRUs which modify the recurrent
layers to handle the vanishing gradient by \textbf{forgetting} some
information, which won't be carried on to the later input steps (and
hence doesn't influence gradient computation). By forgetting, this
information doesn't have to be carried to later input steps, and this
reduces the vanishing gradient issue to some extent.

There is another good further explanation found by Ethel of LSTM
\href{https://colah.github.io/posts/2015-08-Understanding-LSTMs/}{here}.

\hypertarget{applications-of-rnn}{%
\subsection{Applications of RNN}\label{applications-of-rnn}}

Apparently in 2017 bidirectional LSTM (with attention) were the state of
the art for almost every task in NLP. They are still very much used, but
now the state of the art is
\href{Prediction/Transformers.md}{transformers}.

\hypertarget{probability-of-the-sequence}{%
\subsubsection{Probability of the
sequence}\label{probability-of-the-sequence}}

You can get the probability of the input sequence itself. You do this by
making the model predict the next word for every token of the sequence
you give to the model. Then you take the predicted probability for the
class of the correct next token and store it, calculated with softmax.
You know the correct next token because you have the sequence. Do this
for the entire sequence, and then you multiply the probabilities or sum
the logs to get the probability of the entire sentence under the trained
language model.

So the \textbf{product (or sum of log) of the probabilities of each
correct continuation under the softmax} is the probability of the whole
sequence under the trained language model.

\hypertarget{autoregressive}{%
\subsubsection{Autoregressive}\label{autoregressive}}

When a model is autoregressive it means it can generate new language.

This is done in the following way:

\begin{itemize}
\tightlist
\item
  Take the embedding of the beginning of sequence tag (BOS) as input to
  the RNN. Take the word with the highest softmax probability, or sample
  from the top x words.
\item
  Take the embedding of the generated word, run the network on it, and
  again take the word with the highest probability (or sample the top x)
  in the softmax.
\item
  Repeat till EOS (end of sequence symbol)
\end{itemize}

When doing this you \textbf{only need 1} beginning of sequence symbol
and not multiple like with
\href{Prediction/Hidden\%20Markov\%20Models.md}{Hidden Markov Models}.
This is because the hidden layers will `remember' that it has seen a
beginning of sequence symbol.

\hypertarget{predicting-pos-tags}{%
\subsubsection{Predicting Pos tags}\label{predicting-pos-tags}}

Like said above, you can use RNN to predict labels. You can also use
them for predicting \href{Languages/Parts\%20of\%20Speech.md}{Parts of
Speech} tags. However, the performance is not much better than with
\href{Prediction/Hidden\%20Markov\%20Models.md}{Hidden Markov Models}.

\hypertarget{named-entity-recogntion-and-labeleing}{%
\subsubsection{Named entity recogntion and
labeleing}\label{named-entity-recogntion-and-labeleing}}

This task is about finding and appropriately labelling words which
denote entities (countries, people, organizations \ldots. ). So, for
instance, you get the sentence ``New York is a big city''. The model
should find out that New York is both a named entity and also the same
named entity and also a city.

\hypertarget{structure-prediction}{%
\subsubsection{Structure prediction}\label{structure-prediction}}

This task is about, given an input, producing the correct set of actions
to achieve the desired output. Think of Alexa or Google assistants. RNN
are useful for this because of the longer histories which can be used.

\hypertarget{sequence-to-label}{%
\subsubsection{Sequence to label}\label{sequence-to-label}}

You can use RNNs to classify whole sequences as something. These models
are called Seq2Label).

To do this you run the model through a whole sequence and add the end
\textbf{the hidden layer will encode a representation of the whole
sequence} (more from the end). So like deriving an embedding for an
entire sequence basically. You can then put simple
\href{Classification/Logistic\%20Regression.md}{Logistic Regression} on
top and train it to correctly classify these representations. The loss
now refers to the class of the whole sequence. This is not a generative
model, but a discriminative model.

\hypertarget{sequence-to-sequence}{%
\subsubsection{Sequence to Sequence}\label{sequence-to-sequence}}

Sequence to Sequence (seq2seq) is~\textbf{a model that takes a sequence
of items and outputs another sequence of items}. Rather than emitting a
label at the final step, you give out another sequence.

You try to predict a sequence from another sequence without necessarily
having a one to one mapping between units in both sequences. I think
these are the most interesting problems, like translation or summarizing
text.

These types of models are usually made out of two connected subnetworks.
The first \textbf{encodes the source} sequence (input) in a hidden state
which yields an embedding. The second
\textbf{\href{Decoding.md}{decodes} the representation} from the hidden
state into the target sequence. Training happens by jointly updating the
weights of the encoder and the decoder in such a way that the decoded
output resembles the target output as closely as possible.

You might see this as a model taking in the label from a seq2label model
and then predicting a sequence from that.

Nowadays, the underlying neural network architecture of these has
changed, but the idea is still the same.

\hypertarget{hidden-markov-models}{%
\section{Hidden Markov Models}\label{hidden-markov-models}}

Hidden Markov Models (HMM) are like a \href{Markov\%20models.md}{Markov
model}, but we also care about hidden states. In PoS tagging we don't
observe the states we want to predict as we do in
\href{Language\%20Modeling.md}{language modeling}. Basically, the
\href{../Languages/Parts\%20of\%20Speech.md}{PoS} tags are hidden. We
observe a sequence of words and want to find the best sequence of tags
for that particular sequence of words out of all possible sequence of
tags.

A Hidden Markov Model (HMM) consists of the following components: - Q --
A finite set of N states (hidden). - A -- A state transition probability
matrix. - \(\pi\) - An initial probability distribution. - O -- A
sequence of T observations. - B -- An observation likelihood matrix.
Probability for a specific observation.

The last 2 of these are different from normal Markov models. But the
difference is that we use Q, A and \(\pi\) for the hidden states. So the
states in Q are not visible on the surface any more. With O and B we
encode what we know or what is visible. We do know that Q contains BoS
and EoS.

\hypertarget{components}{%
\subsection{Components}\label{components}}

\hypertarget{q}{%
\subsubsection{Q}\label{q}}

Rather than consisting of words or engram, Q consists of PoS tags. So, Q
contains the states of the HMM. This also includes BoS and EoS.

\hypertarget{a}{%
\subsubsection{A}\label{a}}

A is a \textbar Q\textbar-by-\textbar Q\textbar{} matrix, where each
cell \(A_{ij}\) indicates the probability of moving from state
\(i \in Q\) to state \(j \in Q\). This means that we need some corpus
with annotated data where we can observe PoS tags.

We can do the estimation using maximum likelihood estimates. The formula
of that is:
\[p(t_{i}, t_{i-n:i-1}) = \frac{c(t_{i-n:i-1}, t_{i})}{c(t_{i-n:i-1})}\]
c is a count function. So you divide the frequency of the current tags
by the frequency of the preceding tags. That gives you the probability
of the tag given the preceding tags.

\hypertarget{pi}{%
\subsubsection{\texorpdfstring{\(\pi\)}{\textbackslash pi}}\label{pi}}

Similarly to the \href{Markov\%20models.md}{Markov Chain} \(\pi\)
encodes the probability that each state \(q\in Q\) follows the BoS
symbol. \(\pi\) can be fixed in advance if you want to put constraints
on what can come at the start, or you can estimate it from a corpus. So
instead of words, is about tags.

\hypertarget{o}{%
\subsubsection{O}\label{o}}

The set of observed events: In PoS tagging, O contains words. This set
has to be finite, otherwise we can not finish. Every observation in O
has to be able to be tagged from a state in Q.

\hypertarget{b}{%
\subsubsection{B}\label{b}}

B is another matrix. It is of size
\textbar Q\textbar-by-\textbar O\textbar, where each cell \(b_{qo}\)
indicates the probability that a word \(o \in O\) is generated by a
state \(q \in Q\).

To compute B, we need to find out how often each word occurs tagged with
a particular PoS tag in a corpus. Again this needs annotated data.

So B encodes the probability that a certain word occurs since we
observed a certain tag. Given that we observed a noun, how likely is it
that this noun is exactly dog for instance and not aardvark.

This is also calculated with ML:
\[p(w_{i}|t_{i}) = \frac{c(t_{i},w_{i})}{c(t_{i})}\]

This divides the number of times a specific word is tagged as a certain
tag.

So we want to know the likeliest tag for a word, but we compute the
likeliest word after observing a tag. This is like
\href{../Classification/Native\%20baiyes/Bayes\%20rule.md}{Bayes rule}.
We aim for the posterior but compute the likelihood and the prior, and
then estimate the posterior.

\hypertarget{assumptions}{%
\subsection{Assumptions}\label{assumptions}}

Both of these assumptions simplify the problem to make it possible to
compute.

\hypertarget{markov-assumption}{%
\subsubsection{Markov Assumption}\label{markov-assumption}}

The probability of the next tag is only determined by the local history,
not the whole sequence.

\hypertarget{output-independence}{%
\subsubsection{Output independence}\label{output-independence}}

The probability of a word only depends on the state that produced the
corresponding tag, not on any other state or word.

\hypertarget{vanishing-gradient-problem}{%
\section{Vanishing gradient problem}\label{vanishing-gradient-problem}}

When using \href{Recurrence.md}{recurrence}, the effect an input has on
the current prediction lowers the further it is in the past. This causes
the \textbf{vanishing gradient problem}. This is not so noticeable for
short to medium length sequences, but for long sequences, RNN will stop
improving. The reason for this is that with the backpropagation trough
time algorithm, the gradients of the earliest parts of the sequence go
to 0. Basically, the further in the past a part of a sequence is, the
less impact it has on the current prediction. If you go too far away,
the impact will become 0 and back propagation stops working. STM \& GRU
architectures mitigate this issue by forgetting about parts of sequence
too far in the past, but this causes information to be lost through
recurrence.

\href{Transformers.md}{Transformers} attempt to solve this issue without
forgetting.

\hypertarget{markov-assumption-1}{%
\section{Markov Assumption}\label{markov-assumption-1}}

The Markov assumption says (in the language modelling domain) that the
conditional probability of a word appearing next in the sequence can be
approximated by looking at its local history instead of the entire
history. In this case the preceding history is an
\href{../Languages/N-grams.md}{n-gram} of words which indicates how far
you look back. This can be expressed with maths:
\[p(W_{1},...,W_{m}) \approx \prod^{n}_{i = 1}(p(W_{i}|W_{i-n~:~i-1}))\]
Here W is a sequence of words. \(W_1\) is the first word in this
sequence. \(W_m\) is the last word in this sequence. We don't explicitly
define m to indicate that the sequence can be arbitrarily large. You can
approach (\(\approx\)) the probability (\(p\)) of the sequence \(W\)
appearing by multiplying (\(\prod\)) the probability of n words which
came before it.

The sequence of symbols you consider trying to predict the next symbols
is called the \href{../Semantic-Similarity/Context.md}{context}. The
context is defined by \href{../Languages/N-grams.md}{N-grams}. The
bigger the N in N-gram the bigger the context you consider.

So basically don't look at the entire history, but just look at the
context (an N-gram at the end).

\hypertarget{bigram-model}{%
\subsection{Bigram model}\label{bigram-model}}

With the Bigram model, you take N=2. Only look at the preceding word as
history and then find the probability of the next word.

So if you use this model and try to predict what comes after ``a tree
has'' and you look at ``leaves'' then you would get
\(p(\text{has}|\text{leaves})\). You would get
\(p(\text{a tree has}|\text{leaves})\) if you were to consider the
entire history (a 4-gram in this case).

So here we are working under the assumption:
\(\text{p}(\text{a tree has}|\text{leaves}) \approx \text{p}(\text{has}|\text{leaves})\)
this assumption is the Markov assumption.

The nice thing here is that ``has leaves'' is much more likely to appear
in a \href{../Data/Corpus.md}{corpus} than ``a tree if leaves''. This is
good because if your n-gram does not appear at all in the corpus, the
probability will be 0 and then all is lost.

\hypertarget{pros}{%
\subsubsection{Pros}\label{pros}}

\begin{itemize}
\tightlist
\item
  Easy to estimate transitions, reduces sparsity.
\item
  We can use smaller corpora.
\item
  With bi-grams, the chance that you don't find the n-gram in the corpus
  is the smallest as possible.
\end{itemize}

\hypertarget{cons}{%
\subsubsection{Cons}\label{cons}}

\begin{itemize}
\tightlist
\item
  Throws away a lot of information, as can be seen above. Tree gives a
  lot of information, much more than has. However, you could filter
  words like has out with \href{../Data/Normalization.md}{normalization}
  because that in general does not give a lot of information.
\end{itemize}

\hypertarget{maximum-likelihood}{%
\subsubsection{Maximum likelihood}\label{maximum-likelihood}}

The probabilities that we compute are maximum likelihood estimates. So
if we wanted to do has leaves, it would be:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Find and record the occurs of ``has'' followed by the word ``leaves''.
\item
  Divide by the frequency count of all the bigrams that start with
  ``has'' regardless of what follows (frequency of ``has'').
\end{enumerate}

If we do this, then we maximize the likelihood of the corpus used to
collect the counts. This means that given the model, the input corpus is
the likeliest set of sequences we could expect in the
\href{Language\%20Modeling.md}{language model}. This makes sense because
the data that the model is based on decides the probability
distributions.

\hypertarget{how-to-choose-n}{%
\subsection{How to choose n?}\label{how-to-choose-n}}

Choosing n is a trade of between being able to predict and the
information needed. In practical applications, trigrams (or higher)
models are usually preferred to bigram models as they provide a larger
context and practical application often use larger corpora because they
can afford it. Because if you increase n, you really need a much larger
corpus to be able to predict things.

The higher the n the more fine-grained the information, the weaker the
Markov assumption, and the more severe the sparsity.

\hypertarget{recurrence}{%
\section{Recurrence}\label{recurrence}}

Recurrence is the idea of allowing the\textbf{previous state of a model
to influence the current state}, rather than using the previous
\href{../Languages/N-grams.md}{N-grams} to
\href{../Prediction/Prediction.md}{predict} the next event based on the
\href{Markov\%20assumption.md}{Markov assumption}. When using recurrence
in a \href{Feed\%20forward\%20neural\%20networks\%20(FFNN).md}{neural
networks} it becomes a
\href{../Prediction/Recurrent\%20neural\%20network\%20(RNN).md}{recurrent
neural network (RNN)}.

Thanks to \textbf{recurrence}, the value of a hidden unit doesn't simply
reflect the weighted sum of the input, but also \textbf{reflects the
previous hidden state}.

The idea is that you update a hidden layer not only based on the current
input, but also based on another hidden layer from previous
\href{Prediction.md}{predictions}.

\begin{figure}
\centering
\includegraphics{RNN2.png}
\caption{Recurrence in action}
\end{figure}

Using recurrence suffers from the
\href{../Vanishing\%20gradient\%20problem.md}{vanishing gradient
problem}.

\hypertarget{oov-rate}{%
\section{OOV rate}\label{oov-rate}}

When testing a \href{Language\%20Modeling.md}{language model} there
might be words in the testing set that the language model has never seen
before. This can be expressed in the OOV rate. The OOV rate is the
percentage of words in the test set which never occur in training. If it
is high, there is a representability problem.

\hypertarget{overfitting}{%
\section{Overfitting}\label{overfitting}}

Overfitting is when you make a supervised
\href{../Other/Learning.md}{learning} model too focused on the training
data that you have. When this happens, your model will be great at
predicting your training data, but it will suck at predicting data that
you have not seen before, especially if it is close to the decision
boundary.

This can generally be seen in big difference in training scores and test
scores, where training scores are high and test scores a low.

\begin{figure}
\centering
\includegraphics{Pasted_image_20220216115427.png}
\caption{Overfitting example 1}
\end{figure}

\begin{figure}
\centering
\includegraphics{Pasted_image_20220216115205.png}
\caption{Overfitting example 2}
\end{figure}

When you have overfitted your model can predict the training data
extremely wel, but it is not generalizable to new data.

\hypertarget{preventing-overfitting}{%
\subsection{Preventing overfitting}\label{preventing-overfitting}}

You can prevent overfitting by splitting your data into different
groups. You take the biggest part of the data as the \textbf{training
data}. The data that the model actually uses to make predictions. Then
you have the \textbf{test data or development data}, which you use to
access the models' performance during training. When this is done you
need to have a third set of data, the \textbf{validation set}, to check
if you didn't overfit on the test data.

In the knowledge clip, the 3 sets are called differently. See the image:

\begin{figure}
\centering
\includegraphics{Pasted_image_20220216120004.png}
\caption{Prevent Overfitting by splitting data in a training set,
development set and test set}
\end{figure}

\hypertarget{k-fold-cross-validation}{%
\subsection{K-fold cross validation}\label{k-fold-cross-validation}}

Here you split the data into k portions of the same size, then
iteratively train on k-1 sub sets and test on the remaining sub-set.
Then you average scores of the k runs. K is typically like 5 or 10.

\hypertarget{prediction}{%
\section{Prediction}\label{prediction}}

Closely related to
\href{../Classification/Classification.md}{classification} in language,
but prediction is trying to anticipate \textbf{what comes next given
what has happened before} rather assigning a class to an instance.

The difference between classification and prediction is that with
classification you try to predict nominal, classificatory or ordinal
variables based on features of an input while with prediction you try to
predict interval or ratio variables.

But you could see this as classification but just with a lot more
classes to predict. Or an infinite number of classes you can predict.

\hypertarget{language-modelling}{%
\section{Language modelling}\label{language-modelling}}

Language modelling is the activity of creating a computer model which
automatically \href{../Other/Learning.md}{learns} one or more things
about \href{../Languages/Languages.md}{language(s)} and can then make
\href{Prediction.md}{predictions} for a certain task.

Can we even do this? Yes we can. Language is a sequence of things, and
that sequence is not random. What comes before affects what comes next.
This is the heuristic which is used to make good language models.

\hypertarget{probabilities}{%
\subsection{Probabilities}\label{probabilities}}

A language model makes it predictions using a large probability matrix
which encode probabilities about the language which are useful for the
specific task which the language model is trying to solve.

To get these probabilities, a language model is first trained on a bunch
of correct text of a language from a \href{../Data/Corpus.md}{corpus}.
This gives the model a \textbf{large probability distribution of
sequences in the language}. When the model receives a new (unfinished
sequence) it can assign a probability to sequences of symbols that might
come next in the sequence.

There are multiple ways of deriving the probability matrix. Two ways
discussed in the course are counting and neural networks.

These probability distributions can also give a probability of how
likely an entire sentence is to appear in a language. This way, the
model might say that: ``The sun rotates around the earth'' is less
likely than ``The earth rotates around the sun''. Or, the sentence ``A
house is bigger than a person'' is more likely than ``A person is bigger
than a house''.

\hypertarget{uses-of-language-models}{%
\subsection{Uses of language models}\label{uses-of-language-models}}

Language models are usually a means to an end, rarely the goal. They are
useful for:

\begin{itemize}
\tightlist
\item
  Scoring sentences before choosing the best one
\item
  Scoring machine generated language.
\item
  Investigate language processing in humans to inform the development of
  bots.
\item
  Speech/handwriting recognition. In disambiguation, if you have a piece
  of unclear text could be multiple things, you could pick the one the
  language model says is more likely.
\item
  Spelling correction if more alternatives have the same
  \href{../Languages/Edit\%20distance.md}{edit distance}.
\item
  Scoring machine translation models or translation options.
\item
  Which sequences are part of a
  \href{../Languages/Languages.md}{language} and which are not?
\item
  How should a certain sequence be continued?
\item
  What words should be filled in to a blank spot.
\item
  Translation
\item
  \ldots.
\end{itemize}

\hypertarget{generating-language}{%
\subsection{Generating language}\label{generating-language}}

If the model knows the probability distribution of a language, then you
could give it an unfinished sequence of
\href{../Data/Symbol.md}{symbols} and the model can tell you which next
sequence of symbols most likely to be next. The model does this by
picking the continuation with the highest probability to appear next in
the sequence when following the probability distribution (what we
already know or what happened before). So language models can actually
predict what is the most likely to come next. Instead of taking the most
likely continuation, you can also sample according to the probability
matrix.

You could then take the sequence of symbols that was the most likely and
add it to the sequence you had to further the sequence. Now after you
have done this once you can of course do another prediction on the new
sequence and there you go now you're generating language. Language
models, which generate language, we call \textbf{generative models}.

\hypertarget{always-the-same-sentence}{%
\subsubsection{Always the same
sentence}\label{always-the-same-sentence}}

In a model as described above when we give the same sequence and always
take the most likely continuation, the model will always give the same
results because the probability distribution is static. This can cause
you to hit loops if you only consider the last x number of symbols in a
sequence.

\hypertarget{evaluating-language-models}{%
\subsection{Evaluating language
models}\label{evaluating-language-models}}

How do you know whether a language model is good? In this case, good is
that the model encodes probabilities well and assigns high probability
to real, correct sentences and low probabilities to wrong sentences. The
best language models can mimic human performance.

\hypertarget{extrinsic-evaluation-1}{%
\subsubsection{Extrinsic evaluation}\label{extrinsic-evaluation-1}}

With extrinsic evaluation, you evaluate how much impact a model has on a
\textbf{downstream task}. For instance, how many new customers did we
get after we bought that translation software? How large was the
decrease in calls to our support center after we installed that chatbot.

\hypertarget{intrinsic-evaluation-1}{%
\subsubsection{Intrinsic evaluation}\label{intrinsic-evaluation-1}}

With intrinsic evaluation, you evaluate the model itself, often
according to some scoring metric from the literature. If you see an
improvement in the intrinsic evaluation, you can say that you may expect
an improvement of the downstream task, but you can not be sure.

So how do we do intrinsic evaluation? You feed the model new data and
check how well it predicts each token in the sentences and how well it
scores sentence probabilities.

A good language model will \textbf{fit the new data well}. This means it
will usually predict the correct word type or assign a high probability
to the sequences in the new data. This type of evaluation is called
\href{Perplexity.md}{perplexity}.

\hypertarget{bidirectional-models}{%
\subsubsection{Bidirectional models}\label{bidirectional-models}}

A bidirectional model is a language model which analysis text from both
left to right and right to left. It could also mean analysing the text
that came before what you try to predict and analysing what comes after
what you try to predict given that is available.

\hypertarget{techniques-for-nlp}{%
\subsection{Techniques for NLP}\label{techniques-for-nlp}}

Techniques for language modelling discussed include
\href{../Classification/Native\%20baiyes/Na√Øve\%20Bayes\%20Classifier.md}{Na√Øve
Bayes} (doesn't really take preceding words into account),
\href{Markov\%20models.md}{Markov chain models} (we do take preceding
words into account with n-grams),
\href{../Languages/Probabilistic\%20Context\%20Free\%20Grammar.md}{Probabilistic
Context Free Grammar} (using production rules to expand non terminals to
terminals to generate language) and
\href{Feed\%20forward\%20neural\%20networks\%20(FFNN).md}{Feed forward
neural networks (FFNN)} and
\href{Recurrent\%20neural\%20network\%20(RNN).md}{Recurrent neural
network (RNN)} and \href{Transformers.md}{Transformers}.

\begin{figure}
\centering
\includegraphics{Pasted_image_20220605011349.png}
\caption{Language modeling progression}
\end{figure}

\hypertarget{viterbi-algorithm}{%
\section{Viterbi Algorithm}\label{viterbi-algorithm}}

How do you actually do \href{Decoding.md}{Decoding} in practice? Because
there is a large growth in the input. Given a sequence of 4 words and 45
tags there are \(45^4\) possible sequences. So it's \(O(Q^{|T|})\). This
grows fast. This means we can not calculate all the sequences.

The basic principle is to use recursion to compute the best path that
could lead to a certain point given: - The HMM - The observations up
till the point you are - The most probable state sequence to have
generated the observations given the HMM.

The idea is just to compute the best path for the first part of the
sequence. This is not so hard. But then \textbf{we know that the best
path of the first part of the sequence is the best path up till that
point}. This means that we don't have to recompute the probabilities for
that part of the sequence. So if you have word 1 and 2 and 3. You can
first compute the maximum likelyhood tag for 1, and then you can use
that to calculate the maximum likelihood for 2 without recomputing 1.

This means we can start from 3. To calculate the best one we need 2, to
calculate the best one we need 1. Then we hit the base condition, so we
calculate 1 and return it. Now that it returned we can use it to
calculate 2 and return that now that 2 returned we can calculate 3. So
basically you leave the work until you finished another problem.

Using the recursion we get \(O(Q^nT)\) where n is the n in
\href{../Languages/N-grams.md}{N-grams}. This is also called the
\textbf{order of the model}. So n = 2 is a bigram. So this makes it much
better, and you need a lot of data to get higher n.~

\hypertarget{trellis}{%
\subsection{Trellis}\label{trellis}}

The trellis represents the probability that the HMM is in state
\(q \in Q\) after seeing the previous events \(o_{1}...o_{t}\in O\) and
passing through the most probable sequence of states. The trellis is the
name we give to this table we have been building.

The value of each cell at successive states is computed by
\textbf{multiplying the current transition and emission probabilities by
the most probable sequence} which could lead to that cell. This means we
have to find the probable sequence. We do this by taking the
max(transition probability * each column of the A matrix - BOS).

\hypertarget{in-practice}{%
\subsection{In practice}\label{in-practice}}

Create a matrix M with as many rows as there are states \(q \in Q\) and
as many columns as there are events \(o \in T\) to be decoded (so that
is the \href{../Data/Sentences.md}{sentence} you want to tag + BoS and
EoS).

The value of each cell \(M_{qo}\) is computed as follows:
\(p(t_{o}|t_{o-n:o-1})p(w_{o}|t_{o})\). So it's the prior multiplied by
the likelihood. This means that each cell contains the posterior
probability of finding each tag given the current word after having
observed everything that came before.

Each posterior in this matrix is depended on the \textbf{emission and
the local transition} at each word \(w_t\) with \(t \in T\), compute the
posteriors considering

\hypertarget{tutorial}{%
\subsection{Tutorial}\label{tutorial}}

For an example, we need an already filled A and B matrix. \(A_{ij}\)
encodes the probability that the tag in column j occurs, given the tag
in row i before it. \(B_{ik}\) encodes the probability that word k is
observed given that tag i was before. The rows sum to 1.

\begin{longtable}[]{@{}llllll@{}}
\toprule
A & Det & Adj & Noun & Verb & EOS \\
\midrule
\endhead
Det & 0 & 0.2 & 0.8 & 0 & 0 \\
Adj & 0 & 0.3 & 0.6 & 0 & 0.1 \\
Noun & 0 & 0 & 0 & 0.5 & 0.5 \\
Verb & 0.5 & 0.1 & 0.2 & 0 & 0.2 \\
BOS & \textbf{0.5} & \textbf{0.2} & \textbf{0.3} & \textbf{0} &
\textbf{0} \\
\bottomrule
\end{longtable}

So for instance if you find and Adj you have a 0.3 chance to find
another adjective and a 0.6 chance to find a noun. So given adj (on the
side) you have a probability of 0.3 to find another adjective next and
0.6 to find a noun next. So given what's on the side, you have
probability to find something at the top. Because the row values are
probability, this is why rows sum to 1.

The BOS vector at the bottom is \(\pi\). The initial distribution. Given
BoS what is the most likely to follow.

\begin{longtable}[]{@{}llllll@{}}
\toprule
B & dog & the & chases & cat & fat \\
\midrule
\endhead
Det & 0 & 1 & 0 & 0 & 0 \\
Adj & 0 & 0 & 0 & 0 & 0 \\
Noun & 0.5 & 0 & 0 & 0.4 & 0.1 \\
Verb & 0.1 & 0 & 0.8 & 0.1 & 0 \\
\bottomrule
\end{longtable}

The same here. Given a noun, the chance is 0.5 that the word is dog.

So A only looks at tags following other tags and B actually looks at
words.

\hypertarget{now-we-can-start-the-algorithm.}{%
\subsubsection{Now we can start the
algorithm.}\label{now-we-can-start-the-algorithm.}}

First, create the table. \textbar Tag\textbar{} rows by
\textbar words\textbar{} columns + the BoS and EOS\textbar. This table
is called the trellis.

\begin{longtable}[]{@{}lllllllll@{}}
\toprule
Trellis & BoS & The & dog & chases & the & fat & cat & EOS \\
\midrule
\endhead
Det & & & & & & & & \\
Adj & & & & & & & & \\
Noun & & & & & & & & \\
Verb & & & & & & & & \\
\bottomrule
\end{longtable}

First, we can fill in the probabilities for the BoS row by filling in
\(\pi\), but you don't have Bos and Eos on the side. So just to be
clear, we filled in \(\pi\) and \(\pi\) is the bottom row of the A
matrix (the row with BOS). So we flipped that row 90 degrees and also
left out the last element (because EOS doesn't follow BOS). \(\pi\) is
marked bold in the A table.

\begin{longtable}[]{@{}lllllllll@{}}
\toprule
Trellis & BoS & The & dog & chases & the & fat & cat & EOS \\
\midrule
\endhead
Det & 0.5 & & & & & & & \\
Adj & 0.2 & & & & & & & \\
Noun & 0.3 & & & & & & & \\
Verb & 0 & & & & & & & \\
\bottomrule
\end{longtable}

Now we want to take the next word ``the'' and take the ``the'' column
from the B matrix. This is the emission probability.

\begin{longtable}[]{@{}lllllllll@{}}
\toprule
Trellis & BoS & The & dog & chases & the & fat & cat & EOS \\
\midrule
\endhead
Det & 0.5 & 1 & & & & & & \\
Adj & 0.2 & 0 & & & & & & \\
Noun & 0.3 & 0 & & & & & & \\
Verb & 0 & 0 & & & & & & \\
\bottomrule
\end{longtable}

Now you want to multiply the column we got (emission probability) with
the previous column (the transition probability). This gives us:

\begin{longtable}[]{@{}lllllllll@{}}
\toprule
Trellis & BoS & The & dog & chases & the & fat & cat & EOS \\
\midrule
\endhead
Det & 0.5 & 1 * 0.5 = 0.5 & & & & & & \\
Adj & 0.2 & 0 * 0.2 = 0 & & & & & & \\
Noun & 0.3 & 0 * 0.3 = 0 & & & & & & \\
Verb & 0 & 0 * 0 = 0 & & & & & & \\
\bottomrule
\end{longtable}

Now, the \emph{the} column indicates the posterior probability of the
determiner given that the first token is ``the''. We have now decoded
the sequence BoS. Now we can move on to the next item, which is
\emph{dog}.

The value of each cell at successive states is computed by
\textbf{multiplying the current transition and emission probabilities by
the most probable sequence} which could lead to that cell. This means we
have to find the probable sequence. We do this by taking the
max(transition probability * (the full A matrix -- BOS)).

So to find the most probably continuation for, we want to multiply
(multiply not dot product) the current word in the trellis with the
entire A matrix (without the BoS) and take the max.

\begin{figure}
\centering
\includegraphics{viterbi-matrix.png}
\caption{Matrix image}
\end{figure}

Now the max is 0.4. Now we multiply this with the emission probability
of the current word (dog) so we get: 0.4 * {[}0, 0, 0.5, 0.1{]} = {[}0,
0, 0.2, 0.04{]}. This becomes the new column in the trellis.

So we got {[}0, 0, 0.2, 0.04{]} by multiplying the dog column with A and
then taking the max (0.4) and then multiplying the dog column with the
max.

\begin{longtable}[]{@{}lllllllll@{}}
\toprule
Trellis & BoS & The & dog & chases & the & fat & cat & EOS \\
\midrule
\endhead
Det & 0.5 & 0.5 & 0 & & & & & \\
Adj & 0.2 & 0 & 0 & & & & & \\
Noun & 0.3 & 0 & 0.2 & & & & & \\
Verb & 0 & 0 & 0.004 & & & & & \\
\bottomrule
\end{longtable}

We also want to mark which column gave the max in the multiplication. In
this case, it was the first column. It is not guaranteed that this is
always the highest value from the previous column. It depends on A. So
now we have the posterior probabilities for The dog. We don't have to
recompute for dog.

\begin{longtable}[]{@{}lllllllll@{}}
\toprule
Trellis & BoS & The & dog & chases & the & fat & cat & EOS \\
\midrule
\endhead
Det & 0.5 & \textbf{0.5} & 0 & & & & & \\
Adj & 0.2 & 0 & 0 & & & & & \\
Noun & 0.3 & 0 & 0.2 & & & & & \\
Verb & 0 & 0 & 0.004 & & & & & \\
\bottomrule
\end{longtable}

So as you can see, we need 3 pieces of information each time.

\begin{itemize}
\tightlist
\item
  The posterior probability up to the previous state (the trellis)

  \begin{itemize}
  \tightlist
  \item
    This makes it dynamic and saves time
  \end{itemize}
\item
  The transition probabilities from state \(q_i\) to state \(q_j\)

  \begin{itemize}
  \tightlist
  \item
    This is the prior
  \end{itemize}
\item
  The emission probabilities for observation \(o_j\) given state \(q_j\)

  \begin{itemize}
  \tightlist
  \item
    This the likelihood
  \end{itemize}
\end{itemize}

So let's continue:

\begin{figure}
\centering
\includegraphics{Pasted_image_20220308195301.png}
\caption{Calculating Chases col with Viterbi}
\end{figure}

We found that 0.1 is the highest after multiplication. Then we multiply
it with the emission which is {[}0, 0, 0, 0.8{]} so 0.1 * {[}0, 0, 0, 0,
0.8{]} which becomes {[}0, 0, 0, 0.08{]}. Now we can fill that in, and
we should mark the third column.

\begin{longtable}[]{@{}lllllllll@{}}
\toprule
Trellis & BoS & The & dog & chases & the & fat & cat & EOS \\
\midrule
\endhead
Det & 0.5 & \textbf{0.5} & 0 & 0 & & & & \\
Adj & 0.2 & 0 & 0 & 0 & & & & \\
Noun & 0.3 & 0 & \textbf{0.2} & 0 & & & & \\
Verb & 0 & 0 & 0.004 & 0.08 & & & & \\
\bottomrule
\end{longtable}

When we have filled the entire trellis, you can make the sequence of
tags by picking the tag on the side of the column that has the marked
number in each row. So, so far we get {[}Det, Noun{]} which is correct.

Now we can repeat. The probabilities get small quick.

\hypertarget{more-context}{%
\subsection{More context}\label{more-context}}

We can also look at the maximum of the previous transitions from each
posterior more than 1 previous column. So like n = 2 previous columns.
However, it increases complexity but also brings scarcity. We don't want
0 transition to hijack the computation. Because often a transition is
not actually impossible, just a really small chance. To somewhat solve
this we can use smoothing just like with
\href{Markov\%20models.md}{Markov models}. We can also use interpolation
to guess the transition probability, which is estimated by linearly
interpolating using smaller n-grams. A guessed probability is better
than no probability.

You can also use pseudo morphology ‚Üí words consist of smaller units
which influence their PoS tag. We can look at this to add bias to the
probability, or we could compute emission probabilities for part words
(suffixes of different lengths, down till single characters). Whenever
you hit an unknown word, use the emission probability for the suffixes
instead. Something is better than nothing.

\hypertarget{forward-algorithm}{%
\section{Forward Algorithm}\label{forward-algorithm}}

This is similar to the Viterbi algorithm, but used to compute the
likelihood of a sequence of observed events given a HMM. So it tries to
find the likelihood of a sequence appearing at all in the set of all
possible sequences. We want to find the culminative probability at each
step.

All this does is replacing the max with the sum. You do this because you
want to take into account the probability of all possible paths' through
the hidden states, not find the likeliest path. Also, you don't need
backpointers because we don't try to find a tag.

\hypertarget{decoding}{%
\section{Decoding}\label{decoding}}

Decoding is the task of turning something that is in code form into a
non code form.

In the case of this course decoding has to do with
\href{Hidden\%20Markov\%20Models.md}{Hidden Markov Models}. It is the
task of \textbf{determining the sequence of hidden variables given a
sequence of observed events.}

The decoding task takes an estimated HMM \(\lambda\)(A, B, \(\pi\)) and
a sequence of observations O as input to output the likeliest sequence
of states Q to have generated O.

Decoding is also mentioned in this course as the task of decoding an
abstract representation of a sequence to another sequence in the context
of \href{Recurrent\%20neural\%20network\%20(RNN).md}{Recurrent neural
network (RNN) and seq2seq} and \href{Transformers.md}{Transformers}

\hypertarget{formally}{%
\subsection{Formally}\label{formally}}

If we want to write this down formally we get:
\(\hat{t}_{1:n} =\text{argmax}_{t_{1:n}} p(t_{1:n}|W_{1:n})\).

So the approximated tag sequence is the sequence of words with the
highest posterior probability. If you actually calculate this with
\href{../Classification/Native\%20baiyes/Bayes\%20rule.md}{Bayes rule}
you get
\(\hat{t}_{1:n} =\text{argmax}_{t_{1:n}} p(t_{1:n}) \cdot p(W_{1:n})\).
We plug in the formulas to estimate both terms under the
\href{Hidden\%20Markov\%20Models.md}{Markov assumption and the output
independence assumption}. So for a whole sequence:

\[\hat{t}_{1:n} \approx \text{argmax}_{t_{1:n}} \prod^{n}_{i = 1} p(t_{1:n}) \cdot p(W_{1:n})p(w_{i}|t_{i})\]

You can also do this in log space to avoid overflowing. Here you can
also see the output independence. The last p only looks at the i word
with the i tag.

\hypertarget{transition-and-emission}{%
\subsection{Transition and emission}\label{transition-and-emission}}

The posterior is the product of the \textbf{transition probability} PoS
tag n-grams to PoS tag and the \textbf{emission probability} from the
PoS tag to word.

\hypertarget{transition-probability}{%
\subsubsection{Transition Probability}\label{transition-probability}}

Transition probabilities (A) captures the prior. This is how likely the
tag is given the context. Comes from the A matrix stays with the tags
and sort of enforces the Markov assumption in the model.

\hypertarget{emission-probability}{%
\subsubsection{Emission Probability}\label{emission-probability}}

The emission probability is the probability of the word given the word.
This enforces the output independence. You only look at the current tag.

\hypertarget{feed-forward-neural-networks}{%
\section{Feed forward neural
networks}\label{feed-forward-neural-networks}}

Feed forward neural networks (FFNN) can be used to overcome the
linearity problem with \href{Logistic\%20Regression.md}{Logistic
Regression}. To achieve this we feed the output of one logistic
regression multiply-the-weights-vector-with-the-input-vector-step as
input into another logistic regression model. We can also just keep
doing the multiply vector and weights a lot of times before we do the
classification function and making it probabilistic steps. We can also
multiply the input vector with multiple weight vectors only then apply a
classification function to some of the cells of the result vector to
create a new vector of a different size and keep going. It is quite
flexible, to see the graphical representation below.

So stacking logistic regressors will create non-linear classifiers which
can learn \textbf{arbitrary decision boundaries} for linear classifier.
This turns logistic regression into (feedforward) neural networks.

\begin{figure}
\centering
\includegraphics{Pasted_image_20220603203033.png}
\caption{Graphical representation of feed forward network}
\end{figure}

So what you see above is taking the input vector and multiply with
multiple weight vectors every time also adding a bias. Then you take the
sum of these cells and pass it to a function. Typically, a non-linear
function to add non-linearity. This then gives you a new vector (the
gray dots). This is then called a \textbf{hidden layer} because you can
not interpret it any more back to the input. It is a more abstract
representation of the input.

We then keep going, multiplying the hidden layer by another weights
vector and adding a bias and on and on. However, we want to go to a
scaler in the end so to do that you just take one sum from the vector
you are at and apply the classification function (for instance sigmoid
or softmax) to turn the output to a certain range and into a
probability.

\hypertarget{matrix-representation}{%
\subsection{Matrix representation}\label{matrix-representation}}

If you would actually make a computer do this it is useful to use
matrixes to kind glue the weights vectors together and computers know
how to deal with matrixes well and fast. You can visualize the above
picture with matrixes like below:

\begin{figure}
\centering
\includegraphics{Pasted_image_20220603203248.png}
\caption{Feed forward visualized as matrix}
\end{figure}

\hypertarget{feed-forward}{%
\subsection{Feed forward}\label{feed-forward}}

These types of networks are called feed forward because the output of a
lower layer only affect higher layers and do not feed back into the same
layer or lower layers.

These types of networks also sometimes called Multi Layer Perceptrons
(MLP) although perceptron's don't use non-linear
\href{Logistic\%20Regression.md}{activation functions} while FFNN do.

There are also neural network architectures which are not feed forward.
For instance, you can take the previous states of the model into account
with \href{Recurrence.md}{recurrence}. When this happens, the neural
network becomes a
\href{../Prediction/Recurrent\%20neural\%20network\%20(RNN).md}{recurrent
neural network (RNN)}.

\hypertarget{fully-connected}{%
\subsection{Fully connected}\label{fully-connected}}

MLP are typically fully connected (or dense). Each hidden unit computes
the weighted sum over \textbf{all} the units in the previous layer.
There is a direct connection for every pair of hidden units (or neurons)
in each pair of adjacent layers (including the input and output layers).
So basically in the drawing, there is a line from each circle of one
layer to each other circle in the next layer. If this is not the case,
then a network is not fully connected.

\hypertarget{hidden-layers}{%
\subsection{Hidden layers}\label{hidden-layers}}

There can be arbitrary many hidden layers even though one is enough to
learn any kind of function (given infinite time, and resources).
Stacking hidden layers may help learning. Hidden layers can also change
the size of the vector, like seen in the graphical representations with
the gray layer.

\hypertarget{back-propagation}{%
\subsection{Back propagation}\label{back-propagation}}

Adjusting the weights with FFNN is more complicated than with logistic
regression. Backpropagation is the algorithm used for this, but this
course does not go into further detail about it. You can
\href{http://neuralnetworksanddeeplearning.com/chap2.html}{read more
here}.

The general idea is that you apply the forward pass where you generate
the outputs. Then you calculate the loss with the objective function
with the correct outputs vs reality. This requires labels. Then you do
the \textbf{backwards} pass, where you take the error and compute all
the derivatives at every layer. This will sort of show which weights
caused the largest error, and then you adjust those more in the
direction which will cause less error.

\hypertarget{learning-rate}{%
\subsection{Learning rate}\label{learning-rate}}

The learning rate decides the size of the improvement steps taken in
back propagation. It basically scales the adjustments which need to be
made according to the backpropagation algorithm. The larger steps make
the model improve the weights faster, but you can also overshoot and
improve into a `wrong' direction. It captures \textbf{how confident we
want to be in updating our hypothesis} given the input data you just
evaluated and error that caused. You can also start with a higher
learning rate and lower it every time you improve the model. The
learning rate is one of the hyperparameter of the model.

\hypertarget{creating-a-ffnn}{%
\subsection{Creating a FFNN}\label{creating-a-ffnn}}

How do you start of? It seems the best to start all the weights with
small random numbers. The weights are the parameters of the model which
will be learned.

FFNNs have parameters which are learned (the weights) and
hyperparameters which are set by the modeler (you): how many layers, how
many units in each layer, which activation function, which loss, which
optimizer, which input representation, the learning rate. It is
important to explore several constellations and check on a dev. Finding
the hyper parameters can be automated with GridSearch. But hopefully you
know what hyper are as it has been 3 years at this point.

\hypertarget{applications-of-ffnn}{%
\subsection{Applications of FFNN}\label{applications-of-ffnn}}

We can use neural networks to create
\href{../Semantic-Similarity/Vector\%20semantics.md}{word vectors} also
known as \href{../Semantic-Similarity/Embeddings.md}{word embeddings}.
Instead of just \href{../Semantic-Similarity/Co-occurrence.md}{counting
words} we train a model to predict the neighbouring words. This has the
advantage of \textbf{directly building dense vectors} instead of first
making space vectors with the counting based models and then projecting
them into lower dimensional space. One popular technique to
representation learning is
\href{../Semantic-Similarity/Word2Vec.md}{Word2Vec}.

Another application is to
\href{../Prediction/Using\%20FFNN\%20for\%20language\%20modelling.md}{use
FFNN for language modelling}. This works better than
\href{../Prediction/Markov\%20models.md}{Markov models} because you
don't have to do smoothing.

\hypertarget{side-note-about-this}{%
\subsubsection{Side note about this}\label{side-note-about-this}}

It was found by that paper mentioned in the guest lecture
\href{https://scholar.google.nl/scholar?q=improving+distributional+similarity+with+lessons+learned+from+word+embeddings\&hl=en\&as_sdt=0\&as_vis=1\&oi=scholart}{(Levy,
O., et all)} that what logistic regression is doing is very much similar
to doing co-occurrence counts with
\href{../Semantic-Similarity/Point\%20wise\%20mutual\%20information\%20(PMI).md}{PMI}
\href{../Semantic-Similarity/Association\%20measure.md}{Association
measure} as weights. We can explain what it is doing. So that is very
intresting. This does not hold up for larger neural networks.

This is the abstract:

\emph{Recent trends suggest that neural network-inspired word embedding
models outperform traditional count-based distributional models on word
similarity and analogy detection tasks. We reveal that much of the
performance gains of word embeddings are due to certain system design
choices and hyperparameter optimizations, rather than the embedding
algorithms themselves. Furthermore, we show that these modifications can
be transferred to traditional distributional models, yielding similar
gains. In contrast to prior reports, we observe mostly local or
insignificant performance differences between the methods, with no
global advantage to any single approach over the others.}

\hypertarget{markov-models}{%
\section{Markov models}\label{markov-models}}

A Markov model is any \href{Language\%20Modeling.md}{language Model}
model which makes use of the \href{Markov\%20assumption.md}{Markov
assumption}. A Markov model is also called Markov chain.

Markov models make use of:

\begin{itemize}
\tightlist
\item
  A set of history states \textbf{Q}
  \(\{q_{1},q_{2}, q_{3}, ..., q_{n}\}\)
\item
  A set of predicted states \textbf{R}
  \(\{r_{1},r_{2}, r_{3}, ..., r_{m}\}\) (What you try to predict)
\item
  A transition probability matrix A (size: NxM)
\item
  An initial probability distribution \(\pi\)
\end{itemize}

\hypertarget{states}{%
\subsection{States}\label{states}}

In a bigram model, Q and R are the same set. With larger
\href{../Languages/N-grams.md}{N-grams} models, they are not the same. Q
is bigger than R because the histories are longer.

A difference between Q and R is that Q will contain the BoS while R
contains the EoS.

\hypertarget{transition-matrix}{%
\subsection{Transition matrix}\label{transition-matrix}}

The transition probability matrix A encodes the probability of going
from a state \(q \in Q\) to a state \(r \in R\), such that a cell
\(a_{ij} \in A\) with \(i \leq |Q|\) and \(j \leq |R|\) encodes the
probability of finding state j given state i.

Q usually corresponds to the rows of A and R to the columns. This is
done, so we can get relative frequencies by row-normalizing counts.

\hypertarget{initial-distribution}{%
\subsection{Initial distribution}\label{initial-distribution}}

This is starting state. The initial distribution is given by the
transitions between states Q which only consist of BoS symbols and the r
states. This is where all sequences start. The initial distribution can
be set in advance or estimated.

This is usually just embedded in A by augmenting Q with the BoS state.

So usually this a row in A with only beginning of sequence symbols. It
is the initial state.

\begin{figure}
\centering
\includegraphics{Pasted_image_20220223185953.png}
\caption{Transition probability distribution}
\end{figure}

Looking at this table really makes it more concrete. The idea is that
the sum of the entire row is one. The sum of a column does not have to
be one.

\hypertarget{fine-tuning}{%
\subsection{Fine tuning}\label{fine-tuning}}

How do you decide on the n-gram size? Which
\href{Smoothing.md}{discounting} method you will use? What k will you
use if you use Laplace \href{Smoothing.md}{smoothing}? etc.

We can do this like any other machine learning problem. In this case the
loss/cost/error function is \href{Perplexity.md}{perplexity}, and we
want to minimize it. Try to minimize the perplexity of the dev set by
tuning the hyperparameters.

\hypertarget{trade-of}{%
\subsection{Trade of}\label{trade-of}}

Higher N-grams are more constraining and precise, but more scarce. If we
have 20k \href{../Data/Type.md}{types}, then there are: - 20K\(^2\)
bigrams = \(4*10^8\). - 20K\(^{3}\) trigrams = \(8 * 10^{12}\). -
20K\(^{4}\) tetragrams = \(1.6 * 10^{17}\).

So you need to have the data to support this otherwise you will hit
something you have not seen before too often.

THIS is why \href{../Data/Normalization.md}{Normalization} is important
because it reduces the number of different histories from which you can
predict.

\hypertarget{interpretability-guest-lecture}{%
\section{Interpretability (Guest
Lecture)}\label{interpretability-guest-lecture}}

When something is interpretable, you can know what a certain value or
vector of values represent. So with
\href{../Semantic-Similarity/Connotations.md}{connotations} it is clear
what each number in the vector means, while with the hidden layers of a
\href{Feed\%20forward\%20neural\%20networks\%20(FFNN).md}{neural
networks} it is not clear how exactly the numbers relate to the input.

This topic is what the guest lecture (by Jasmijn Bastings) was about.
How do you explain good performance of models.

Interpretability is useful to prevent scenarios where the model takes a
shortcut through the data and therefor becomes less generalizable.

\hypertarget{explaining-predictions}{%
\subsection{Explaining predictions}\label{explaining-predictions}}

Interpretability allows you to explain the predictions of a model. For
instance, with a linear classifier you can know exactly \textbf{why}
something was predicted because you can just look at the formula. This
means linear models are interpretable. This is not the case with neural
networks, but there are approaches to do this.

\hypertarget{occlusion}{%
\subsubsection{Occlusion}\label{occlusion}}

Occlusion-based methods compute input saliency by occluding (or erasing)
input features and measuring how that affects the model output. So
deleting parts of the input data and then looking how the model behaves.

\begin{figure}
\centering
\includegraphics{Pasted_image_20220605012313.png}
\caption{Occlusion}
\end{figure}

This can indicate the words for instance a model is picking up on to
give a good probability to an answer.

Occlusions will try taking out every combination of the input which
takes a lot of time. So one word at the time.

\hypertarget{perturbations}{%
\subsubsection{Perturbations}\label{perturbations}}

With perturbations you basically sample masks which you use to take out
some of the input data and you run the model. Then you try to predict
these outputs with a linear model from the masks. This linear model will
then assign importance scores to some of the words. So you use the fact
that linear models are interpretable to interpret your model.

This is like a faster version of occlusion where you don't try all
possibilities but you sample possibilities.

\begin{figure}
\centering
\includegraphics{Pasted_image_20220606165320.png}
\caption{Pertubations}
\end{figure}

\hypertarget{gradient-l_2}{%
\subsection{\texorpdfstring{Gradient
\(L_2\)}{Gradient L\_2}}\label{gradient-l_2}}

Also known as Sensitivity Analysis (SA), we can use the gradient from a
standard backpropagation pass as relevance scores:

In practice, we want a single scalar relevance score per input word,
instead of one for each word embedding dimension, so we take the L2
norm: Requires one forward pass and one backward pass.

Gradient L2 says how much a change in a word's embedding affects the
output of a model.

Instead of using the gradient to improve the model, you use it to
explain the results.

Grad L2 shows the sensitivity of the model, i.e., how much a change in
input changes the output.

\hypertarget{gradient-x-input}{%
\subsection{Gradient x Input}\label{gradient-x-input}}

You can also take the gradients and multiply them with the embedded
inputs. This arrives at the gradients times input measure. To get a
scaler you do the dot product between the embedded input and the
gradient times.

Gradient x Input shows the saliency, i.e., the marginal effect of each
input word on the prediction

\hypertarget{integrated-gradients}{%
\subsection{Integrated Gradients}\label{integrated-gradients}}

Integrated gradients (IG) is a gradient-based method which deals with
the problem of saturation: gradients may get close to zero for a
well-fitted function.

IG requires a baseline \(b_{1:n}\)e .g., all-zeros vectors or repeated
{[}MASK{]} vectors. The math looks like this:
\[\frac{1}{m}\sum\limits^{m}_{k=1} \nabla f_c(b_{1:n}+\frac{k}{m}(\mathbf{x}_{1:n}) \cdot (\mathbf{x}_i-b))\]
where m is the number of steps we take (i.e., the number of gradients we
get).

\begin{figure}
\centering
\includegraphics{Pasted_image_20220606165428.png}
\caption{Example of integrated gradients}
\end{figure}

In the example above, it appears that the model above can already get a
very good score with only a fraction of the image information.

\hypertarget{layer-wise-relevance-propagation-lrp}{%
\subsection{Layer-wise Relevance Propagation
(LRP)}\label{layer-wise-relevance-propagation-lrp}}

Layer wise relevance propagation (LRP) starts with a forward pass to
obtain the output \(f_c(x_{1:n})\), which is the top-level relevance.

It then uses a special backward pass that, at each layer, redistributes
the incoming relevance among the inputs of that layer. Each type of
layer has its own propagation rules. E.g., different rules for
feed-forward layers and LSTMs. Relevance is redistributed until we
arrive at the input.

For models with only ReLU and max pool nonlinearities, and Œµ=0, this is
the same as Gradient * input.

\hypertarget{attention}{%
\subsection{Attention}\label{attention}}

Many models have attention mechanisms. These provide some sense of
interpretability. If you visualize them, they show what the model is
``looking at'' at one particular layer.

\begin{itemize}
\tightlist
\item
  Many papers were written on whether attention is explanation (or not)
  see \href{Transformers.md}{Transformers}
\item
  A major issue is that, when attention is applied over hidden states,
  information from other time steps was already mixed in.\\
\item
  It's better to use saliency methods if you want input importance
  scores as a model developer.
\end{itemize}

\begin{figure}
\centering
\includegraphics{Pasted_image_20220606165657.png}
\caption{Attention example}
\end{figure}

\hypertarget{evaluation-strategies}{%
\subsection{Evaluation strategies:}\label{evaluation-strategies}}

\begin{itemize}
\tightlist
\item
  Use occlusion (or gradient) as the ground truth.
\item
  Use human annotations as the ground truth.
\item
  Train a model on a synthetic task for which a ground truth is
  available.
\item
  See if removing the most (least) important features results in a
  performance difference.
\item
  Use a linguistic task and see if the explanation identifies the
  linguistically relevant words for predicting the linguistic feature.
\item
  Concatenate an unrelated input text to each input text, and see if the
  explanation only indicates words in the original input text.
\item
  Let humans simulate the model.
\item
  Train a student model with the explanations.
\item
  Retrain a model where the least important features are removed.
\item
  Inject special indicators in the data and see if the explanation picks
  up on those.
\end{itemize}

\hypertarget{faithfull-metrics}{%
\subsection{Faithfull metrics}\label{faithfull-metrics}}

\begin{figure}
\centering
\includegraphics{Pasted_image_20220605014258.png}
\caption{Faitfull metrics}
\end{figure}

\hypertarget{language-interpretability-tool-lit}{%
\subsection{Language interpretability tool
(LIT)}\label{language-interpretability-tool-lit}}

The guest lecturer worked on a tool to interpret models better.

\begin{figure}
\centering
\includegraphics{Pasted_image_20220605013925.png}
\caption{Interpreting models}
\end{figure}

\hypertarget{towards-effective-explanations}{%
\section{Towards effective
explanations}\label{towards-effective-explanations}}

\begin{itemize}
\tightlist
\item
  Be careful about describing internal AI representations in human terms
  (concepts,\\
  rationalizations) as it communicates intentionality.\\
\item
  Be careful about attributing real causes to model reasoning.\\
\item
  Allow for interactive explanations to resolve contradictions.\\
\item
  Be clear on who the explainees are, what priors they may leverage.
\end{itemize}

\hypertarget{transformers}{%
\section{Transformers}\label{transformers}}

Transformers solve the problems with
\href{Recurrent\%20neural\%20network\%20(RNN).md}{Recurrent neural
network (RNN)}

RNN seem to be useful in any application in
\href{../Languages/Natural\%20languages.md}{Natural Language} Processing
(NLP). However, they suffer from the
\href{Vanishing\%20gradient\%20problem.md}{Vanishing gradient problem}.

Transformers get rid of \href{Recurrence.md}{recurrence} while still
dropping the \href{Markov\%20assumption.md}{Markov assumption}. This is
done by introducing a \textbf{self attention layer}. This layer maps an
input sequence \((x_{1}, x_{2}, ..., x_{n})\) to an output sequence
(\(y_1\), \(y_2\), \ldots{} \(y_n\)). These sequences could be anything
but this course focusses on sequences of \href{../Data/Words.md}{words}
which are \href{../Data/Sentences.md}{sentences}. These sequences can be
any arbitrary length without compromises. How does the attention layer
do this?

\hypertarget{self-attention-layer}{%
\subsection{Self attention layer}\label{self-attention-layer}}

The self attention layer \textbf{compares} an element (word) to a
collection of other elements (words) of the context (including the word
itself).

\hypertarget{example-4}{%
\subsubsection{Example}\label{example-4}}

So if you have the sentence. \emph{He is biting the apple}. We then need
\href{../Semantic-Similarity/Embeddings.md}{embeddings} for each word
like so:

\begin{longtable}[]{@{}lllll@{}}
\toprule
He & is & biting & the & apple \\
\midrule
\endhead
\(\mathbf{x}_{1}\) & \(\mathbf{x}_{2}\) & \(\mathbf{x}_3\) &
\(\mathbf{x}_4\) & \(\mathbf{x}_5\) \\
\bottomrule
\end{longtable}

and lets say the target element is apple then \(x_{5}\) is compared to
\(\mathbf{x}_{1},~\mathbf{x}_{2},~\mathbf{x}_{3},~\mathbf{x}_{4},~\mathbf{x}_{5}\).
Each comparison is going to give a score which you call alpha. So the
comparison give us \[a_{1.5},~a_{2.5},~a_{3},~a_{1},~a_{5.5}\] So the
first score of \(a_{1.5}\) is expressed as
\(a_{1.5} = \text{score}(\mathbf{x}_{1}, \mathbf{x}_{5})\). \textbf{We
want each score to tell how relevant the compared word is to predict the
target word}. You can imagine apple being left out of the sentence, and
we want to predict apple in that location. The scores tell us the
relevance of each compared word to predict the target word. This is
normally computed with the scaled dot product. Then to get an output of
these comparison scores, you take the weighted sum of the (normalized)
scores. This means taking all the alphas and multiplying with their
representations (the \(\mathbf{x}\)-es) and then taking the sum. So that
looks like this:
\[y_{5} = \text{sum}(a_{1.5}\mathbf{x}_1,~a_{2.5}\mathbf{x}_{2},~a_{3}\mathbf{x}_{3},~a_{1}\mathbf{x}_{4},~a_{5.5}\mathbf{x}_{5})\]
The alphas are giving us weights which indicate how much we should
consider each of the items in the sequence (in this case to predict the
word at position 5). Or basically how much attention to give each item
in the sequence. So \(y\) is the weighted output.

Then you compute the weighted output for all the target words to get
\(y_{1},~y_{2},~y_{3},~y_{4},~y_{5}\).

\hypertarget{self-attention-thermology}{%
\subsubsection{Self attention
thermology}\label{self-attention-thermology}}

In the example above, each word can play 3 roles. These are given more
specific names to which allows us to be more specific.

So each item in the sequence can be: - A \textbf{query} (q), when it is
compared to other items in the sequence. - A \textbf{key} (k), when it
is part of the sequence to compare with. - So the scores are written as
\(a_{i,j} = \text{score}(\mathbf{k}_{i},\mathbf{q}_{j})\) - A
\textbf{value} (v) when used to compute the output: -
\(y_{5} = sum(a_{1.5}\mathbf{v_1},~a_{2.5}\mathbf{v_{2}},~a_{3}\mathbf{v_{3}},~a_{1}\mathbf{v_{4}},~a_{5.5}\mathbf{v_{5}})\)

In the example \(x_{5}\) plays the query role. \(x_{1}\) , \(x_{2}\) ,
\(x_3\) , \(x_4\) , \(x_5\) all play the key role and \(x_{1}\) ,
\(x_{2}\) , \(x_3\) , \(x_4\) , \(x_5\) play the value role.

As you can see, each word can play multiple roles. \textbf{The key to
self attention is to learn separate weights for each role} This way get
separate weights for each role. Then if you want to use an input as a
certain role, you can just multiply the input with the correct weights.
More concrete, each input \(x_i\) is transformed into its role by
multiplying the \href{../Semantic-Similarity/Embeddings.md}{embeddings}
with the corresponding weights. Like this:

\begin{itemize}
\tightlist
\item
  \(q_{i}=W^Q\mathbf{x}_1\)
\item
  \(k_{i}=W^K\mathbf{x}_1\)
\item
  \(v_{i}=W^V\mathbf{x}_1\)
\end{itemize}

These matrixes are obtained trough training. These matrixes give very
fine-grained information about each item of the sequence (word in the
sentence(s)). In practice, the representations of the words are
different depending on the role the word is playing.

So this means there are a lot of weights to learn. However, it is more
efficient than recurrence as: - We get rid of the
\href{Vanishing\%20gradient\%20problem.md}{Vanishing gradient problem} -
All the computations for each score are independent. This allows for
large \textbf{parallelization}.

These matrixes are called the attention, I think.

\hypertarget{multi-head-attention}{%
\subsection{Multi head attention}\label{multi-head-attention}}

You can vary the score function. Different ways of scoring might make
more sense for different tasks depending on whether you focus on syntax,
semantics, discourse. For all these tasks, you can compute different
matrixes or attentions for each word when they play a certain role. You
can even have attentions for when you want an attention for when both
syntax and discourse are important. So each type of relation has its own
weights, which is its own attention. With this, the model size
increases, but it is not so bad because you can
\href{../Other/Learning.md}{learn} the weights in parallel.

Each set of self-attention layers is called a \emph{head}. So it is
called multi head attention because you use multiple types of attention
together, which is multiple heads.

\hypertarget{losing-word-order}{%
\subsection{Losing word order}\label{losing-word-order}}

So for some reason using attention is good\ldots{} but it is not clear
how or why.

Anyways getting rid of recurrence seems to solve the vanishing
gradients' problem by training a matrix for each value in a sequence for
different goals. Then if you want to do something with the value, you
use the attention matrix to get an embedding which is good for that
goal?

However, by removing recurrence, you lose the positional information.
The transformer architecture doesn't take the order into account
anymore. So even though a transformer can deal with sequences of any
length, which means it doesn't need the
\href{Markov\%20assumption.md}{Markov assumption}. What the model gets
is more a bag of words than a sequence. This means that if you do the
above example of ``He is biting the ??'' And instead do ``Is he biting
the ??'' You would still get apple both times. The exact same output. So
by dropping recurrence, word order information is lost. For this example
it doesn't matter because apple is still right, but if you would give a
random order of the sentence you would still get apple. This is a
problem because the word order of most languages is quite predictable or
quite constraint.

So you get position information back? You just encode the position of
the item in the sequence as a continues value that preserves order of
the information. So basically you \textbf{add a feature to the input
which encodes the position}. Often this is done with the sine or cosine
functions. It is quite arbitrary how they encode the position feature.

\begin{figure}
\centering
\includegraphics{Pasted_image_20220605011623.png}
\caption{Transformer slide from the guest lecture}
\end{figure}

\hypertarget{other-explanation}{%
\subsection{Other explanation}\label{other-explanation}}

If you (like me) are still a bit confused about transformers then read
\url{https://jalammar.github.io/illustrated-transformer/} which has a
good explanation with also more pictures.

\hypertarget{applications}{%
\subsection{Applications}\label{applications}}

Using the architecture outlined above, the most powerfull NLP models in
the world have been created. Some of the most popular ones are: - BERT
(Bidirectional Encoder Representations from Transformers) by Google -
and variants like RoBERTam XLNet (bigger) and DistilBERT (smaller) - GPT
(Generative Pre-trained Transformer) by OpenAI (microsoft) - GTP-1 to
GTP-3 - These are the most popular in the world right now.

\hypertarget{philosophical-questions}{%
\section{Philosophical questions}\label{philosophical-questions}}

These models caused a transformer revolution because they perform so
well. This sparks debates about what these models can and can't do, and
how close they are to achieve human-like linguistic abilities and even
general Artificial Intelligence.

Open questions: - How much do transformers really learn about natural
language, or do they just do well because they are trained on a lot of
data? - Do transformers learn the same syntactic relations defined in
linguistics - Do Transformers emulate human linguistic abilities - Do
transformers process sentence in a similar way

\hypertarget{the-paper}{%
\subsection{The paper}\label{the-paper}}

BERT especially has been studied a lot and this was covered in the
lecture. The study of the BERT model is informally referred to as
BERTOLOGY. This lecture consisted of the lecturer asking questions about
the paper, which you were supposed to answer in groups. For instance, a
question like this:

Question 1: What is the advantage of the BERT model: - More parallelable
- No vanishing gradient problem.

Marieke was kind enough to send me the questions and answers to these
questions, and they can be found in the
\href{../Other/Bert\%20Lecture.md}{Bert Lecture file} (Other/Bert
Lecture.md)

\hypertarget{contingency-table}{%
\section{Contingency table}\label{contingency-table}}

A contingency table shows all the classifications that have been done
with the real value. In a contingency table, you can see exactly how
things were wrongly classified as what.

\hypertarget{example-5}{%
\section{Example}\label{example-5}}

Here is an example table in a result when classifying languages.

\begin{longtable}[]{@{}llllll@{}}
\toprule
Language & German & French & Dutch & Italian & English \\
\midrule
\endhead
German & 4030. & 1. & 2. & 4. & 13. \\
French & 3. & 3385. & 2. & 11. & 7. \\
Dutch & 20. & 2. & 1230. & 4. & 24. \\
Italian & 0. & 4. & 2. & 10682. & 4. \\
English & 10. & 20. & 13. & 43. & 16559. \\
\bottomrule
\end{longtable}

So what you can read from this table is that 4030 German words were
classified as German 4030 times but also 3 French, 20 Dutch and 10
Italian words were classified as German.

1 German word was also classified as French. 2 German words were also
classified as Dutch. 4 German words were also classified as Italian. 13
German words were also classified as English.

So that is how you read these tables.

Here is example code to create the
\href{Evaluating\%20Classification\%20models.md}{evaluating
classification models} scores.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ perf\_measure(y\_actual, y\_hat):}
\NormalTok{    TP }\OperatorTok{=} \DecValTok{0}
\NormalTok{    FP }\OperatorTok{=} \DecValTok{0}
\NormalTok{    TN }\OperatorTok{=} \DecValTok{0}
\NormalTok{    FN }\OperatorTok{=} \DecValTok{0}

    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(y\_hat)): }
        \ControlFlowTok{if}\NormalTok{ y\_actual[i]}\OperatorTok{==}\NormalTok{y\_hat[i]}\OperatorTok{==}\DecValTok{1}\NormalTok{:}
\NormalTok{           TP }\OperatorTok{+=} \DecValTok{1}
        \ControlFlowTok{if}\NormalTok{ y\_hat[i]}\OperatorTok{==}\DecValTok{1} \KeywordTok{and}\NormalTok{ y\_actual[i]}\OperatorTok{!=}\NormalTok{y\_hat[i]:}
\NormalTok{           FP }\OperatorTok{+=} \DecValTok{1}
        \ControlFlowTok{if}\NormalTok{ y\_actual[i]}\OperatorTok{==}\NormalTok{y\_hat[i]}\OperatorTok{==}\DecValTok{0}\NormalTok{:}
\NormalTok{           TN }\OperatorTok{+=} \DecValTok{1}
        \ControlFlowTok{if}\NormalTok{ y\_hat[i]}\OperatorTok{==}\DecValTok{0} \KeywordTok{and}\NormalTok{ y\_actual[i]}\OperatorTok{!=}\NormalTok{y\_hat[i]:}
\NormalTok{           FN }\OperatorTok{+=} \DecValTok{1}

    \ControlFlowTok{return}\NormalTok{ TP, FP, TN, FN}

\KeywordTok{def}\NormalTok{ precision(y\_actual, y\_hat):}
\NormalTok{    TP, FP, TN, FN }\OperatorTok{=}\NormalTok{ perf\_measure(y\_actual, y\_hat)}
    \ControlFlowTok{return}\NormalTok{ TP }\OperatorTok{/}\NormalTok{ (TP }\OperatorTok{+}\NormalTok{ FP)}


\KeywordTok{def}\NormalTok{ recall(contingency\_table, label):}
\NormalTok{    TP, FP, TN, FN }\OperatorTok{=}\NormalTok{ perf\_measure(y\_actual, y\_hat)}
    \ControlFlowTok{return}\NormalTok{ TP }\OperatorTok{/}\NormalTok{ (TP }\OperatorTok{+}\NormalTok{ FN)}

\KeywordTok{def}\NormalTok{ F\_measure(y\_actual, y\_hat, Beta}\OperatorTok{=}\DecValTok{1}\NormalTok{):}
\NormalTok{    P }\OperatorTok{=}\NormalTok{ precision(y\_actual, y\_hat)}
\NormalTok{    R }\OperatorTok{=}\NormalTok{ recall(y\_actual, y\_hat)}
    \ControlFlowTok{return}\NormalTok{ ((Beta}\OperatorTok{**}\DecValTok{2} \OperatorTok{+} \DecValTok{1}\NormalTok{) }\OperatorTok{*}\NormalTok{ P}\OperatorTok{*}\NormalTok{R) }\OperatorTok{/}\NormalTok{ (Beta}\OperatorTok{**}\DecValTok{2} \OperatorTok{*}\NormalTok{ P }\OperatorTok{+}\NormalTok{ R)}
\end{Highlighting}
\end{Shaded}

\hypertarget{logistic-regression}{%
\section{Logistic Regression}\label{logistic-regression}}

Logistic regression gives you the probability of a discrete outcome. You
can know how sure the model is of predicting a certain class.

In the case of computational linguistics, we classify our surface level
items, let's say words, as discrete units. We can use logistic
regression to uncover the probability of a word given some input instead
of just \href{../Semantic-Similarity/Co-occurrence.md}{counting words}.

The math for logistic regression looks like this:
\[z = \mathbf{x} \cdot \mathbf{w} + b\] - \(\mathbf{x}\) is the feature
representation of one input data point. For instance, a word as an
\href{Embeddings.md}{embedding} or a
\href{Connotations.md}{connotations} vector. - \(\mathbf{w}\) is a
vector of weights assigned to each feature depending on the importance
of each feature. - \(b\) is the bias term. This is an offset you start
off with. The bias is a scaler you add to all the items of a weight
vector. - \(z\) is the result. The weighted sum of the evidence for a
certain class. To get z you take the sum of the resulting vector you get
by doing \(\mathbf{x} \cdot \mathbf{w} + b\) to get a scaler.

Logistic regression is a linear
\href{../Classification.md}{classification} method. It is also a
\textbf{discriminative classifier}. This means it doesn't make use of
likelihood terms but attempts to directly compute the
\href{../Classification/Native\%20baiyes/Bayes\%20rule.md}{posterior},
i.e., \(p(c|d)\) No knowledge of how to generate documents of a class is
required.

Logistic regression is also a \textbf{probabilistic classifier}. Rather
than providing the best guess as the output, a probabilistic classifier
gives the probability that the answer is a certain answer. This is great
because it keeps options open and provides more information than just
one answer as it could be interesting that the answer is 30 \% to be
class one and 28 \% class 2 while the classes 3-100 are divided over the
other 32\%. Sometimes, however, there are too many classes to keep the
probabilities for every class. In that case, you have to prune the lower
probabilities.

\begin{figure}
\centering
\includegraphics{Pasted_image_20220603190300.png}
\caption{Probabilistic classifier}
\end{figure}

To get make a logistic regression, we need 4 components:

\hypertarget{feature-representation}{%
\subsection{Feature representation}\label{feature-representation}}

A \textbf{feature representation}: a vector of numeric or symbolic
features which encodes each input item.

\hypertarget{classification-function}{%
\subsection{Classification function}\label{classification-function}}

A \textbf{classification function} computes the probability of the class
given the input. This function you apply after you have calculated
\(z\). Remember, \(z\) was the sum of the vector you get by multiplying
the feature vector with the weights vector. This sum can be any value,
and we want to bring it to a certain range because we want to interpret
it as a probability. We are sort of
\href{../Data/Normalization.md}{normalizing} the outputs. A good choice
is the sigmoid (œÉ) function: \[y = \sigma(z)=\frac{1}{1+e^{-z}}\] After
applying the classification function (in this case \(\sigma\)) we want
to be able to interpret the output as probabilities. To this end, we
need to make sure that the sum of all the outputs of \(\sigma\) for
every class is 1. For binary problems you can simply do you can do
\(p(y=1) = \sigma(z)\) and then for the other class you can do
\(p(y=0) = 1 - p(y=1)\). So you just say the probability of the second
class is the probability that it is not the first class.\\
Now the probabilities for all classes sum up to one, and we can
interpret the output as the probability of a certain class given the
input. That is what we want. As the probability of one answer being the
correct one is 100\%.

\hypertarget{multi-class-problems}{%
\subsubsection{Multi class problems}\label{multi-class-problems}}

For multi class problems like the one below, making the output
probabilistic is harder.

\begin{figure}
\centering
\includegraphics{Pasted_image_20220603192841.png}
\caption{Multiclass probem}
\end{figure}

You make probabilistic by using the SoftMax function, with gives you the
probability that your output is a certain class given the input.

This works by estimating different weights and biases for every class
rather than a single vector, and then you divide every single vector by
the sum of all the vectors. When doing this the activation formula
becomes like this:
\[p(y=c|\mathbf{x}) = \frac{e^{\mathbf{w}_{c} \cdot \mathbf{x} + b_c}}{\sum\limits^{K}_{k~=~1}e^{\mathbf{w}_{k}~\cdot~x+b_{k}}}\]
This might look daunting, but it becomes more clear if we call it the
SoftMax function and replace with the part where you multiply in input
vector with the weights vector + the bias with z.

\[softmax(z_{i}) = \frac{e^{z_{i}}}{\sum\limits^{K}_{k~=~1}e^{z_{k}}} 1\leq i \geq K\]
The idea is to divide the classification function result of one class
(\(e^z\)) by the sum of all the results, which is just calculating a
probability. Doing this will always give you a result between 0 and 1.
So you get this:

\begin{figure}
\centering
\includegraphics{Pasted_image_20220603200100.png}
\caption{From sigmoid to probability}
\end{figure}

\hypertarget{graphical-representation}{%
\subsubsection{Graphical
representation}\label{graphical-representation}}

So above, you can see that you multiply the feature vector with the
weights vector to get the resulting new vector. Then you take the sum of
this vector, and you add the bias. Then you apply the
\textbf{classification function} to bring the outputs to a certain range
to be able to then turn the output of that into a probability.

\begin{figure}
\centering
\includegraphics{Pasted_image_20220603184638.png}
\caption{Logistic}
\end{figure}

\hypertarget{objective-function}{%
\subsection{Objective function}\label{objective-function}}

An \textbf{objective function} measures how close a model is getting to
learning what it is supposed to \href{../Other/Learning.md}{learn} i.e.,
the loss, error or cost, these are all the same. So you give this
function what was predicted and the right answer, and the objective
function will give a score which indicates how wrong the model is. You
want to minimise this score. The higher the error score, the worse the
model. The lower the error score, the better the model is at predicting
the data (for which you know the answers).

Multiple loss functions exist like Mean squared error (MSE) or cross
entropy etc. For these functions to work, you need to know what a good
result is or what the right answer is.

For the problem of finding word embeddings, we will use cross entropy.

\hypertarget{cross-entropy}{%
\subsubsection{Cross entropy}\label{cross-entropy}}

The cross entropy score is also known as \textbf{negative log
likelihood}. The cross entropy score is the lowest (no error) when the
correct class has a 100\% prediction, or basically when
\(\sigma(z) = 1\) for the correct class. Then, if we want the loss of
the whole data set, the idea is to calculate the average cross entropy
for each item to obtain the loss for the whole dataset. You can
interpret the resulting cross entropy number as the probability of
predicting the reality given an input.

\begin{figure}
\centering
\includegraphics{Pasted_image_20220603192054.png}
\caption{Cross entropy in practice with a binairy problem}
\end{figure}

\hypertarget{optimizer}{%
\subsection{Optimizer}\label{optimizer}}

An \emph{optimizer} updates the model based on the errors in the
decisions it makes. Basically, based on the output of the objective
function, it updates the weights. For instance, with gradient descent or
an evolutionary approach. For example, you could use the derivative of
the loss function to go move the weights into the direction which lowers
the loss.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{linear-boundaries}{%
\subsection{Linear boundaries}\label{linear-boundaries}}

A major downside of logistic regression that it can only make linear
decision boundaries even though the problem could just be not linearly
separable, for instance like the picture below:

\begin{figure}
\centering
\includegraphics{Pasted_image_20220603200910.png}
\caption{Xor seperation}
\end{figure}

To overcome this problem, you have to turn to
\href{../Prediction/Feed\%20forward\%20neural\%20networks\%20(FFNN).md}{feed
forward neural networks (FFNN)}. Basically, connecting multiple logistic
regression models together to create non-linearity. It is called feed
forward because the output of a lower layer only affect higher layers
and do not feed back into the same layer or lower layers. When you do
have future layers affecting previous layers you have a
\href{../Prediction/Recurrent\%20neural\%20network\%20(RNN).md}{Recurrent
neural network (RNN)}.

\hypertarget{classification}{%
\section{Classification}\label{classification}}

Classification is when you get a data point that you have not seen
before, and you have to assign it to a \textbf{class}. You do this based
on some representation of the data point and evidence of how features
relate to a class.

\hypertarget{types-of-classification-relevant-to-cl}{%
\subsection{Types of classification relevant to
CL}\label{types-of-classification-relevant-to-cl}}

\begin{itemize}
\tightlist
\item
  Language identification
\item
  Author profiling
\item
  Spam filters
\item
  E-mail routing
\item
  Topic detection
\item
  Sentiment analysis ‚Üí Is a text positive or negative
\item
  Fake news detection
\item
  \ldots{}
\end{itemize}

\hypertarget{how-do-we-do-this-first-attempt}{%
\subsection{How do we do this (first
attempt)?}\label{how-do-we-do-this-first-attempt}}

\hypertarget{rule-systems}{%
\subsubsection{Rule systems}\label{rule-systems}}

A crude solution is rule based. This means a lot of if elif else rules.
This gives you fine-grained control on classification outcomes, and we
always know \textbf{why} something happened.

Rule based is \textbf{robust} because you can make rules for even very
rare cases which would otherwise be hard to train for. It is also a good
way to add \textbf{intuitions} and \textbf{expert domain knowledge} into
a system. You also don't need large datasets.

But language is too complex for this and changes too much to do it like
this. It is probably \textbf{expensive} to write. You have to get the
domain knowledge and if it is
\href{../Languages/Ambiguity.md}{ambiguous} then you have to write a lot
of rules.

\hypertarget{supervised-learning}{%
\subsection{Supervised learning}\label{supervised-learning}}

The opposite of rule based systems is supervised
\href{../Other/Learning.md}{learning}. Here the idea is that you have a
large set of data points where you already know the correct
classification, and you try to build a model that can predict the points
you already have. The idea is that the model will learn the rules
itself. The hope is then that the model will also do well on new data,
this depends on bias and
\href{../Prediction/Overfitting.md}{overfitting}. This is the most
important. \textbf{We don't want to prove our self correct, we want to
make a generalizable model}. This removes the need to write rules, but
has many other problems.

\hypertarget{types-of-supervised-learning-classifiers}{%
\subsection{Types of supervised learning
classifiers}\label{types-of-supervised-learning-classifiers}}

\hypertarget{discriminative-classifiers}{%
\subsubsection{Discriminative
Classifiers}\label{discriminative-classifiers}}

Discriminative classifiers learn which features best predict a certain
class.

VS

\hypertarget{generative-classifiers}{%
\subsubsection{Generative Classifiers}\label{generative-classifiers}}

Generative classifiers learn a model of how the data are generated and
could because of this also generate new data. Classification happens by
choosing the class that most likely generated the data. It's like the
reverse.

\begin{figure}
\centering
\includegraphics{Pasted_image_20220216114704.png}
\caption{Generative vs Discriminative Classifiers}
\end{figure}

\hypertarget{linear-classifiers}{%
\subsubsection{Linear Classifiers}\label{linear-classifiers}}

Linear classifiers can only draw a linear decision boundary.

\hypertarget{non-linear-classifiers}{%
\subsubsection{Non-Linear classifiers}\label{non-linear-classifiers}}

Non-Linear classifiers can draw a non-linear decision boundary.

\begin{figure}
\centering
\includegraphics{Pasted_image_20220216114812.png}
\caption{Linear vs Non Linear Classifier}
\end{figure}

\hypertarget{evaluating-classification-models}{%
\section{Evaluating Classification
Models}\label{evaluating-classification-models}}

After you have made your \href{../Classification.md}{classification}
system, how do you evaluate it against other options?

\hypertarget{intrinsic-evaluation-2}{%
\subsection{Intrinsic Evaluation}\label{intrinsic-evaluation-2}}

Define a metric and check which system does best.

\hypertarget{extrinsic-evaluation-2}{%
\subsection{Extrinsic Evaluation}\label{extrinsic-evaluation-2}}

Embed a tool in a larger system and check how much performance on a
downstream task improves given the output of two different versions of
your tool. So you have the classifier, but you see how much the
performance on something the classifier would be used for improves.

\hypertarget{example-6}{%
\subsection{Example}\label{example-6}}

Let's say spam detection.

\begin{itemize}
\tightlist
\item
  Intrinsic evaluation would be we have a bag of spam emails and normal
  emails. Then we could say the classifier was correct 95\% of the times
  and incorrect 5\% of the times.
\item
  Extrinsic evaluation would be like we got 66\% decrease in people that
  got scammed that used our tool.
\end{itemize}

So extrinsic is not the tool itself you evaluate, but you see in how the
tool helps to achieve a goal it was made for and if it helps at all.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

We would like a spam filter to be perfect, but this is not going to
happen. Either you classify emails as spam when it's not spam, or you
miss emails as spam when it is spam. You have to choose which one is
worse for your application.

\hypertarget{intrinsic-evaluation-methods}{%
\section{Intrinsic evaluation
methods}\label{intrinsic-evaluation-methods}}

Whenever you get results from your model, you get:

\begin{itemize}
\tightlist
\item
  True positives (TP): Correctly classified that it was this class.
\item
  True negatives (TN): Correctly classified that it was not this class.
\item
  False positives (FP): Incorrectly classified that it was this class.
\item
  False negatives (FN): Incorrectly classified that it was not this
  class.
\end{itemize}

From these, we can come up with intrinsic evaluations.

\begin{figure}
\centering
\includegraphics{Pasted_image_20220216130019.png}
\caption{Confusion matrix}
\end{figure}

\hypertarget{accuracy}{%
\subsection{Accuracy}\label{accuracy}}

Accuracy is the number of correctly classified points. Simple.

\hypertarget{precision}{%
\subsection{Precision}\label{precision}}

The first one is precision. The idea of this is that you divide the true
positives by all the data points that were classified as this class. So
this means: \[Precision = \frac{TP}{TP+FP}\]

This can be seen as a percentage of all points that were classed as this
class that was correct. So precision really punishes false positives. It
doesn't matter if you missed some true positives, as long as you don't
have a lot of false positives.

To use this measure you need to classify at least one true positive
otherwise you try to divide 0 and then the score is 0.

\hypertarget{recall}{%
\subsection{Recall}\label{recall}}

Recall is the proportion of correctly classified data points out of the
data points which belonged to that class.

\[Recall = \frac{TP}{TP+FN}\]

You can see recall of the number of correctly classified spam emails out
of the emails that should have been classified as spam. You don't care
about the mistakes, but you care about that you catch everything you
have to catch.

With recall, it doesn't matter how often you wrongly guessed, as long as
you got all data points which belonged to the class (the true
positives). So this punishes false negatives. I think false negatives
are the worst.

There is another
\href{https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall}{explanation
by google here} about recall and precision.

\hypertarget{f-measure}{%
\subsection{F-Measure}\label{f-measure}}

F-Measure combines precision and recall into a new, shiny formula.
F-Measure is described as the harmonic mean between precision and
recall. The harmonic mean is more conservative than the arithmetic mean.

This is the formula:

\[F~Measure = \frac{(Œ≤^{2} + 1) \cdot Precision \cdot Recall}{Œ≤^{2}  \cdot Precision + Recall}\]

The idea of the Œ≤ is a weight that you can use to make either precision
or recall more important. If you do Œ≤ \textgreater{} 1 you make recall
more important and Œ≤ \textless{} 1 than you make precision more
important.

Typically, you don't make one more significant than the other, and you
just set \(Œ≤ = 1\). When you don't, you call the F-Measure score the
\textbf{F1 score}. Setting Œ≤ is mostly based on domain knowledge. If you
don't have it, then just set it to 1.

\hypertarget{code}{%
\subsubsection{Code}\label{code}}

Here is code to get the scores in python.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ precision(contingency\_table, label):}
\NormalTok{    index }\OperatorTok{=}\NormalTok{ labels2ids[label]}
\NormalTok{    result }\OperatorTok{=}\NormalTok{ contingency\_table.T[index]}
    
\NormalTok{    tp }\OperatorTok{=}\NormalTok{ result[index]}
    
\NormalTok{    mask }\OperatorTok{=}\NormalTok{ np.ones(}\BuiltInTok{len}\NormalTok{(result), }\BuiltInTok{bool}\NormalTok{)}
\NormalTok{    mask[index] }\OperatorTok{=} \VariableTok{False}
    
\NormalTok{    fp }\OperatorTok{=} \BuiltInTok{sum}\NormalTok{(result[mask])}
    
    \ControlFlowTok{return}\NormalTok{ tp }\OperatorTok{/}\NormalTok{ (tp }\OperatorTok{+}\NormalTok{ fp)}


\KeywordTok{def}\NormalTok{ recall(contingency\_table, label):}
\NormalTok{    ct }\OperatorTok{=}\NormalTok{ contingency\_table}
\NormalTok{    index }\OperatorTok{=}\NormalTok{ labels2ids[label]}
    
\NormalTok{    result }\OperatorTok{=}\NormalTok{ ct.T[index]}
    
\NormalTok{    tp }\OperatorTok{=}\NormalTok{ result[index]}
    
\NormalTok{    fn }\OperatorTok{=} \BuiltInTok{sum}\NormalTok{(}\BuiltInTok{map}\NormalTok{(}\KeywordTok{lambda}\NormalTok{ x : x[[}\BuiltInTok{len}\NormalTok{(ct)]](ct}\OperatorTok{|}\VariableTok{True} \ControlFlowTok{if}\NormalTok{ i }\OperatorTok{==}\NormalTok{ index }\ControlFlowTok{else} \VariableTok{False} \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(ct}\OperatorTok{|}\NormalTok{[}\BuiltInTok{len}\NormalTok{(ct}\OperatorTok{|}\NormalTok{[}\BuiltInTok{len}\NormalTok{(ct}\OperatorTok{|}\NormalTok{[}\BuiltInTok{len}\NormalTok{(ct)](ct))])])), ct.T))}
    
    \ControlFlowTok{return}\NormalTok{ tp }\OperatorTok{/}\NormalTok{ (tp }\OperatorTok{+}\NormalTok{ fn)}
    


\KeywordTok{def}\NormalTok{ F\_measure(contingency\_table, label, Beta}\OperatorTok{=}\DecValTok{1}\NormalTok{):}
\NormalTok{    P }\OperatorTok{=}\NormalTok{ precision(contingency\_table, label)}
\NormalTok{    R }\OperatorTok{=}\NormalTok{ recall(contingency\_table, label)}
    \ControlFlowTok{return}\NormalTok{ ((Beta}\OperatorTok{**}\DecValTok{2} \OperatorTok{+} \DecValTok{1}\NormalTok{) }\OperatorTok{*}\NormalTok{ P}\OperatorTok{*}\NormalTok{R) }\OperatorTok{/}\NormalTok{ (Beta}\OperatorTok{**}\DecValTok{2} \OperatorTok{*}\NormalTok{ P }\OperatorTok{+}\NormalTok{ R)}
\end{Highlighting}
\end{Shaded}

\hypertarget{macro-averaging}{%
\subsubsection{Macro averaging}\label{macro-averaging}}

When there are more than two classes, we compute the F-measure for all
classes separately and then average them, assigning equal importance.
\textbf{This is useful when good performance is necessary in all the
classes}, regardless of the frequency in which they appear. Because if
you do it like this, one class that has bad performance will decrease
the averaged F1 score a lot.

\hypertarget{micro-averaging}{%
\subsubsection{Micro averaging}\label{micro-averaging}}

With micro averaging you collect all the decisions for all the classes
in a single \href{Contingency\%20table.md}{Contingency table} and then
compute precision and recall from that table. This is useful when good
performance is more important for the most frequent classes.

\hypertarget{statistical-test}{%
\subsection{Statistical test}\label{statistical-test}}

You can often not use statistical test like t-test because often
classification samples are not normally distributed.

\hypertarget{bootstrapping}{%
\subsection{Bootstrapping}\label{bootstrapping}}

Bootstrapping is when you artificially increase the number of test sets
by drawing a lot of samples from a given test set with replacement (use
multiple times), perform your task, record the score, factor the
performance of the whole test set, then simply check the percentage of
runs in which a system beats the other. This is not super important.

So you pick samples of data points of the entire test set and then run
multiple times. You can then see if one system is more better than the
other on many samples.

\hypertarget{dependency-parsing}{%
\section{Dependency parsing}\label{dependency-parsing}}

With dependency parsing, you examine the dependencies between the
phrases of a sentence in order to determine its grammatical structure.
You parse an input sequence by staring at the root of the input, and
then you jump from token to token based on grammatical binary
relationships (dependencies).

If you have jumped to every part of the sentence, then you have parsed
the sentence. The \href{Parse\%20Tree.md}{parse tree} is the sentence
you get a **directed graph.

\begin{figure}
\centering
\includegraphics{Pasted_image_20220526224636.png}
\caption{Dependency parsing 1}
\end{figure}

\begin{figure}
\centering
\includegraphics{Pasted_image_20220526224656.png}
\caption{Dependency parsing 2}
\end{figure}

\begin{figure}
\centering
\includegraphics{Pasted_image_20220526224725.png}
\caption{Dependency parsing 3}
\end{figure}

The description of images above: You start with the root, which then
goes into the main verb of the sentence. Then you go to the subject of
that verb. Then you get the argument of the verb. Then you get modifiers
of the argument. For instance morning.

So there is no hierarchy, you just describe the sentence based on the
relationships which exists between the tokens. It is mostly based on
structure and not on meaning, but you can also base on meaning.

\hypertarget{what-is-this-for}{%
\subsection{What is this for}\label{what-is-this-for}}

Dependency parsing explicitly and directly encode information about
relationships across tokens which are often quite buried and hard to see
in constituent based trees like a context free grammar.

\textbf{Dependency parsing also deals very well with morphological
complex languages with free word order} (which is not like English)
without needing to have super specific grammar. For instance, an affix
could mark the subject. For these types of languages,
\href{Chunking.md}{chunking} or \href{Constituency.md}{constituency}
based parsing works worse. So it's like a set of words connected
together by grammar binary relationships.

The main content words in the sentence are called heads, and the
relations are based on the heads.

\hypertarget{types-of-relations}{%
\subsection{Types of relations}\label{types-of-relations}}

\hypertarget{clauses}{%
\subsubsection{Clauses}\label{clauses}}

Syntactic roles like subject, object. Like the verb

\hypertarget{modifier}{%
\subsubsection{Modifier}\label{modifier}}

There are modifiers of heads like blue sky where blue is a modifier of
the head sky.

\hypertarget{data-structure}{%
\subsection{Data structure}\label{data-structure}}

After you have analysed a sentence, you can store it as a graph. You can
store a graph as a set of tuples with two items.
\(\{(a,b), (b,c), (c,d)\}\). So in this sentence there would be an arc
from \(a\) to \(b\), \(b\) to \(c\) and \(c\) to \(d\).

We call this here \(G\) with \(G = (V,A)\) where \(V\) is a set of
vertices (tokens in the vocabulary or stems and affixes) so like the
sentence or what you can connect. \(A\) is a labelled ordered pais of
vertices (the arcs). This includes the type of the binary relationship
as well.

\(V\) is what you can connect and \(A\) is how it is connected.

\hypertarget{dependency-tree}{%
\subsection{Dependency tree}\label{dependency-tree}}

A dependency tree is a \textbf{directed graph} where

\begin{itemize}
\tightlist
\item
  There is one root node with no incoming arc.
\item
  Every other vertex has exactly one incoming arc.
\item
  There is only one path from the root to each vertex in V.
\end{itemize}

These conditions are necessary to have the dependency tree in the way we
care about.

\hypertarget{how-to-get-these-models}{%
\subsection{How to get these models?}\label{how-to-get-these-models}}

Using special \href{../Data/Treebank.md}{treebanks}. Dependency
treebanks. Linguists have created \href{../Data/Corpus.md}{corpora} with
annotated typed binary directed decency relations that are used to train
dependency parsers.

If you don't have a dependency treebank, you can also use deterministic
approaches with a constituent based treebank, but this does not work
very well. A trained linguist can do better.

\hypertarget{evaluation-1}{%
\subsection{Evaluation}\label{evaluation-1}}

\hypertarget{label-score}{%
\subsubsection{Label score}\label{label-score}}

This checks if it went to the correct label. So it's ok if you come from
the good head as long as where the relationship goes is the correct
label.

\hypertarget{unlabelled-score}{%
\subsubsection{Unlabelled score}\label{unlabelled-score}}

This does not check if it went to the correct label. So it's ok if you
come from the correct head as long as where the relationship goes is the
correct head.

\hypertarget{label-detached-score}{%
\subsubsection{Label detached score}\label{label-detached-score}}

This combines the labelled and unlabelled scores.

This scores how many estimated dependencies have the same head and
dependent as the gold standard, and also the correct type.

\hypertarget{probabilistic-context-free-grammar}{%
\section{Probabilistic Context Free
Grammar}\label{probabilistic-context-free-grammar}}

A Probabilistic Context Free Grammar is a
\href{Context\%20free\%20grammars.md}{Context free grammar} which also
has probabilities assigned to every production rule, which indicates how
likely the right-hand side (RHS) of the rule is transformed into the
left-hand side (LHS). So each rule looks like this:

\[X \rightarrow YZ[p(YZ|X)]\] or another example:
\[X \rightarrow y[p(y|X)]\] So you can see the second part
(p(y\textbar X)) means the chance of y given X. Or the chance of YZ
given X.

\hypertarget{getting-the-probability-of-a-parse-tree}{%
\subsection{Getting the probability of a parse
tree}\label{getting-the-probability-of-a-parse-tree}}

With a PCFG, it is possible to calculate the probability of a whole
\href{Parse\%20Tree.md}{parse tree}. So how is this done? With this
formula: \[P(T, s) = \prod^{|T|}_{i=1}{p(RHS_i|LHS_i)}\] \textgreater{}
T = the parse tree \textgreater{} s = a sentence \textgreater{} p(T,s) =
the joint probability of the parse and the sentence \textgreater{}
\(\prod\) = multiply together just like \(\sum\limits\)

The left-hand side is asking what is the probability of this parse tree
on this sentence?

Then you just multiply the probability of each rule to expand the
non-terminals in the parse tree T. Here, we make the assumption that one
grammar rule appearing is independent of another grammar rule appearing.
This allows us to multiply the probabilities of the grammar rules
together. So you basically go through the parse tree and at every step
you multiply the probabilities of all the production rules you used to
parse the sentence.

With an \href{Ambiguity.md}{ambiguous} sentence, there will be more than
one \href{Parse\%20Tree.md}{Parse Tree} possible. So in that case you
just use the above formula to calculate the probability of each possible
parse tree, and you pick the one with the highest probability. This is
the maximum probability tree is then called \(\hat{T}\).

Doing the multiply for every parse tree is captured in the
\textbar T\textbar{} above the \(\prod\) here you basically say for all
possible parse trees that we can derive from this sentence multiply
this.

We call the parse trees of a sentence the parse trees which
\textbf{yield} s.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Calculating the probability this way actually gives you two things. It
gives you the probability of the parse tree occurring on this sentence:
\(p(T,s)\) but it also gives you the probability of the parse tree in
general in this PCFG. So why is this?

Basically what we did above is this: \[P(T,s) = p(s|T_{i})p(T_{i})\]
This is just
\href{../Classification/Native\%20baiyes/Bayes\%20rule.md}{Bayes rule}.
But in this specific case of parse threes we can actually reduce this to
\[p(T_{i},s) = p(T_i)\] because \(p(s|T_i)\) is always 1 because we only
consider parse trees which can actually parse the sentence. So the
probability of the parse tree given the sentence is always 100\%.
Because of this, we can say that we can also get the probability of the
parse tree in general. Now this allows us to go backwards with this
because an equality goes both ways. We can now say that to get the
probability of a parse tree and sentence together, we only have to
calculate the probability of the parse tree.

So once you have seen a parse tree before for another sentence, you know
the probability of it already.

We can use this with a dynamic programming solution to get all the
possible parse trees.

So basically we can just multiply production rules.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

In peoples heads it seems to work similar, that they build up parse
trees out of constituents of the grammer based on probabilities. When a
parse tree then is weird like. The cabin behind the horse race fell.
Apparently, fell is not expected there. This confuses humans and
requires more processing power. AI which generates language should keep
this cost in mind.

\hypertarget{how-to-make-pcfg}{%
\subsection{How to make PCFG?}\label{how-to-make-pcfg}}

To do this you need to traverse a parse tree which requires tree banks.

You can also use have probabilistic \href{CKY.md}{CKY}. The changes are
that you represent a sentence using a tensor t of the shape n+1 x n+1 x
V where n is the length of the sentence and V is the size of the
non-terminals. Each cell t{[}i,j,A{]} contains the probability of type A
to span positions from i to j. So the idea is to add another dimension.
There is also this rule change:

\begin{figure}
\centering
\includegraphics{Pasted_image_20220526203433.png}
\caption{CKY modification}
\end{figure}

So whenever you update table{[}i,j,A{]} because you found a parse from i
to j with a higher probability you need to record where you were and
were coming from in back{[}i,j,A{]} so that if and when you find the
state S in the final cell, you can recompute the best parse by going
through the best sub parses.

\hypertarget{deriving-the-probabilities}{%
\subsection{Deriving the
probabilities}\label{deriving-the-probabilities}}

You derive the probabilities for the productions by using
\href{../Data/Treebank.md}{treebanks}. The idea is to count how many
times each expansion of a non-terminal occurs and normalize by the
occurrence of the non-terminals. So you basically count how often a
certain non-terminal goes to non-terminal and terminal out of all the
options and divide by all the options. Just like the counting
\href{N-grams.md}{N-grams} but now with by looking at each LHS going to
which RHS.

\hypertarget{problems-of-pcfgs}{%
\subsection{Problems of PCFGs}\label{problems-of-pcfgs}}

Two assumptions have to be made.

\begin{itemize}
\tightlist
\item
  \textbf{Independence} of productions occurring.

  \begin{itemize}
  \tightlist
  \item
    Of course, it is not independent. But this kind of shortcoming of
    the whole context free grammar and trying to use it for natural
    languages as the grammar of a natural language is not context free.
    So independent assumption is caused by the context freedom
    assumption.\\
  \item
    This is needed to be able to calculate easily.
  \end{itemize}
\item
  (P)CFG \textbf{don't know about lexical items,} which results into
  poor solutions to structural ambiguities. Humans seem to have no
  problems with this, and this often solves ambiguity. So it ignores
  rules which are based on specific lexical items and rules. But of
  course, the meaning has large effects on the probabilities.

  \begin{itemize}
  \tightlist
  \item
    For instance The guy looked at the girl with the telescope AND the
    guy waved at the man with the telescope. Because of the meaning it
    is clear to humans what is what but the computer won't know because
    it doesn't know about the meaning. It will always prefer one of
    them. So either the guy is holding the telescope or the girl. The
    model will always choose the same and sometimes will be wrong.
  \end{itemize}
\end{itemize}

\hypertarget{to-try-to-solve-the-independence}{%
\subsubsection{To try to solve the
independence:}\label{to-try-to-solve-the-independence}}

Another problem with the current setup of PCFG is that we never change
the probability of a production. This means that if you always take the
highest probability that you will also be garandeert to be wrong in the
other cases. At least you will be right most of the cases.

One solution to this is making more productions which are more finely
tuned for different types of grammar productions. So you have one where
the object is at the start and one where the verb is at the start, for
instance. This gives you separate distributions. However, this requires
very fine-tuned annotations which don't really exist.

However, you can do \textbf{parent annotations} where you annotate text
by the parent terminal it came from. You can also do this for lexical
categories, which I think makes it more clear. So you can look how often
cat was RHS of Noun and how often it was RHS of verb. Then you can
assign different productions to each of those, so you have cat\_verb and
cat\_noun. However, this leads to overfitting because if you split too
much the results are not generalizable, so you have to pick the right
granularity for splits of non-terminals and pre terminals or use
algorithms that do this for you like split and merge but it's not
covered.

\hypertarget{to-try-to-solve-the-no-lexical}{%
\subsubsection{To try to solve the no
lexical:}\label{to-try-to-solve-the-no-lexical}}

You can also try to make productions which also involve the lexical
annotations. But this becomes infeasible fast, and you overfit fast.

In order to deal with sentences and complex grammar, we have to make an
assumption to be able to compute it, but this also means we won't get a
perfect model because we made the assumptions.

If you want to take into account lexical information, you have to look
into assigning vectors to words to encode their meaning. This is called
\href{../Semantic-Similarity/Vector\%20semantics.md}{Vector semantics}.

\hypertarget{parts-of-speech-pos}{%
\section{Parts-of-Speech (PoS)}\label{parts-of-speech-pos}}

PoS are clusters of words which have a similar behaviour in a language.
For instance, they have a similar context of occurring, or they can be
combined with the same set of morphological affixes. PoS tags are also
called lexical clusters.

You should think of examples as: Nouns, Verbs, Adjectives, Pronouns etc.
These parts of speech again contain smaller parts which are even more
consistent with themselves. For instance, past verbs, present verbs or
irregular verbs etc. Different languages have different PoS tags. The
closer a language, the more close the PoS.

\hypertarget{open-class-words}{%
\subsection{Open Class words}\label{open-class-words}}

One very famous PoS is the open class of words. This includes nouns,
verbs, adjectives, adverbs. This class is called \textbf{productive}
because it changes often. New words are added to this class all the
time. Other words disappear or go undercover. Typically, open class
words are content words. This means the words that convey the contents
of a message. Most languages have all 4 major open classes.

\hypertarget{closed-class-words}{%
\subsection{Closed Class Words}\label{closed-class-words}}

This class is words is closed. It includes conjunctions, determiners,
pronouns, prepositionsm, etc. This class is \textbf{not productive}. It
barley changes and is basically fixed. The words in this class can be
contained in a single courses.

\hypertarget{special-corpora}{%
\subsection{Special corpora}\label{special-corpora}}

There are special corpora with information about the PoS of every word
in it. Examples are: - Brown Corpus -- 1 million words, balanced - WSJ
-- 1 million words, news articles - Switchboard -- 2 million words,
transcribed phone conversations.

Often these annotations are hand corrected versions of an automated
system which assigned PoS to words. Doing this is called PoS tagging.

\hypertarget{pos-tagging}{%
\section{PoS Tagging}\label{pos-tagging}}

The process of \textbf{automatically} assigning PoS tags to words in a
corpus. This process takes a tokenized corpus as input and outputs a
sequence of PoS tags for each input token.

First of all why not just have a list of words and their PoS tags? We
could even use dictionaries for this. We can't do this because of
\href{Ambiguity.md}{ambiguity}. PoS tagging aims to resolve this
ambiguity.

PoS tagging is useful for \href{Parsing.md}{parsing}, named entity
recognition and co-reference resolution and more!

\hypertarget{baseline}{%
\subsection{Baseline}\label{baseline}}

Most of the \href{../Data/Type.md}{types} in a corpus are nouns. The
baseline is to assume everything is a noun. This will get you an
accuracy of around 60\% on types, but much lower on tokens.

\hypertarget{most-frequent}{%
\subsection{Most frequent}\label{most-frequent}}

We can already do better by looking at the most frequent examples of
tags. Cat can be a verb, but it is almost always a noun. If you hit an
ambiguous word, then you just assign the most frequent PoS tag (which is
noun). However, if you do this you are bound to make mistakes because
you are ignoring the lower frequency explicitly.

\hypertarget{context-2}{%
\subsection{Context}\label{context-2}}

Can we do better? We can look at words that come before to predict a PoS
just like with \href{../Prediction/Language\%20Modeling.md}{Language
Modeling}. You can also look at words that come after (preceding words).

\hypertarget{parse-tree}{%
\section{Parse Tree}\label{parse-tree}}

A parse tree is a proof that you can parse a sentence of a language
under a certain \href{Grammar.md}{grammar}. If the sequence is properly
formed according to the productions of a certain grammar, then you can
create the parse tree to show how you can \href{Parsing.md}{parse} it.
Parse trees are basically the proof of a parse while also being a useful
data structure. Parse trees are useful because one you have them,
computers can iterate over them while being able to make assumptions the
grammar gives them.

You make a parse tree by using the \href{Grammar.md}{grammar} rules to
make a path from the input sequence to non-terminals to terminals. Then
you can then visualize these paths with a parse tree.

\begin{figure}
\centering
\includegraphics{Pasted_image_20211127140252.png}
\caption{Parse Tree example}
\end{figure}

When multiple parse trees exist under a grammar for the same input, you
have a grammar with \href{Ambiguity.md}{ambiguity}. One way to resolve
this ambiguity is by using a
\href{Probabilistic\%20Context\%20Free\%20Grammar.md}{Probabilistic
Context Free Grammar}.

\hypertarget{grammars}{%
\section{Grammars}\label{grammars}}

A grammar is a language for defining \href{../Languages.md}{languages}.
If you write rules for a language, then these rules are also called the
grammar. The individual rules you write are called a production.
Grammars define the structure of the
\href{../Data/Sentences.md}{sentences} in the language.

A grammar consists of multiple productions. Productions can be seen as
rewrite rules. If the left side matches, you can replace it with the
other side. Also, if you already have something that is part of the
language, you can make more things in the language.

Formally you have: - N - A finite set of non-terminals (states) -
\(\sum\limits\) - A finite set of terminals, disjoint from N - P - A
finite set of production rules - \(S \in N\) A distinguished start
non-terminal state from N.

The symbol from the \href{Alphabet.md}{alphabets} are also called
\textbf{terminals}. \(\epsilon\) is also a terminal. Remember, epsilon
is the empty input.

The grammar makes use of auxiliary symbol which is called
\textbf{non-terminals}. These are not part of the alphabet and hence
cannot be part of the final word/sentence. The non-terminals are
supposed to be replaced with terminals when you're parsing. This is
called a rewrite rule. Non-terminals can be of two types.

\begin{itemize}
\tightlist
\item
  Pre-terminals like PrN and V are \href{Parts\%20of\%20Speech.md}{Parts
  of Speech}. Or atomic non-terminals. The production rules indicate
  which sequences they can generate.
\item
  \href{Constituency.md}{Constituents} (NP and VP) are abstract units
  which absolve complex syntactic functions.
\end{itemize}

\begin{figure}
\centering
\includegraphics{Pasted_image_20220314185901.png}
\caption{Tree of parts of speech tags}
\end{figure}

The grammar rules are kind of defined like inductive rules.

The idea is that you replace the non-terminals with a parse tree or an
abstract syntax tree. This abstract syntax you can then evaluate.

The start state is nice because if you can get from an input to the
start state by following the rewrite rules, then you know your input is
in the language. The start state represents to most abstract place in
your language.

We usually read the rules left to right, butt you can always go back if
you want.

\hypertarget{writing-down-the-rules}{%
\subsection{Writing down the rules}\label{writing-down-the-rules}}

There are different ways to write down the rules. There are also
different formats. Once such format is the
\href{Chomsky\%20Normal\%20Form.md}{Chomsky Normal Form}.

Also remember that the rules in a grammar for a language HAS to be
finite. Otherwise, you would also have to consider another rule.

You can use non-terminals on both sides. This allows for good
abstraction. This is done using \href{Constituency.md}{Constituency}.

\hypertarget{examples-1}{%
\subsection{Examples}\label{examples-1}}

If you have this grammar:

\begin{itemize}
\tightlist
\item
  A ‚Üí aAa
\item
  A ‚Üí bAb
\item
  A ‚Üí \(\epsilon\)
\end{itemize}

Then this word is in the language: abaaba. Because you can parse it like
this: 1. abaaba 2. abAba 3. aAa 4. A

Usually the non-terminals are capitalized.

\hypertarget{examples-2}{%
\subsection{Examples}\label{examples-2}}

\hypertarget{palindrome-example}{%
\subsection{Palindrome example}\label{palindrome-example}}

\begin{figure}
\centering
\includegraphics{Pasted_image_20211127131012.png}
\caption{Palindrome grammar}
\end{figure}

The idea is that it sort of does not matter what P is here.

When still talking about palindrome.

\begin{figure}
\centering
\includegraphics{Pasted_image_20211127130856.png}
\caption{Palindrome grammar rewritten}
\end{figure}

S is a start.

So you can define a grammar like this:

\begin{figure}
\centering
\includegraphics{Pasted_image_20211127131455.png}
\caption{Defining grammar rules}
\end{figure}

This is an example of a grammar which only allows strings of a and b.

This is a special grammar where there is only non-terminal on the left.
This is a \textbf{Context Free} grammar. If there are multiple
non-terminal characters than you have a context-sensitive grammar.

In this course we are only looking at context free languages. There are
a lot of languages that we can't describe with this. There are also
languages that you could never describe with a grammar. There are sadly
more languages that you can't describe with grammar. Most programming
languages are context free.

You can also have more grammars for the same language.

\begin{figure}
\centering
\includegraphics{Pasted_image_20211127131955.png}
\caption{Grammars example}
\end{figure}

This makes sense because the CPU also looked very different for
everyone.

\hypertarget{more-examples-with-digit}{%
\subsection{More examples with digit}\label{more-examples-with-digit}}

\begin{figure}
\centering
\includegraphics{Pasted_image_20211127132201.png}
\caption{Digit grammar}
\end{figure}

Here there is one non-terminal Dig that can be rewritten to each digit.

We can also now define Digs which is at least two digits. With this we
can define any sequence of numbers.

\begin{figure}
\centering
\includegraphics{Pasted_image_20211127132647.png}
\caption{Making constituents with dig}
\end{figure}

So we can do Digs ‚Üí Dig* to say Digs is 0 or more Dig

What about numbers that don't start with 0?

We can make Dig-0 ‚Üí 1 \textbar{} 2 \textbar{} 3 \textbar{} 4 \textbar{}
5 \textbar{} 6 \textbar{} 7 \textbar{} 8 \textbar{} 9 Then with that we
can define a sequence of natural numbers like: Nat ‚Üí 0 \textbar{} Dig-0
\textbar{} Digs

Then we can define integers by defining a sign: Sign ‚Üí +\textbar- Int ‚Üí
Sign Nat \textbar{} Nat

This means an Int is a natural number with an optional sign. There is a
shorthand for this like:

Int -\textgreater{} Sign ? Nat

This means the sign can be there but does not have to be.

\hypertarget{letters}{%
\subsection{Letters}\label{letters}}

Letters are a lot like digits, but we just have more things.

SLetter ‚Üí a\textbar b\textbar\ldots.\textbar z CLetter ‚Üí
A\textbar B\textbar\ldots\textbar Z

This also means that you before you write down the grammar you have to
specify what you can use as non terminal.

Letter ‚Üí SLetter \textbar{} CLetter

Now we can set up something like an identifier. Like a var name.

Identifier -\textgreater{} Letter \textbar{} AphaNum* AlphaNum
-\textgreater{} Letter \textbar{} Dig

You can easily extend this with \_ or other things you want to be in
identifiers. Honestly I really like this so far!

The reason we do a letter first is that this way the compiler does not
get confused that something is actually a number like a Nat.

\hypertarget{fragment-of-c}{%
\section{Fragment of C}\label{fragment-of-c}}

With this we can actually define programming languages grammar to.

Var -\textgreater{} Identifier Op -\textgreater{} Sign Stat
-\textgreater{} Var = Expr; \textbar{} if (Expr) Stat else Stat
\textbar{} While (Expr) Stat Expr -\textgreater{} Integer \textbar{} Var
\textbar{} Expr Op Expr

\begin{figure}
\centering
\includegraphics{Pasted_image_20211127142739.png}
\caption{Concrete and abstract syntax}
\end{figure}

\hypertarget{ambiguity}{%
\section{Ambiguity}\label{ambiguity}}

When something is ambiguous, it means that it can mean multiple things.
Additionally, it is also not really clear what the correct meaning
should be.

Often humans can deal with things like ambiguous sentences by looking at
the context of the sentence or the speaker's face expression and body
language but dealing with ambiguity is really hard for computers. Rule
based systems can not deal well with ambiguity.

When something is unambiguous, the meaning is clear and can only mean
one thing.

\hypertarget{structural-ambiguity}{%
\subsection{Structural ambiguity}\label{structural-ambiguity}}

Structural ambiguity is~a kind of ambiguities which occurs when a
phrase, clause or sentence can be given two or more different
interpretations as a result of the arrangement of words (the structure).

\hypertarget{example-7}{%
\subsubsection{Example}\label{example-7}}

An example of structural ambiguity can be seen in this sentence:
\texttt{I\ shot\ an\ elephant\ in\ my\ pajamas}

Were you in your pyjamas when you shot the elephant, or did you shoot
the elephant into your pyjamas? This ambiguity can only be solved by
looking at the meaning. But that is hard for computers, and so you can
also try to solve it by looking at what types of \textbf{structures}
appear more often in a language. Typically the propositional phrase (the
in my pyjamas part) modifies the subject. So if you know that bias, then
you can try to solve structural ambiguity using a language models based
on structures.

\hypertarget{coordination-ambiguity}{%
\subsection{Coordination Ambiguity}\label{coordination-ambiguity}}

An example of this is
\texttt{Tasty\ sandwiches\ and\ flowers\ make\ my\ grandma\ happy}. Do
we mean tasty sandwiches and tasty flowers? Or only tasty sandwiches.
Both are valid. But we probably don't eat flowers, so just tasty
sandwiches. This can be resolved with \href{../Data/Thesaurus.md}{Lesk
similarity} or by trying to
\href{../Semantic-Similarity/Vector\%20semantics.md}{represent words as
vectors}.

\hypertarget{resolution}{%
\section{Resolution}\label{resolution}}

We can show that there is ambiguity with the \href{CKY.md}{CKY}
algorithm. However, when we know that a sentence is ambiguous, how do we
pick one? We can look at the structure to assign probabilities to
different options and pick the one with the highest one. One step
deeper, we can assign a probability to every production rule in the
\href{Grammar.md}{Grammar}. Then with that you can assign a probability
to every possible right-hand side of a production rule
\href{../Classification/Native\%20baiyes/Bayes\%20rule.md}{given} the
left-hand side. This allows you to calculate the likelihood of a parse
tree given the grammar.

When we assign probabilities to rules in a
\href{Context\%20free\%20grammars.md}{context free grammar}, it becomes
a \href{Probabilistic\%20Context\%20Free\%20Grammar.md}{probabilistic
context free grammar} (PCFG). So a PCFG is a CFG whose rules are
augmented with probabilities.

\hypertarget{natural-languages}{%
\section{Natural languages}\label{natural-languages}}

Natural languages are \href{Languages.md}{languages} that are:

\begin{itemize}
\tightlist
\item
  Conventional
\item
  A set of related systems
\item
  There is redundancy
\item
  Subjected to change over time
\item
  Context dependent.
\item
  \href{Ambiguity.md}{Ambiguous}
\item
  Follow a \href{Zipfian\%20Distribution.md}{Zipfian Distribution} with
  their \href{../Data/Words.md}{words}.
\end{itemize}

Natural languages aren't: - Formal logic - A Programming language - A
Machine language - Mostly static - \href{Ambiguity.md}{unambiguous}

\hypertarget{examples-3}{%
\subsection{Examples}\label{examples-3}}

Examples of natural languages are: English -- Dutch -- Spanish -
Japanese -- Chinese -- German -- Spanish etc.

\hypertarget{synonyms}{%
\section{Synonyms}\label{synonyms}}

Synonyms are 2 or more \href{../Data/Words.md}{Words} which mean the
same thing, or at least mean something very similar. For instance,
scream and screech mean about the same thing but are different words.

More specifically, you can say that two \href{../data/lemma.md}{word
forms} a synonym when they share (almost) the same
\href{../data/lemma.md}{word sense}.

\begin{quote}
\textbf{Formal definition of the book page 104} More formally, two words
are synonymous if they are substitutable for one another in any sentence
without changing the truth conditions of the sentence, the situations in
which the sentence would be true. We often say in this case that the two
words have the same propositional meaning.
\end{quote}

\begin{quote}
\textbf{Definition in the lecture} Synonyms are words which can be
substituted with each other in some context. Hence, two words are more
similar when the overlap of their contexts is large.
\end{quote}

Often the meaning is not exactly the same. You can use
\href{https://www.thesaurus.com/browse/scream}{this website} to find
synonyms. Words can also be similar in other ways, see
\href{../Semantic-Similarity/Word\%20similarity.md}{word similarity}.

\hypertarget{principle-of-contrast}{%
\subsection{Principle of contrast}\label{principle-of-contrast}}

If you take \emph{water} and \(H_2O\) they mean exactly the same thing.
However, if you are on a hike you would say water and in scientific
contexts you say \(H_2O\). This difference is part of the meaning of the
word.

\hypertarget{parsing}{%
\section{Parsing}\label{parsing}}

Parsing is proving that a sequence is in the
\href{Languages.md}{language} set.

Parsing can be described by a problem statement:

Given a grammar G and a string s. Does s belong to L(G). Or
\(s \in L(G)\)\$

L is a function that returns the \href{Languages.md}{languages} set from
a \href{Grammar.md}{grammar}. Many problems can be reduced to parsing
problems.

\begin{quote}
Having a parser for a language is extremely useful. For instance, if you
load a .json file into Python and turn it into a dictionary, this is
actually parsing happening. The JSON parser verifies (create a proof
with a parse tree) that the content of the file you opened belonged to
the JSON language. If this is the case Python can use the resulting
parse tree to construct the dict.

Standard protocols like https, json or toml or pdf are often just
standardized languages which means everyone can use the same parsers.

Compiling programming languages are also just parsing problems with a
code generation step at the end using the parse tree. This is why
programming language input has to be 100\% correct. If the computer
can't parse your code, no parse tree, and then nothing can happen.
\end{quote}

If the parser thinks this is the case, we also want a proof or evidence
that this is the case. This usually takes the form of a
\href{Parse\%20Tree.md}{parse tree}. So this is a tree to go from the
grammar to s! But it can also make a
\href{Dependency\%20Parsing.md}{Dependency graph}.

\begin{figure}
\centering
\includegraphics{Pasted_image_20220314184733.png}
\caption{Parse Tree vs Brackets}
\end{figure}

You can also represent parse trees with brackets. But this is only done
because of language limitations.

Of course, a \emph{no parse} answer is also always possible. It is also
possible to prove that a sequence does not belong to a language set, but
this is harder. One way to do it is with pumping lemmas.

An example of a succesvol parsing is like defining
\href{Regular\%20expression.md}{regular expression} that finds
something. If they find something, then you have a valid parse and your
sequence belongs to the language. If they don't then you don't have a
valid parse and your sequence does not belong to the language set under
the given grammar.

\hypertarget{approaches-to-parsing}{%
\section{Approaches to parsing}\label{approaches-to-parsing}}

The \href{CKY.md}{CKY} algorithm is a way to parse
\href{Context\%20free\%20grammars.md}{Context free grammars}. More
approaches are below:

\hypertarget{parser-generator}{%
\subsection{Parser generator}\label{parser-generator}}

You give this a grammar, and it generates a parser for your grammar.

\begin{itemize}
\tightlist
\item
  External program
\item
  Based on a bottom up algorithm, usually LL or LR
\item
  Can be complex theory
\item
  Limited look ahead, usually one token
\item
  Only build in abstractions (from the program you use)
\item
  Generated parsers are extremely fast!
\item
  An example is \href{Regular\%20expression.md}{Regular expressions} you
  define the grammar and the parser program
  (\href{finite\%20state\%20automata.md}{finite state machine}) is
  generated.
\end{itemize}

\hypertarget{parser-combinators}{%
\subsection{Parser Combinators}\label{parser-combinators}}

\begin{itemize}
\tightlist
\item
  Library
\item
  Based on top down algorithm
\item
  Underlying theory is simple
\item
  In principle unlimited look-ahead
\item
  User definable abstractions
\item
  Fast as long as certain constructions are not used (like a lot of look
  ahead)
\end{itemize}

A combinator is a self-contained function in lambda calculus. The formal
system that is the basis of Haskell and other functional programming
languages. So parser combinators are a set of small (library) functions
that can be used to construct parsers for parts of the language. The
idea is that in context free languages, the context can't change the
parse of the string. This allows you to write parser combinators for
small problems, which can then be combined to parse bigger problems,
basically writing the \href{Constituency.md}{Constituents} as parser
functions and building until you have covered the entire grammar. This
results in one function which can parse the entire grammar, starting at
the most abstract.

Both approaches place certain but different restrictions on the grammar
that you can use.

\hypertarget{handrolling-your-own-parser}{%
\subsection{Handrolling your own
parser}\label{handrolling-your-own-parser}}

\begin{itemize}
\tightlist
\item
  You can also do whatever you want with while and for loops, but this
  can get quite hard and messy quick.
\end{itemize}

\hypertarget{complexity}{%
\subsection{Complexity}\label{complexity}}

\begin{figure}
\centering
\includegraphics{Pasted_image_20220314190559.png}
\caption{Complexity of a parse tree}
\end{figure}

If a parse tree is deep or long, then the sentence tends to be more
complex.

\hypertarget{chomsky-normal-form}{%
\section{Chomsky Normal Form}\label{chomsky-normal-form}}

A \href{Grammar.md}{Grammar} is in \textbf{Chomsky Normal Form} if each
production rule has one of the following forms:

\begin{itemize}
\tightlist
\item
  S ‚Üí BC
\item
  B ‚Üí x
\end{itemize}

Here A B and C are non-terminals.

\begin{itemize}
\tightlist
\item
  x is a terminal.
\item
  S is the start symbol of the grammar.
\end{itemize}

Any non-terminal which is not the start state can not be empty. So
\textbf{B and C cannot be S}. If it's not the start state, it's not
empty.

So what is important is that each non-terminal can only be rewritten to
a rule with ends in only one terminal. So you can have Ab, but not Abb.

These are the bullet points from the slide. A grammar is in CNF if: -
There are no rules generating the empty string - Rules have either
terminals or sequence of two non-terminals as the right-hand side.

You can rewrite any \href{Context\%20free\%20grammars.md}{Context free
grammars} to Chomsky normal form. This means you can also rewrite any
\href{Regular\%20Languages.md}{regular grammar} to Chomsky normal form.

\hypertarget{types-of-grammars}{%
\subsection{Types of grammars}\label{types-of-grammars}}

The Chomsky hierarchy species types of languages.

\begin{itemize}
\tightlist
\item
  Type 0: unrestricted grammars, recursively enumerable languages.

  \begin{itemize}
  \tightlist
  \item
    Require a Turing machine for acceptance (successful parsing)
  \item
    As expressive as any other computational formalism
  \end{itemize}
\item
  Type 1: Context sensitive grammars and languages
\item
  Type 2: \href{Context\%20free\%20grammars.md}{context free grammars}
  and languages

  \begin{itemize}
  \tightlist
  \item
    Parsed using a push down automata in polynomial time
  \end{itemize}
\item
  Type 3: \href{Regular\%20Languages.md}{Regular grammars and languages}

  \begin{itemize}
  \tightlist
  \item
    Recognized by a finite state automata
  \item
    Require only linear time and constant space
  \item
    Equivalent to \href{Regular\%20expression.md}{regular expressions}
  \end{itemize}
\end{itemize}

\hypertarget{constituency}{%
\subsection{Constituency}\label{constituency}}

You can use \href{Grammar.md}{grammar} rules to make constituents out of
terminals. For instance if you have the terminals
\(\{0,1,2,3,4,5,6,7,8,9\}\) then you could define the constituent N. For
instance defined as:

N = 0 \textbar{} 1 \textbar{} 2 \textbar{} 3 \textbar{} 4 \textbar{} 5
\textbar{} 6 \textbar{} 7 \textbar{} 8 \textbar{} 9

The cool thing is that you can now include these constituencies to
create more abstract things. For instance, what if we also have:

L = a \textbar{} b \textbar{} c \textbar{} d \textbar{} e \textbar{} f
\textbar{} g \textbar{} h \textbar{} i \textbar{} j \textbar{} k
\textbar{} l \textbar{} m \textbar{} n \textbar{} o \textbar{} p
\textbar{} q \textbar{} r \textbar{} s \textbar{} t \textbar{} u
\textbar{} v \textbar{} w \textbar{} x \textbar{} y \textbar{} z

Then now you could define the constituent POSTCODE as:

POSTCODE = NNNNLL

This way you can keep abstracting parts of language.

\hypertarget{coordination}{%
\subsection{Coordination}\label{coordination}}

Constituencies can be put behind each other without problems. You could
have a grammar rule PP = POSTCODEPOSTCODE, and it would be fine. This is
often used as a test of constituency. If two phrases can be coordinated
without violating any rules, then they are constituents.

\hypertarget{evaluation-2}{%
\subsubsection{Evaluation}\label{evaluation-2}}

Recall (correct in hypothesis over correct in gold standard), precision
(correct in hypothesis over hypothesized) and f1 is used.

A constituent is labelled as correct if there is a constituent in the
gold standard with the exact same starting point, end point and
non-terminal symbol.

\hypertarget{n-gram}{%
\section{N-Gram}\label{n-gram}}

N-Grams is a contiguous sequence of~\emph{n}~items from a given~sample
of text or speech.

When doing \href{../Data/Normalization.md}{Normalization} of a sequence,
your first intuition when working with a language that has spaces might
be to split the words along the spaces, but often having shorter splits
of maybe 2-3 symbols gives better results. These remaining symbols are
N-grams. So when splitting this sentence: ``Attack at dawn'' up into
3-Grams you get: {[}``Att'', ``ack'', '' at'', ``Daw'', ``n''{]}

\hypertarget{word-n-gram}{%
\subsection{Word N-Gram}\label{word-n-gram}}

A word N-Gram is a sequence of n words: - Uni-Gram: Dog - Uni-Gram: You
are - Tri-Gram: The building blocks - Tetra-Gram: City of shivering
darkness \ldots{}

\hypertarget{encoding-position}{%
\subsection{Encoding position}\label{encoding-position}}

\href{../Classification/Native\%20baiyes/Na√Øve\%20Bayes\%20Classifier.md}{Na√Øve
Bayes Classifier} is bad for making
\href{../Prediction/Language\%20Modeling.md}{language models} because
the bag of words assumption but the idea where it splits off the words
to normalize the probability can also be used for making language
models.

\hypertarget{predicting-n-grams}{%
\subsection{Predicting N-grams}\label{predicting-n-grams}}

A great use case for N-grams is using them for language models.

\hypertarget{predicting-next-word}{%
\subsubsection{Predicting next word}\label{predicting-next-word}}

For instance, to predict the next word: How likely is it that given
words W1 to Wn we observe Wn+1? So given a Tree, how likely is it we
observe a number.

\hypertarget{predicting-the-likelihood-of-whole-sentences}{%
\subsubsection{Predicting the likelihood of whole
sentences}\label{predicting-the-likelihood-of-whole-sentences}}

Out of all the sequences of n words, how likely is sequence A? Out of
all sequences of 6 words, how likely is ``The earth revolves around the
sun''.

\hypertarget{getting-probabilities}{%
\subsection{Getting probabilities}\label{getting-probabilities}}

We can calculate chance of number given A tree has with :
\[p(\text{leaves}|\text{A tree has }) = \text{c}\frac{\text{leaves}}{\text{c(a tree has)}}\]

Here c is a count function which counts how many times ``a tree has''
appears (regardless of continuation) in the big
\href{../Data/Corpus.md}{corpus} and how many times it is followed by
``leaves'' in the corpus. Then you divide them to get a probability.
This is always between 0\ldots1 because the continuation is at most all
the times, and then you get x/x = 1.

\hypertarget{probabilities-for-sequences}{%
\subsubsection{Probabilities for
sequences}\label{probabilities-for-sequences}}

To get the probability for a sequence we do the same, but we divide how
often the sequence you are interested in appears by the number of all
the sequences of the same length. So for 4 word sequences that is:
\[p(\text{a tree has leaves}) = \text{c}\frac{\text{c(a tree has leaves)}}{\text{c(sequences of 4 words)}}\]

Because there are probably a lot more sequences of the length than the
sequence that you are interested in, the probabilities are going to be
very small (sparse). So small even that it could be considered
unreliable. This is why it is better to calculate the probability of a
sentence based on the probability of each word \(*\) the probability of
the words before it appearing before it. We can just calculate the
probabilities like this because of the \textbf{probabilities chain
rule}. So that looks like this: \[p(w_{1},...,w_{m})\]
\[= p(w_{1} \cdot p(w_2|w_{1}) \cdot p(w_3|w_{1},w_{2}) \cdot ... p(w_{m}|w_1,...,w_{m-1})\]
\[=\prod_{{i}=1}^m(p(w_i|w_{1:i-1}))\]

So every time, you multiply the probability of the word by the
probability of the sequence before it. This you repeat for the entire
sequence.

\hypertarget{problems}{%
\subsubsection{Problem's}\label{problems}}

Language is infinite, which makes language models age, but it also means
that you can come up with sequences that are not in the corpus. The
higher the n of an n-gram, the lower the chance that the n-gram appears
at all in the corpus. Or the larger the n-gram, the higher the chance
that we won't find it anywhere in a finite corpus. This is solved by
assuming the \href{../Prediction/Markov\%20assumption.md}{Markov
assumption}.

\hypertarget{log-space}{%
\subsubsection{Log space}\label{log-space}}

Whenever you deal with chained probabilities, it is best to convert all
probabilities to logs so that we can sum instead of multiplying and
avoid having very little numbers. This is a risk because of underflow.
(Computers can't deal well with very small numbers). Another benefit is
that we can sum instead of having to multiply.

\[\log \text{p}(w_{1},...,w_{m} \approx \sum\limits^{m}_{i=1} (\log p(w_{i}|w_{i-n:i-1}))\]

\hypertarget{sequence-boundaries}{%
\subsection{Sequence boundaries}\label{sequence-boundaries}}

It is very important to put sequence boundaries at the start and end of
your sequence. When using N-grams the start boundaries need to be
N-symbols long while the end boundaries are N-1. This way there you can
still have valid N-grams.

\hypertarget{example-8}{%
\subsubsection{Example}\label{example-8}}

\^{}\^{}Hi there\^{} when using bigrams. This way you can go over the
text with n-gram length. Kind of like a sliding window, but you still
know what is at the start because of the \^{}.

\hypertarget{abbreviations}{%
\subsubsection{Abbreviations}\label{abbreviations}}

End of sequence is activated so EoS. Beginning of sequence is activated
so BoS.

\hypertarget{overfitting-1}{%
\subsection{Overfitting}\label{overfitting-1}}

The longer the N-gram choice the better you can fit the specific
training data you have, which means the more chance of
\href{../Prediction/Overfitting.md}{Overfitting}. The more overfitting,
the less general your model becomes.

\hypertarget{finite-state-automate}{%
\section{Finite State Automate}\label{finite-state-automate}}

Finite state atomata is to a network with finate states.

If every state has a transition for each possible input than it is
\textbf{total}.

They are deterministic when each state has only one path for each state.

\begin{figure}
\centering
\includegraphics{Pasted_image_20211212163706.png}
\caption{Finite state automate}
\end{figure}

We can express this in a tuple of 5 things.

(X, Q, d, S, F)

\begin{itemize}
\tightlist
\item
  X is an input \href{Alphabet.md}{alphabeth}
\item
  Q is a set of states
\item
  d is a transition function of type Q -\textgreater{} X -\textgreater{}
  Q
\item
  a start state S \(\in\) Q
\item
  A set of final states \(F \subset Q\)
\end{itemize}

Now that we have this general we can just make one function to run the
automata!

You can also have multiple start points or multiple connections for the
same input these things cause nondeterminism. So you have deterministic
and non-deterministic state machine. You can resolve this non
determinism by going down all the path. You could then follow the
shortest path or something. You can always transform the NFA into one
that does have a single start state.

This way you can parse. If there is at least one path then you can parse
the world.

Double circles are other finite states.

We call non deterministic finite automata NFA. We call deterministic
finite automata DFA.

These are actually equivalent. We can express every DFA as a NFA and
otherwise. The idea is to make them as NFA and then compile them to DFA,
so you can run them without worry.

\includegraphics{Pasted_image_20211212165403.png}
\includegraphics{Pasted_image_20211212165450.png}

\hypertarget{example-9}{%
\subsection{Example:}\label{example-9}}

\begin{figure}
\centering
\includegraphics{Pasted_image_20211212165625.png}
\caption{NFA to DFA conversion}
\end{figure}

The empty set is like to stop state.

You can actually simplify it to:

\begin{figure}
\centering
\includegraphics{Pasted_image_20211212165811.png}
\caption{Simplified DFA}
\end{figure}

So this means that you can use the algorithm to turn a NFA into a DFA,
but this won't give you the only one or the smallest one. A better way
is to do a simulation this gives you the best.

Another way to do this is that you add a transition state that always
goes. So this way you create new states because you can have these
lambda transitions. This is great because then you can have one start
state for your NFA.

NFA are closed under: - union - concatenation - intersection - star
closure - complement

Being closed under an operation means that if we take languages defined
by NFA we can build another NFA which recognizes the language defined by
applying the operation.

\hypertarget{lookahead}{%
\section{Lookahead}\label{lookahead}}

Sometimes you have finite state machines that only go forward. In this
case you need to have lookahead to decide what you have to choose. This
slows everything down. But you can paralyze this. But really this is
more of a problem with pushdown automata. You can always define a finite
state machine that does not need lookahead. Calculating lookahead is not
really that simple.

\hypertarget{what-is-an-alphabet}{%
\section{What is an alphabet?}\label{what-is-an-alphabet}}

An \textbf{alphabet} is a (finite) set of symbols that can be used to
form \href{../Data/Sentences.md}{sentences} or
\href{../Data/Words.md}{words}.

An alphabet is the set of symbols that can be used to construct valid
sequences in a \href{../Languages.md}{language}.

Some examples: - \({a, b, c}\) -
\({a, b, c, d, e, f, g, h,i, j, k, l,n, o,p, q,r,s, t,u, v,w, y,z}\) -
\({a, d, e, f,g, h, i, k,l, n, o, p,q, r, s, t, u, v,y, z}\) - \({0,1}\)
- The set of Latin characters. - Unicode set - \{if, else, while, for,
return, try\} ‚Üê keywords. Here if, then, else are their own symbols. -
\{if, else, raise, for, while, return, try\} \(\cup\) ASCII set

\hypertarget{examples-4}{%
\subsection{Examples}\label{examples-4}}

Given that the alphabet is almost always finite, we can consider
sequences of elements from the alphabet.

For instance, if the alphabets is \{a, b, c\} you can have sequences
like this: - a - b - c - ab - cab - \(\epsilon\) ‚Üê Empty sequence.
Always valid! Don't put the empty sequence in your alphabet.

\hypertarget{regular-expression}{%
\section{Regular expression}\label{regular-expression}}

Regular expressions are a domain specific
\href{Languages/Languages.md}{language} for the concise description and
\href{Parsing.md}{parsing} of \href{Regular\%20Languages.md}{regular
Languages}.

Regular languages officially start and end with /. So you have /regular
expression/. Often you leave these / out.

So let's have a regular expression for god. The regular expression is
/god/.

The grammar for this language looks like this:

\begin{itemize}
\tightlist
\item
  S ‚Üí gA
\item
  A ‚Üí oB
\item
  B ‚Üí d
\end{itemize}

Regular languages can be expressions in regular expressions which can be
expressed in \href{finite\%20state\%20automata.md}{finite state
automata}. This is great because this makes regular expressions really
fast to parse.

\hypertarget{symbols-1}{%
\subsection{Symbols}\label{symbols-1}}

You can parse symbols at a position by just including them in the
regular expression /hello/ \href{Parsing.md}{parses} the word hello. But
what if we want to parse multiple words. Then we have to define a
regular expression where one position of the sentence can have multiple
symbols. This is done with symbol ranges.

\hypertarget{symbol-ranges}{%
\subsubsection{Symbol Ranges}\label{symbol-ranges}}

Now you can have groups at any position. So if we want to pare: {[}god,
bod{]} then we can have a regular expression like this: /{[}gb{]}od/.
This is like having a regular grammar rule like this.

\begin{itemize}
\tightlist
\item
  S ‚Üí gA
\item
  S ‚Üí bA
\item
  A ‚Üí oB
\item
  B ‚Üí d
\end{itemize}

There is special syntax for ranges /{[}a-z{]}od/ here we can parse any
lowercase letter of the \href{Alphabet.md}{alphabet} followed by od.
These ranges are defined by the
\href{https://en.wikipedia.org/wiki/ASCII}{ASCII} number of the symbol
which happens to be convenient. Some common ranges: - {[}A-z{]} is all
letters lowercase uppercase - {[}0-9{]} is all numbers 0 to 9 -
{[}4-7{]} is all numbers 7 to 7.

\hypertarget{the-wildcard}{%
\subsubsection{The wildcard}\label{the-wildcard}}

If you don't care what the symbol of a position in the word is then you
can use the . wildcard. So /h.i/ can match hai hoi hbi etc. The only
thing the . does not match with is a newline character. This is like an
enter or maybe \textbackslash n.~

\hypertarget{negation}{%
\subsubsection{Negation}\label{negation}}

If you use \^{} at the start of a character range than everything that
is not this range matched. So you say {[}\^{}0-4{]} then everything that
is not in the set \(\{0,1,2,3,4\}\) is parsed.

\hypertarget{counters-repeating}{%
\subsection{Counters (repeating)}\label{counters-repeating}}

Often you want to repeat certain parts in a parse. This can be done with
counters.

\hypertarget{one-or-more}{%
\subsubsection{One or more}\label{one-or-more}}

If something has to occur once or more than we can use the +.

So /so+/ will parse so, soooo, sooo but not s because o has to appear at
least once.

\hypertarget{zero-or-more}{%
\subsubsection{Zero or more}\label{zero-or-more}}

Sometimes you want to check that the same part repeats sometimes. You
can just define that like this: /aaaao/ but what if you don't know how
often the a will appear? Then there are special symbols for this. /a* o/
will parse a string that starts with zero or more a's followed by a
space followed by an o.

This is for instance how we can parse so and sooooo as the same word.
/so\emph{/ However this also parses s because } means zero or more.

\hypertarget{zero-or-once}{%
\subsubsection{Zero or once}\label{zero-or-once}}

Sometimes you have something optional that can occur or not. For
instance in someone's last name. This is also sometimes called the
optional. You can specify this with a ? So if you have /hi?/ then it
would parse both h and hi.

\hypertarget{custom-repeat}{%
\subsubsection{Custom repeat}\label{custom-repeat}}

You can also define custom ranges. If we only want to parse a so with 4
to 10 o then we can say /so\{4,10\}/. The ? is a shortcut for \{0,1\}.
If you have \{n,m\} n is the minimum number it has to appear and m is
the maximum it can appear.

\hypertarget{combining-with-symbol-ranges}{%
\subsubsection{Combining with symbol
ranges}\label{combining-with-symbol-ranges}}

You can use +, * , ? and \{\} with character ranges as well. So you can
say /age:{[}0-9{]}\{1,3\}/ to parse any string like: ``age:32'' or
``age:120''.

Or let's say /d{[}a-z{]}+d/. This parses any string starting and ending
with a d with one or more lowercase letter in between them.

\hypertarget{markers-end-and-start-of-the-line}{%
\subsection{Markers (End and start of the
line)}\label{markers-end-and-start-of-the-line}}

\^{} can also mean start of the string where \$ means end of the string
line. This is based on newline characters. This means that something it
only parsed if it is at the start or end of a line. So if you have
/\^{}hi/ then it will only parse something like:

\textbf{hi} there but it won't parse this hi

\hypertarget{escaping}{%
\subsection{Escaping}\label{escaping}}

So far we defined special characters like . ? * + \{\} {[}{]} and there
are more like \^{}\{ and \$ which mean end of string start of string or
\^{} can also mean negation. However maybe you want to literally check
for these characters. When you want this you have to put ~before the
special character to indicate this. So you could use /.+./ to check if
sentences end with a string. The . is escaped here. Or what if you want
to check for something like {[}1,2,3,4{]} then you have to do
/\textbackslash{[}{[}0-9, {]}*~{]}/. Basically you want to literally
check for the {[} and {]} characters. Note that this also matches
{[},9,9,9{]}~we need groups to fix this.

\hypertarget{special-escapes}{%
\subsubsection{Special escapes}\label{special-escapes}}

If you escape things that are letters so only a literal meaning you get
opposite. So \textbackslash n is a new line (very common thing). But
\textbackslash t is a tab or \textbackslash b is a word boundary for
instance. A word boundary is anything that is not a letter, digit or
underscore. Often the opposite of this selection is the capital version
of this so \textbackslash B only match word boundary. These can be very
useful. You can also use these special escapes in ranges like:
{[}\textbackslash b{]} is the same as {[}A-z0-9\_{]} is the same as
\textbackslash b.

\hypertarget{grouping}{%
\subsection{Grouping}\label{grouping}}

Often you want to repeat longer productions. In this case you can use
groups. A group is (). So you can say (ha)+ which means the string ha
repeated. So that can match ha, haha, hahaha, but not hah. These groups
can contain anything we defined before, so you can have something like:

\texttt{/\textbackslash{}{[}({[}0-9{]},)*\textbackslash{}{]}/} which
matches {[}{]} and also {[}1,2,4,{]}

The point of grouping is that it involves multiple characters.

\hypertarget{capture-groups}{%
\subsubsection{Capture groups}\label{capture-groups}}

Any time you define a group you can refer back to it by using the index
with a \textbackslash{} in front. The index is just the number that you
defined the group as.

So if you have /a(b)\textbackslash1/ then this matches abb. You repeat
the group with the b again. You could also have
/(a)(b)\textbackslash2\textbackslash1/ this would match abba.

Now this is the thing what the \textbackslash1 has to match what you
found in the group! So if you define
/my\_age:({[}0-9{]}\{1-3\})-I\_am\_\textbackslash1\_years\_old\textbackslash./
Then this can match: my\_age:12,I\_am\_12\_years\_old. But it couldn't
match my\_age:12,I\_am\_33\_years\_old. Because the capture group and
the referral back to it are not the same! So the result is really stored
in memory. This is also great if you want to get the results of the
group later.

\hypertarget{actions}{%
\subsection{Actions}\label{actions}}

\hypertarget{substitution-replace}{%
\subsubsection{Substitution (replace)}\label{substitution-replace}}

If you start your regular expresion with an s you define a replacement.
Something like this for instance replaces one or more white spaces with
a tab.

\texttt{s/\ +/\textbackslash{}t/} The s indicates replace the first
block by the second block. The block is defined by what's between the /
/ .

You can add a g at the end to do this everywhere. This means global. So
then you get \texttt{s/\ +/\textbackslash{}t/g}.

Or lets say you have: s/{[}0-9{]}/\textbar number\textbar/g this would
replace:

Counting goes like 1, 2, 3, 4, 5! with Counting goes like
\textbar number\textbar,\textbar number\textbar,\textbar number\textbar,\textbar number\textbar,\textbar number\textbar!

If you don't have the g it would go to:

Counting goes like \textbar number\textbar, 2, 3, 4, 5!

You can also replace by capture group!

\hypertarget{online-resources}{%
\section{Online resources}\label{online-resources}}

You can use websites to test your regular expressions. For instance this
one is usefull:

https://regex101.com

Play around with it. This website can also export Python code so that is
pretty cool.

This is a website I made to obfuscate minecraft behavior packs. It also
uses a lot of regular expressions to figure out what to replace.
https://minecraft-obfuscator.dev

\hypertarget{example-of-email-regex}{%
\subsection{Example of email regex}\label{example-of-email-regex}}

\begin{quote}
\texttt{(?:{[}a-z0-9!\#\$\%\&\textquotesingle{}*+/=?\^{}\_\textasciigrave{}\{\textbar{}\}\textasciitilde{}-{]}+(?:\textbackslash{}.{[}a-z0-9!\#\$\%\&\textquotesingle{}*+/=?\^{}\_\textasciigrave{}\{\textbar{}\}\textasciitilde{}-{]}+)*\textbar{}"(?:{[}\textbackslash{}x01-\textbackslash{}x08\textbackslash{}x0b\textbackslash{}x0c\textbackslash{}x0e-\textbackslash{}x1f\textbackslash{}x21\textbackslash{}x23-\textbackslash{}x5b\textbackslash{}x5d-\textbackslash{}x7f{]}\textbar{}\textbackslash{}\textbackslash{}{[}\textbackslash{}x01-\textbackslash{}x09\textbackslash{}x0b\textbackslash{}x0c\textbackslash{}x0e-\textbackslash{}x7f{]})*")@(?:(?:{[}a-z0-9{]}(?:{[}a-z0-9-{]}*{[}a-z0-9{]})?\textbackslash{}.)+{[}a-z0-9{]}(?:{[}a-z0-9-{]}*{[}a-z0-9{]})?\textbar{}\textbackslash{}{[}(?:(?:25{[}0-5{]}\textbar{}2{[}0-4{]}{[}0-9{]}\textbar{}{[}01{]}?{[}0-9{]}{[}0-9{]}?)\textbackslash{}.)\{3\}(?:25{[}0-5{]}\textbar{}2{[}0-4{]}{[}0-9{]}\textbar{}{[}01{]}?{[}0-9{]}{[}0-9{]}?\textbar{}{[}a-z0-9-{]}*{[}a-z0-9{]}:(?:{[}\textbackslash{}x01-\textbackslash{}x08\textbackslash{}x0b\textbackslash{}x0c\textbackslash{}x0e-\textbackslash{}x1f\textbackslash{}x21-\textbackslash{}x5a\textbackslash{}x53-\textbackslash{}x7f{]}\textbar{}\textbackslash{}\textbackslash{}{[}\textbackslash{}x01-\textbackslash{}x09\textbackslash{}x0b\textbackslash{}x0c\textbackslash{}x0e-\textbackslash{}x7f{]})+)\textbackslash{}{]})}
\end{quote}

This one works for 99.99 \% of all emails.

\hypertarget{edit-distance}{%
\section{Edit distance}\label{edit-distance}}

When you see a mispeled word, for instance snowbakl. Did the writer mean
snowball or snowplow? Both are valid, but if you as a human look at this
your intuition will say snowball. Why is this? This can be quantified
with the edit distance.

If you want to go from snowbakl to snowball you need one step, but if
you want to go from snowbakl to snowplow you need 4 steps.

\begin{quote}
Another reason why snowbakl is more likely is that the k is next to the
l on a US keyboard, which makes it a likely mistake.
\end{quote}

\hypertarget{transformations}{%
\subsection{Transformations}\label{transformations}}

The steps are called transformations. You get the edit distance by
counting the minimum number of needed transformations from one word to
another. How are transformations defined?

\begin{itemize}
\tightlist
\item
  Insert ‚Üí Insert a new character.
\item
  Delete ‚Üí Delete a character.
\item
  Substitute ‚Üí Switch a character for another.
\end{itemize}

You could also ban substitutions (and replace them with delete-insert)
which basically means substitutions count for 2 transformation.

\begin{figure}
\centering
\includegraphics{Pasted_image_20220217165311.png}
\caption{Edit distance alignment example}
\end{figure}

\begin{figure}
\centering
\includegraphics{Pasted_image_20220217165328.png}
\caption{Edit distance example}
\end{figure}

\hypertarget{how-to-find-the-shortest-path}{%
\subsection{How to find the shortest
path}\label{how-to-find-the-shortest-path}}

This is expressed in dynamic programming. You identify sub problems and
then solve those, and then combine the solution you found to solve the
bigger problem.

The search space is very large, but caching helps, and also you can keep
track of the minimum you have found so far and then discard branches
that go further than that.

The in the lecture he gives an algorithm for it:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ D(source: }\BuiltInTok{str}\NormalTok{, target: }\BuiltInTok{str}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{int}\NormalTok{:}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(source)):}
        \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(target)):}
            \ControlFlowTok{if}\NormalTok{ source[i] }\OperatorTok{!=}\NormalTok{ target[j]:}
\NormalTok{                D(i, j) }\OperatorTok{=} \DecValTok{1} \OperatorTok{+}\NormalTok{ arrgmin( D(i}\OperatorTok{{-}}\DecValTok{1}\NormalTok{), D(i}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, j}\OperatorTok{{-}}\DecValTok{1}\NormalTok{), D(i, j}\OperatorTok{{-}}\DecValTok{1}\NormalTok{))}
            \ControlFlowTok{else}\NormalTok{:}
\NormalTok{                D(i,j) }\OperatorTok{=}\NormalTok{ D(i}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, j}\OperatorTok{{-}}\DecValTok{1}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ D(n,m)}
\end{Highlighting}
\end{Shaded}

So if the item in the source is the target then move on if it's not,
find out the shortest distance.

\hypertarget{alignment}{%
\paragraph{Alignment}\label{alignment}}

To get the best alignment, we have to store \textbf{back pointers} in
each cell, so we know where we came from when we reached a
transformation.

Then we \textbf{trace back} our steps and favour the substitution every
time we can.

\hypertarget{graphically}{%
\subsection{Graphically}\label{graphically}}

\begin{figure}
\centering
\includegraphics{Pasted_image_20220217170600.png}
\caption{Edit distance shortest path}
\end{figure}

The \# is the empty string. Moving over the diagonal is a substitution,
so you could count this as 2. You can go fast by first only considering
the outer layers. This is the simplest, where you just go from the empty
string to the other word. Then use this information when making
decisions. Then, when going further, the number only goes up if the
letter is not the same. If the number is the same, the number goes down.

You have to be able to solve problems like this on the exam:

\begin{figure}
\centering
\includegraphics{Pasted_image_20220217171236.png}
\caption{Edit distance challenge}
\end{figure}

\hypertarget{minimum-edit-distance-full-tutorial}{%
\subsection{Minimum Edit distance full
tutorial:}\label{minimum-edit-distance-full-tutorial}}

f3 help: edit distence, editdistance, edit distanse, edit distense,
minimum edit distense

We will now calculate the edit distance from Dirt to Flirt.

\hypertarget{steps}{%
\subsubsection{Steps}\label{steps}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Get cost for Insert, Delete, Substitute (given in the question)
\item
  Make a table with one word on top and one on the left one letter in
  each cell. Starting with the empty string (\#)
\end{enumerate}

\begin{longtable}[]{@{}llllll@{}}
\toprule
& \# & D & I & R & T \\
\midrule
\endhead
\# & & & & & \\
F & & & & & \\
L & & & & & \\
I & & & & & \\
R & & & & & \\
T & & & & & \\
\bottomrule
\end{longtable}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Now we have to fill everywhere how many actions it takes. Start with
  the first row and column. The first is from the empty string, so it
  starts at 0 (actions to go from \# to \# is 0) going up by 1. So from
  \# to D is 1, from \# to DI is 2 etc.
\end{enumerate}

With Flirt, you are deleting letters. With Dirt, you are inserting
letters. So you have to delete 3 letters from FLI to get to \#.

\begin{longtable}[]{@{}llllll@{}}
\toprule
x & \# & D & I & R & T \\
\midrule
\endhead
\# & 0 & 1 & 2 & 3 & 4 \\
F & 1 & & & & \\
L & 2 & & & & \\
I & 3 & & & & \\
R & 4 & & & & \\
T & 5 & & & & \\
\bottomrule
\end{longtable}

Ok now we do the column with D at the top. Look at the empty cell and
ask these questions:

\begin{itemize}
\tightlist
\item
  Are the two letters different? (Yes D != F).

  \begin{itemize}
  \tightlist
  \item
    If they are different, look at the cell to the left, above left
    diagonal and above. So that is: {[}1,0,1{]} Take the minimum value =
    0, add 1.
  \item
    If the minimum came from above left diagonal it is a substitution,
    and you add another 1 (This is a choice, cost of substitution, be
    sure to read the question if it is required).
  \item
    So 0 + 1 + 1 = 2
  \end{itemize}
\end{itemize}

\begin{longtable}[]{@{}llllll@{}}
\toprule
x & \# & D & I & R & T \\
\midrule
\endhead
\# & 0 & 1 & 2 & 3 & 4 \\
F & 1 & 2 & & & \\
L & 2 & & & & \\
I & 3 & & & & \\
R & 4 & & & & \\
T & 5 & & & & \\
\bottomrule
\end{longtable}

Now repeat for next cell down (L,D): - Are they the same? No - If no
take minimum of above cell, left cell, up, left diagonal cell = 1 - Add
1, so 1 + 1 and add another one because the min was the diagonal so 1 +
1 + 1 = 3

\begin{longtable}[]{@{}llllll@{}}
\toprule
x & \# & D & I & R & T \\
\midrule
\endhead
\# & 0 & 1 & 2 & 3 & 4 \\
F & 1 & 2 & & & \\
L & 2 & 3 & & & \\
I & 3 & & & & \\
R & 4 & & & & \\
T & 5 & & & & \\
\bottomrule
\end{longtable}

Now one more:

Cell: I,D, same? no - min(3,2,3) = 2 - was min diagonal? - yes?, add 2 -
no?, add 1 - 2 + 2 = 4

\begin{longtable}[]{@{}llllll@{}}
\toprule
x & \# & D & I & R & T \\
\midrule
\endhead
\# & 0 & 1 & 2 & 3 & 4 \\
F & 1 & 2 & & & \\
L & 2 & 3 & & & \\
I & 3 & 4 & & & \\
R & 4 & & & & \\
T & 5 & & & & \\
\bottomrule
\end{longtable}

I am going to continue now till we hit a case where the letter is the
same:

\begin{longtable}[]{@{}llllll@{}}
\toprule
x & \# & d & i & r & t \\
\midrule
\endhead
\# & 0 & 1 & 2 & 3 & 4 \\
f & 1 & 2 & 3 & & \\
l & 2 & 3 & 4 & & \\
i & 3 & 4 & & & \\
r & 4 & 5 & & & \\
t & 5 & 6 & & & \\
\bottomrule
\end{longtable}

Now we have I,I these are the same. In this case, just copy the value
from above left diagonal cell!

\begin{longtable}[]{@{}llllll@{}}
\toprule
x & \# & d & i & r & t \\
\midrule
\endhead
\# & 0 & 1 & 2 & 3 & 4 \\
f & 1 & 2 & 3 & & \\
l & 2 & 3 & 4 & & \\
i & 3 & 4 & 3 & & \\
r & 4 & 5 & & & \\
t & 5 & 6 & & & \\
\bottomrule
\end{longtable}

The \textbf{minimum edit distance} is the value of the bottom right
cell.

To get the best alignment, you need to store where you came from every
time. You can also do it by looking at the best path, working back.
Basically you can only go to cells up, left, left up diagonal from the
bottom right cell if they have a lower value then the current cell. So
with this cell I, I you can only go to l,d (to get the best alignment).
When working back, you always prioritize diagonal to go back.

Ok next one:

\begin{longtable}[]{@{}llllll@{}}
\toprule
x & \# & d & i & r & t \\
\midrule
\endhead
\# & 0 & 1 & 2 & 3 & 4 \\
f & 1 & 2 & 3 & & \\
l & 2 & 3 & 4 & & \\
i & 3 & 4 & 3 & & \\
r & 4 & 5 & & & \\
t & 5 & 6 & & & \\
\bottomrule
\end{longtable}

\begin{itemize}
\tightlist
\item
  Cell: i,r
\item
  Same? no
\item
  min(5,4,3) = 3

  \begin{itemize}
  \tightlist
  \item
    was min diagonal?

    \begin{itemize}
    \tightlist
    \item
      yes?, add 2
    \item
      no?, add 1
    \end{itemize}
  \end{itemize}
\item
  3 + 1 = 4
\end{itemize}

\begin{longtable}[]{@{}llllll@{}}
\toprule
x & \# & d & i & r & t \\
\midrule
\endhead
\# & 0 & 1 & 2 & 3 & 4 \\
f & 1 & 2 & 3 & & \\
l & 2 & 3 & 4 & & \\
i & 3 & 4 & 3 & & \\
r & 4 & 5 & 4 & & \\
t & 5 & 6 & & & \\
\bottomrule
\end{longtable}

Ok now I will fill in the whole thing:

\begin{longtable}[]{@{}llllll@{}}
\toprule
x & \# & d & i & r & t \\
\midrule
\endhead
\# & 0 & 1 & 2 & 3 & 4 \\
f & 1 & 2 & 3 & 4 & 5 \\
l & 2 & 3 & 4 & 5 & 6 \\
i & 3 & 4 & 3 & 4 & 5 \\
r & 4 & 5 & 4 & 3 & 4 \\
t & 5 & 6 & 5 & 4 & 5 \\
\bottomrule
\end{longtable}

See the image above for how to work back.

Be sure to check the question if substitution has a cost of 2!

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

With Growing and Glowing you get this:

\begin{longtable}[]{@{}lllllllll@{}}
\toprule
X & \# & G & r & o & w & i & n & g \\
\midrule
\endhead
\# & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\
G & 1 & 0 & 1 & 2 & & & & \\
l & 2 & 1 & 2 & 3 & & & & \\
o & 3 & 2 & 3 & 2 & & & & \\
w & 4 & 3 & 4 & & 2 & & & \\
i & 5 & 4 & 5 & & & 2 & & \\
n & 6 & 5 & 6 & & & & 2 & \\
g & 7 & 6 & 7 & & & & & 2 \\
\bottomrule
\end{longtable}

Subs count for 2.

You know that it will be 2, and you can skip a lot because the rest of
the word matches. Marking matching cells beforehand can help.

\hypertarget{context-free-grammars}{%
\section{Context free grammars}\label{context-free-grammars}}

The context free grammars are grammars where you can form rules of the
form:

\begin{itemize}
\tightlist
\item
  N = (N \(\cup\) \(\sum\))*
\end{itemize}

The left-hand side can only be ONE non-terminal. The right-hand side can
be any ordered combination of terminals and non-terminals, of any
length.

Context free grammars are much more expressive than
\href{Regular\%20Languages.md}{regular grammars}.

You can \href{Parsing.md}{parse} context free grammars with parser
combinators, parser generators or the \href{CKY.md}{CKY} algorithm.
There are a lot more options as well.

Context free grammars are called context free because there is no
\href{../Semantic-Similarity/Context.md}{context} on the right-hand side
of the \href{Grammar.md}{grammar} production, as in there is only one
non-terminal on the right. Knowing this makes parsing much easier.

\hypertarget{regular-languages}{%
\section{Regular languages}\label{regular-languages}}

A regular language is a \href{Languages/Languages.md}{Languages} that
can be described by a grammar where all the productions are in the
following form:

\begin{itemize}
\tightlist
\item
  S ‚Üí xB
\item
  B ‚Üí b
\end{itemize}

This defines a languages of a sequence of x and then one b at the end.

Where x is a possibly empty \textbf{sequence} of terminals and A and B
are non-terminals. Every right-hand side has at most one non-terminal
that must occur at the end.

You can see the overlap between
\href{finite\%20state\%20automata.md}{finite state automata}. You have
one state, and you can go to another.

We call a language regular if it can be described by a regular grammar.
Those are grammars like the above.

Regular languages can be described by
\href{Regular\%20expression.md}{regular expression}.

Often the first grammar production is S.

\hypertarget{chunking}{%
\section{Chunking}\label{chunking}}

Sometimes you don't need the full \href{Parse\%20Tree.md}{parse tree} of
a sentence, but you only need the main building blocks of a sentence.
This is also called shallow or partial \href{Parsing.md}{parsing}.

Chunking stays on the surface level. There is only one layer of
non-terminals, and the non-terminals can not be rewritten to other
non-terminals.

The idea of chunking is that you chop a sentence into non-terminals or
chunks. These chunks don't overlap, and you are not going to chunk the
chunk.

Chunks are not recursive, they never contain smaller phrases of the same
type.

\hypertarget{example-10}{%
\subsection{Example}\label{example-10}}

\textbf{The man} - \emph{guarding} - \textbf{the door} - \emph{fell
asleep} - \textbf{and the thieves} -- \emph{managed to enter} --
\textbf{the building and run away} - \sout{with the diamonds}.

So here we have verbs and nouns and at the end a prepositional phrase.

So it boils down to determining the boundaries of the chunks. For
instance, you can keep going until you hit something that is not in the
same group as before and if it's not, you put a boundary. So you do
\textbf{splitting}, but you also \textbf{classify} what the chunk is.

\hypertarget{evaluation-3}{%
\subsection{Evaluation}\label{evaluation-3}}

A correct chunk starts and ends at the right place and has the right
token. We use precision, recall and f1 scores to evaluate this.

\textbf{Precision} is the ratio between the correct chunk produced by
the system and all the chunks produced by the system. \textbf{Recall}
weights the correct chunks over the correct chunks in the text.
F-measure combines recall and precision.

\hypertarget{parseval}{%
\subsubsection{Parseval}\label{parseval}}

Parseval is a competition to evaluate chunking algorithms. This measures
to what existent the \href{Constituency.md}{constituents} in the
hypothesized parses look like the actual constituents, defined by
linguists (gold standard \href{../Data/Corpus.md}{corpus} with
annotations is required for this)

Performance is assessed at the constituent level because parsers will
probably make some mistakes.

\hypertarget{cky-algorithm}{%
\section{CKY algorithm}\label{cky-algorithm}}

The CKV algorithm is a dynamic programming solution to parsing
\href{Context\%20free\%20grammars.md}{Context free grammars}.

\hypertarget{pseudocode}{%
\subsection{Pseudocode}\label{pseudocode}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ CKYparser(string, grammar):}
    \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \DecValTok{1}\NormalTok{:length(string)  }
        \ControlFlowTok{for} \BuiltInTok{all}\NormalTok{ rules A }\OperatorTok{|}\NormalTok{ A‚Üís[j] }\KeywordTok{in}\NormalTok{ ‚Ñí  }
\NormalTok{            table[j}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,j] }\OperatorTok{=}\NormalTok{ table[j}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,j] union A  }
        \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in}\NormalTok{ reverse(}\DecValTok{0}\NormalTok{:j}\OperatorTok{{-}}\DecValTok{2}\NormalTok{):  }
            \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in}\NormalTok{ i}\OperatorTok{+}\DecValTok{1}\NormalTok{:j}\OperatorTok{{-}}\DecValTok{1}  
                \ControlFlowTok{for} \BuiltInTok{all}\NormalTok{ rules A‚ÜíBC }\KeywordTok{in}\NormalTok{ ‚Ñí }\KeywordTok{and}\NormalTok{ B }\KeywordTok{in}\NormalTok{ table[i,k] }\KeywordTok{and}\NormalTok{ C }\KeywordTok{in}\NormalTok{ table[k,j]  }
\NormalTok{                    table[i,j] }\OperatorTok{=}\NormalTok{ table[i,j] union A  }
    \ControlFlowTok{return}\NormalTok{ table}
\end{Highlighting}
\end{Shaded}

\hypertarget{actual-python-function}{%
\subsection{Actual python function}\label{actual-python-function}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ CKYparser(string: }\BuiltInTok{str}\NormalTok{, grammar: }\StringTok{\textquotesingle{}Grammar\textquotesingle{}}\NormalTok{) }\OperatorTok{{-}\textgreater{}} \BuiltInTok{list}\NormalTok{[}\BuiltInTok{list}\NormalTok{[}\BuiltInTok{str}\NormalTok{]]:}
\NormalTok{    table }\OperatorTok{=}\NormalTok{ [[}\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(string))] }\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(string))]}
\NormalTok{    rules }\OperatorTok{=}\NormalTok{ grammar.rules}
    \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, }\BuiltInTok{len}\NormalTok{(string)):}
        \ControlFlowTok{for}\NormalTok{ rule }\KeywordTok{in}\NormalTok{ rules:}
\NormalTok{            table[j}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,j] }\OperatorTok{=}\NormalTok{ table[j}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,j] }\KeywordTok{or}\NormalTok{ A}
        \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in}\NormalTok{ reverse(}\BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{,j}\OperatorTok{{-}}\DecValTok{2}\NormalTok{)):}
            \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i}\OperatorTok{+}\DecValTok{1}\NormalTok{, j}\OperatorTok{{-}}\DecValTok{1}\NormalTok{):}
                \ControlFlowTok{for}\NormalTok{ rule }\KeywordTok{in}\NormalTok{ rules:}
                    \ControlFlowTok{for}\NormalTok{ B, C }\KeywordTok{in}\NormalTok{ table[i,k], table[k,j]:}
\NormalTok{                        table[i,j] }\OperatorTok{=}\NormalTok{ table[i,j] }\KeywordTok{or}\NormalTok{ A}
    \ControlFlowTok{return}\NormalTok{ table}
\end{Highlighting}
\end{Shaded}

Don't reinvent the wheel! Use existing and optimized libraries to do
statistical language modelling, for instance like KenLM. \#\# Tutorial

\hypertarget{a-grammar}{%
\subsubsection{A grammar}\label{a-grammar}}

This grammar is in \href{Chomsky\%20Normal\%20Form.md}{Chomsky Normal
Form}:

\begin{itemize}
\tightlist
\item
  S ‚Üí AB \textbar{} BC
\item
  A ‚Üí BA \textbar{} a
\item
  B ‚Üí CC \textbar{} b
\item
  C ‚Üí AB \textbar{} a
\end{itemize}

Can we get to S from abba?

Yes!

So we have a table with a row for each input string. So let's take input
= abba

\begin{longtable}[]{@{}lllll@{}}
\toprule
& a & b & b & a \\
\midrule
\endhead
X & & & & \\
X & X & & & \\
X & X & X & & \\
X & X & X & X & \\
X & X & X & X & X \\
\bottomrule
\end{longtable}

What are the rules where a appears on the right side? This is A and C.
So we fill these non-terminals into the table.

\begin{longtable}[]{@{}lllll@{}}
\toprule
& a & b & b & a \\
\midrule
\endhead
x & A,C & & & \\
X & X & & & \\
X & X & X & & \\
X & X & X & X & \\
X & X & X & X & X \\
\bottomrule
\end{longtable}

Now we do the same for b.

\begin{longtable}[]{@{}lllll@{}}
\toprule
& a & b & b & a \\
\midrule
\endhead
x & A,C & & & \\
X & X & B & & \\
X & X & X & & \\
X & X & X & X & \\
X & X & X & X & X \\
\bottomrule
\end{longtable}

Now you want to put in the cell between the cell with A, C and B the
rules that contain combinations of those non-terminals on the right-hand
side. The possible combinations are: AB and CB. The thing to the left
has to come first. Now we basically try to find a constituent of a and
b.

The possibilities are S and C so let's fill it in.

\begin{longtable}[]{@{}lllll@{}}
\toprule
& a & b & b & a \\
\midrule
\endhead
x & A,C & S,C & & \\
x & x & b & & \\
x & x & x & & \\
x & x & x & x & \\
x & x & x & x & x \\
\bottomrule
\end{longtable}

Now the bottom of the next row is B again just like before. This is the
only non-terminal that can come from b.

\begin{longtable}[]{@{}lllll@{}}
\toprule
& a & b & b & a \\
\midrule
\endhead
x & A,C & S,C & & \\
x & x & b & & \\
x & x & x & b & \\
x & x & x & x & \\
x & x & x & x & x \\
\bottomrule
\end{longtable}

But now we have to fill in the things above the b in the row. To do
this, we only look at the cell directly 1 up from the last be we did. So
we only look at the cell marked K now.

\begin{longtable}[]{@{}lllll@{}}
\toprule
& a & b & b & a \\
\midrule
\endhead
x & A, C & S,C & & \\
x & x & B & K & \\
x & x & x & B & \\
x & x & x & x & \\
x & x & x & x & x \\
\bottomrule
\end{longtable}

If we do this, we can repeat what we did before and look at sequences of
non-terminals in the grammar of BB. If we found this non-terminal, you
would fill it in and move one cell up in the row. Do you feel the
abstraction building? There is no BB, so we put empty set
(\(\emptyset\)).

\begin{longtable}[]{@{}lllll@{}}
\toprule
& a & b & b & a \\
\midrule
\endhead
x & A, C & S, C & & \\
x & x & B & \(\emptyset\) & \\
x & x & x & B & \\
x & x & x & x & \\
x & x & x & x & x \\
\bottomrule
\end{longtable}

Now we move up again. However, now we have to combine this cell with a
lot of other cells because if you look from the right then you do
include the whole column. Because basically the lowest cell in the
second b row is saying we can rewrite b to B. Then the one above that
says can we rewrite BB? No.~The one above that should look for SB or CB.
Because you try to rewrite what came before. However, SB or CB do not
exist in the grammar, so it is empty set again.

\begin{longtable}[]{@{}lllll@{}}
\toprule
& a & b & b & a \\
\midrule
\endhead
x & A, C & S, C & \(\emptyset\) & \\
x & x & B & \(\emptyset\) & \\
x & x & x & B & \\
x & x & x & x & \\
x & x & x & x & x \\
\bottomrule
\end{longtable}

Now we can basically fill in the table. The first is simple, how can you
write a. We already did this.

\begin{longtable}[]{@{}lllll@{}}
\toprule
& a & b & b & a \\
\midrule
\endhead
x & A, C & S, C & \(\emptyset\) & \\
x & x & B & \(\emptyset\) & \\
x & x & x & B & \\
x & x & x & x & A, C \\
x & x & x & x & x \\
\bottomrule
\end{longtable}

Then the cell above it looks for BA or BC. BA occurs in A and BC occurs
in S. So we write S and A.

\begin{longtable}[]{@{}lllll@{}}
\toprule
& a & b & b & a \\
\midrule
\endhead
x & A, C & S, C & \(\emptyset\) & \\
x & x & B & \(\emptyset\) & \\
x & x & x & B & S, A \\
x & x & x & x & A,C \\
x & x & x & x & x \\
\bottomrule
\end{longtable}

For the next one, we check BS and BA. BA occurs in A and BS does not
occur. So we put A. So we looked at marked cells.

\begin{longtable}[]{@{}lllll@{}}
\toprule
& a & b & b & a \\
\midrule
\endhead
x & A, C & S, C & \(\emptyset\) & \\
x & x & B & \(\emptyset\) & A \\
x & x & x & B & S, A \\
x & x & x & x & A,C \\
x & x & x & x & x \\
\bottomrule
\end{longtable}

For the last one, we have to check all the cells in the top row with A.

So we check for AA, CA, SA and CA. All these rules do not occur, so its
\(\emptyset\)

\begin{longtable}[]{@{}lllll@{}}
\toprule
& a & b & b & a \\
\midrule
\endhead
x & A, C & S, C & \(\emptyset\) & \(\emptyset\) \\
x & x & B & \(\emptyset\) & A \\
x & x & x & B & S, A \\
x & x & x & x & A,C \\
x & x & x & x & x \\
\bottomrule
\end{longtable}

We conclude that the word is not in the language.

\begin{figure}
\centering
\includegraphics{Pasted_image_20220314195726.png}
\caption{Try it yourself}
\end{figure}

\hypertarget{languages}{%
\section{Languages}\label{languages}}

Languages are the set of valid combinations of an
\href{Alphabet.md}{alphabet} following the \href{Grammar.md}{grammar}
productions specified. The process of proving that a sequence belongs to
a language is called \href{Parsing.md}{parsing}.

Let's do another definition:

A language is a set of \href{../Data/Sentences.md}{sentences} build from
an \href{Alphabet.md}{alphabet}, which for some reason we call correct.
This reason is that the sentences follow the grammar rules we defined
for the language.

Parsing is the act of proving that some input belongs to a set of
correct sentences (the language set).

\hypertarget{types-of-languages}{%
\section{Types of languages}\label{types-of-languages}}

\textbf{\href{Natural\%20languages.md}{Natural languages}} are languages
where you can also have sentences that are almost correct, and they will
still be accepted mostly without any complaints. \textbf{Formal
Languages} are languages where every sentence has to be from the set of
correct sentences, otherwise there will be complaint (by the compiler).

\textbf{Formal Language} A formal language is a language where you
\textbf{have} to follow the rules, and you can not deviate. This means
that you can always use the rewrite rules. Every sentence has to be from
the set of correct sentences, otherwise there will be complaint (by the
compiler). There are different classes of formal languages based on the
rules that the grammar may use. For instance, you have regular languages
and context free languages. These types of grammars are defined by the
\href{Chomsky\%20\%20Hierarchy.md}{Chomsky Hierarchy}.

\hypertarget{zipfian-distribution}{%
\section{Zipfian Distribution}\label{zipfian-distribution}}

With a Ziphian the most frequent class, in this course words, is twice
as likely to occur than the second most frequent word in a language. The
second most frequent is twice as likely to occur than the third frequent
word in a language, and so on. It doesn't have to be twice as likely,
but it just means each rank is much less likely. This means that most of
the probability mass is given to the smallest top.

A Ziphian distribution is often seen with
\href{Natural\%20languages.md}{Natural languages}.

How frequent a word is can be expressed in the rank. The most frequent
word has rank 1, the second most frequent word has rank 2 etc.

\begin{figure}
\centering
\includegraphics{Pasted_image_20220224133421.png}
\caption{Zipfian distribution - The right is in log space and the left
is absolute.}
\end{figure}

This also holds for almost all natural languages. It also holds at the
level of letters. Some letters appear more often than others.

So each thing outside the top 50 is rare, but all the rare things
together still make up 30\% of everything, so you still have to deal
with it.

\hypertarget{explanation-analysis-questions}{%
\section{Explanation / Analysis
Questions}\label{explanation-analysis-questions}}

Practice open questions were provided.

\begin{quote}
These are the Answers are by Marieke.
\end{quote}

\hypertarget{pts---100-words-explanationanalysis-questions}{%
\subsection{8 pts - 100 words: EXPLANATION/ANALYSIS
QUESTIONS}\label{pts---100-words-explanationanalysis-questions}}

\hypertarget{explain-the-steps-involved-in-computing-ppmi-and-what-role-ppmi-plays-in-the-pipeline-used-to-derive-a-count-based-dsm-distributional-semantic-network.}{%
\subsubsection{Explain the steps involved in computing PPMI and what
role PPMI plays in the pipeline used to derive a count-based DSM
(distributional semantic
network).}\label{explain-the-steps-involved-in-computing-ppmi-and-what-role-ppmi-plays-in-the-pipeline-used-to-derive-a-count-based-dsm-distributional-semantic-network.}}

PPMI
(\href{../Semantic-Similarity/Point\%20wise\%20mutual\%20information\%20(PMI).md}{Positive
Point wise mutual information}) is a measure of how informative a word
and context combination is by finding the ratio of the word occurring in
the context divided by the occurrence of the word and the context on
their own (count based probabilities). All negative values are set to 0
to have a sense of interpretability of the informativeness. The PPMI is
then used as weights when computing cosine similarity between vector
embeddings to give more importance to more informative data points.
These weighted cosine similarities can then be leveraged to find
semantic groupings. (95 words)

\hypertarget{break-down-the-working-of-the-cky-algorithm-focusing-on-the-three-nested-loops-used-to-fill-out-the-parse-table}{%
\subsubsection{Break down the working of the CKY algorithm focusing on
the three nested loops used to fill out the parse
table}\label{break-down-the-working-of-the-cky-algorithm-focusing-on-the-three-nested-loops-used-to-fill-out-the-parse-table}}

The \href{../Languages/CKY.md}{CKY} algorithm is used to determine
whether a sentence is valid given a grammar by applying a nested looping
algorithm. It first determines for each item in the sentence which forms
it may take given the grammar, rewriting each item that appears as RHS
of a rule as the LHS. Then each further cell of the table is filled;
take all combinations of subsets of sentence items up to that point,
note the LHS's of the rules for which the subsets make up a RHS. If the
top right cell contains the `S' symbol, the sentence is valid. (99
words)

\hypertarget{consider-bayes-rule-and-pos-tagging-using-hmms-hidden-markov-models.-explain-how-transition-and-emission-probabilities-relate-to-prior-and-likelihood.}{%
\subsubsection{Consider Bayes rule and PoS tagging using HMMs (hidden
Markov models). Explain how transition and emission probabilities relate
to prior and
likelihood.}\label{consider-bayes-rule-and-pos-tagging-using-hmms-hidden-markov-models.-explain-how-transition-and-emission-probabilities-relate-to-prior-and-likelihood.}}

In \href{../Classification/Native\%20baiyes/Bayes\%20rule.md}{Bayes
rule}, the prior represents the expectations the algorithm is given
regarding how often the class will appear. This is the ratio of how
often the class occurs and the size of the document. In HMMs, the
transition probabilities capture this idea by representing how often
each state follows another state. The likelihood in Bayes' rule says how
likely an item is to occur given that its class is known. This relates
to emission probabilities in HMMs because the emission probability also
shows the ratios of each word occurring given a tag. (93 words)

\hypertarget{pts-200-words.-evaluation-questions}{%
\subsection{12 pts -- 200 words. EVALUATION
QUESTIONS}\label{pts-200-words.-evaluation-questions}}

\hypertarget{compare-and-contrast-a-markov-chain-and-an-lstm-for-language-modeling.-highlight-similarities-and-differences-in-training-data-assumptions-goals-architecture-the-design-of-the-algorithm-and-evaluation}{%
\subsubsection{Compare and contrast a Markov Chain and an LSTM for
language modeling. Highlight similarities and differences in training
data, assumptions, goals, architecture (the design of the algorithm) and
evaluation}\label{compare-and-contrast-a-markov-chain-and-an-lstm-for-language-modeling.-highlight-similarities-and-differences-in-training-data-assumptions-goals-architecture-the-design-of-the-algorithm-and-evaluation}}

Both MC's and LTSMs take some form of history into account, but the MC
does it based on the Markov assumption (probability of a word given
total history can be approximated with local history) whereas the LTSM
instead learns to consider some history and forget other parts. MC's
also require smoothing, while LSTM's don't. MC are evaluated with
perplexity, and LSTMs are evaluated by calculating loss between the
prediction and expectation. (71 words)

\hypertarget{compare-lexical-semantic-representations-in-thesauri-a-synonym-dictionary-and-in-dsms-focusing-on-how-they-are-derived-mention-one-advantage-of-dsms-over-thesauri-and-one-advantage-of-thesauri-over-dsms-finally-compare-how-word-similarities-are-computed-in-both-approaches.}{%
\subsubsection{Compare lexical-semantic representations in thesauri (a
synonym dictionary) and in DSMs, focusing on how they are derived;
mention one advantage of DSMs over thesauri and one advantage of
thesauri over DSMs; finally, compare how word-similarities are computed
in both
approaches.}\label{compare-lexical-semantic-representations-in-thesauri-a-synonym-dictionary-and-in-dsms-focusing-on-how-they-are-derived-mention-one-advantage-of-dsms-over-thesauri-and-one-advantage-of-thesauri-over-dsms-finally-compare-how-word-similarities-are-computed-in-both-approaches.}}

\href{../Data/Thesaurus.md}{Thesauri} are created by people, in an
intentional way. The people making thesauri leverage world knowledge and
their linguistic reasoning and understanding to express an agreed upon
definition of the semantic meaning of a word. This has the advantage
that there is transparency of how the representations came to be. Word
similarities can be determined in thesauri by calculating edit
distances, finding Lesk similarity (gloss overlap), or in the case the
thesaurus holds synsets or a tree structure, also with path-length
similarity or Resnik similarity. Another advantage is that the
information in a thesaurus is easier for human interpretation.

DMSMs on the other hand use statistical methods to learn semantic
distributions in an unsupervised way. Each word type gets a vector
embedding based on a chosen metric, such as co-occurrence counts. These
vectors can then be weighted by their information content ((P)PMI) and
the weighted cosine between these vectors then indicates something about
the similarity between words. This has the advantage that no linguistic
experts are needed to index the language, only one or a few
computational linguists and their computers, which makes it cheaper and
faster. It may also reveal latent information about the structure of
language and semantics. (200 words)

\hypertarget{pts---300-words.-synthesis-question}{%
\subsection{20 pts - 300 words. SYNTHESIS
QUESTION}\label{pts---300-words.-synthesis-question}}

\hypertarget{people-read-more-predictable-words-given-the-preceding-context-faster.-discuss-how-you-would-extrinsically-evaluate-two-language-models-an-n-gram-model-and-an-lstm-using-a-dataset-which-provides-the-average-time-it-took-to-people-to-read-each-word-in-a-sentence-as-measured-with-an-eye-tracker.-what-information-would-you-extract-from-each-model-how-do-you-expect-it-relates-to-reading-times-what-data-would-you-use-to-train-the-language-models-for-them-to-be-good-at-predicting-reading-times-justify-your-choice.}{%
\subsubsection{People read more predictable words given the preceding
context faster. Discuss how you would extrinsically evaluate two
language models, an n-gram model and an LSTM, using a dataset which
provides the average time it took to people to read each word in a
sentence as measured with an eye-tracker. What information would you
extract from each model? How do you expect it relates to reading times?
What data would you use to train the language models for them to be good
at predicting reading times? Justify your
choice.}\label{people-read-more-predictable-words-given-the-preceding-context-faster.-discuss-how-you-would-extrinsically-evaluate-two-language-models-an-n-gram-model-and-an-lstm-using-a-dataset-which-provides-the-average-time-it-took-to-people-to-read-each-word-in-a-sentence-as-measured-with-an-eye-tracker.-what-information-would-you-extract-from-each-model-how-do-you-expect-it-relates-to-reading-times-what-data-would-you-use-to-train-the-language-models-for-them-to-be-good-at-predicting-reading-times-justify-your-choice.}}

The extrinsic evaluation of each model can be done by comparing the
reading times to the probability output of each model for certain words
(correlation). From the \href{../Languages/N-grams.md}{N-grams} model,
we would take the perplexity for each word and expect it to be a good
predictor for reading time, with lower perplexity correlating with
shorter reading time and vice versa. In the LSTM the expectation would
be that a higher probability for a word given a context leads to shorter
reading times and vice versa. The data for training the models should be
representative for the goal, so in this case it should be text that is
usually read by people, such as Wikipedia articles or subtitles. This
should ideally be related to or match with the dataset that was used to
generate the reading times. (135 words)

\hypertarget{goals-1}{%
\section{Goals}\label{goals-1}}

The goals of looking at natural languages are:

\begin{itemize}
\tightlist
\item
  Infer the components symbols of a language, their roles, the rules for
  combining them, and their meaning.
\item
  Formalize the rules for combining symbols
\item
  Combine atomic meanings. Atomic is the smallest part so how do you
  combine the meanings of the smallest parts.
\item
  Understand large portions of text.
\item
  Produce complex sentences.
\end{itemize}

\hypertarget{computational-linguistics-class-about-the-bert-paper}{%
\section{Computational linguistics class about the BERT
paper}\label{computational-linguistics-class-about-the-bert-paper}}

BERTology. (Bidirectional Encoder Representations from Transformers).
Headings are per section of the paper.

\begin{quote}
Thanks to Marike for this file
\end{quote}

\hypertarget{advantages-of-bert-as-compared-to-other-neural-net-language-models.}{%
\subsection{Advantages of BERT as compared to other (neural net)
language
models.}\label{advantages-of-bert-as-compared-to-other-neural-net-language-models.}}

BERT is better at dealing with longer sentences (long-range
dependencies). This is because it is a transformer model (not a neural
net, not exactly deep learning). Problem with recurrent neural nets:
\href{../Prediction/Vanishing\%20gradient\%20problem.md}{Vanishing
gradient problem}. This means that if you go too far back in the input,
it gets multiplied by the 0-1 parameters over and over again and gets
smaller and smaller; this makes it so that the model stops learning. The
BERT model (transformer) doesn't have this issue, as it uses a different
version of recurrence.

Parallel processing is also efficient in BERT given the right hardware.
The model is pre-trained on a huge amount of data, and it is very big;
the architecture itself is efficient (more than LSTM) but very big.

\hypertarget{explain-the-workflow-of-bert.}{%
\subsection{Explain the workflow of
BERT.}\label{explain-the-workflow-of-bert.}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Pre-training, self-supervised. Train model with a large amount of data
  (Wikipedia, books, etc). To learn general knowledge about language.

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Masked language modelling: given a sentence, mask out a word and
    train the model to predict the missing word (MLM)
  \item
    Next sentence prediction: given two sentences, the model predicts
    whether they are consecutive (NSP)
  \end{enumerate}
\item
  Finetuning (adding one or more layers on top of the pre-trained
  architecture). Here, the model is trained on a more specific task or
  dataset, such as sentiment analysis or POS tagging.
\end{enumerate}

\hypertarget{which-type-of-syntactic-information-does-bert-seem-to-learn}{%
\subsection{Which type of syntactic information does BERT seem to
learn}\label{which-type-of-syntactic-information-does-bert-seem-to-learn}}

\emph{Syntax in language is very symbolic. Finding information in a NN
about its syntactic knowledge is hard; is it representing the right
thing?}

The model has some representations of hierarchical information (word
dependency tree structures). More specifically; part-of-speech and
chunks and thematic roles (agent-patient) are embedded in the model.
This is impressive as the model learns unsupervised using NSP and MLM.
Model is not very good at Negative Polarity Items (between syntax and
semantics); it can understand the use of `Whether..ever' but not the
correct scope of those or negations. The syntactic information that the
model learns is not necessarily similar to annotated linguistic
resources.

\hypertarget{what-is-the-reason-why-bert-struggles-with-world-based-reasoning}{%
\subsection{What is the reason why BERT struggles with world-based
reasoning?}\label{what-is-the-reason-why-bert-struggles-with-world-based-reasoning}}

My guess: maybe the MLM unsupervised learning leads to this difficulty
and the other way around (predict sentence from word) might improve the
ability of the model to perform reasoning tasks. Raquel says, `idk, but
interesting!'.

The model might also improve when training on structure and relationship
(tagged) data, rather than pure unsupervised learning. Humans might have
a specific reasoning ability that is not built into the model. A lot of
knowledge and reasoning and world-awareness is known by people, which is
not built into our language. This means the data the model is trained on
is not complete with this inferred information; we don't say `I walk
into the house because the house is bigger than I am'. When we say `I
walk into the house', there is a natural implication and social
understanding that the house is indeed bigger than I am. The model does
not have this information in the training data and as such cannot (does
not) learn the reasoning.

\hypertarget{if-you-have-a-bert-model-with-vectors-that-reflect-semantic-relations-then-which-part-of-the-model-represents-the-semantic-similarity-space-higher-layers-intermediate-combinations}{%
\subsection{If you have a BERT model with vectors that reflect semantic
relations, then which part of the model represents the semantic
similarity space? (Higher layers, intermediate,
combinations?)}\label{if-you-have-a-bert-model-with-vectors-that-reflect-semantic-relations-then-which-part-of-the-model-represents-the-semantic-similarity-space-higher-layers-intermediate-combinations}}

Semantic information is represented throughout the model's layers. In
later stages, it involves tokens (individual occurrences of words) and
in earlier layers, the semantic value of the word more generally
speaking. Then there is also the vector space which collapses the
semantic information into a sort of `cone' shape in the vector space.

Semantics are learnt from the context in the MLM task. The model gives
us a different vector for each token; so every occurrence of a word,
even if it's the same word, gets its own representation vector. These
need to be aggregated for getting type information. The BERT model is
very deep (word2vec is not deep; only the first layer is used). In BERT
you can use different layers' information depending on your task.
Probing can be done to see which layer is more useful, can also do
averages or weighted averages. For example, can take the first layer(s)
because they are more useful for getting a certain type of information
about a sentence.

\hypertarget{how-would-you-change-your-answer-to-the-previous-question-after-reading-the-following-section-4.3-bert-layers-if-at-all}{%
\subsection{How would you change your answer to the previous question
after reading the following section, 4.3 BERT Layers, if at
all?}\label{how-would-you-change-your-answer-to-the-previous-question-after-reading-the-following-section-4.3-bert-layers-if-at-all}}

Combining information from different layers is the best (only?) way to
get the semantic information. This is spread across all layers of the
model, independent of the model size. The first layers capture simple
word order rules, middle layers syntactic information, and later layers
capture task specific information. The semantics are weaved through all
layers; this makes intuitive sense too because often the semantic
meaning of our real-world language use also changes based on context
(task) and social situation (grammar, word order).

In the end, language is used to communicate; semantics (what does it
\emph{mean}) is a basic goal of language use. Everything else always
interacts and interfaces with semantics.

\hypertarget{why-is-overparametrization-a-problem}{%
\subsection{Why is overparametrization a
problem?}\label{why-is-overparametrization-a-problem}}

Typically, the more parameters a model has, the more it can learn,
though it can also lead to
\href{../Prediction/Overfitting.md}{overfitting}. However, with BERT the
problem is more so that the enormous amount of parameter also leads to
intense computational complexity and electricity use. This is an
environmental concern, especially as long as the majority of power is
not generated in a clean (green) way. It also leads to differences
between countries, companies and individuals with more vs less resources
(financial, hardware, human). Reproducibility is also a big issue; the
bigger the model, the harder it is to reproduce and do science on
whether it is really that good or if they be lyin' to us.

\hypertarget{quiz-1-resources}{%
\section{Quiz 1 Resources}\label{quiz-1-resources}}

\hypertarget{question-1}{%
\subsection{Question 1}\label{question-1}}

In CL we use a lot of statistics and probability theory. Why?

\begin{itemize}
\tightlist
\item
  A: They give us several distributions to characterise how people use
  language.
\item
  B: Natural languages are ambiguous.
\item
  C: We need to perform statistical tests to compare models.
\item
  D: They offer a set of efficient algorithms for automating processes.
\end{itemize}

\hypertarget{question-2}{%
\subsection{Question 2}\label{question-2}}

Which of the following features does not apply to natural languages?

\begin{itemize}
\tightlist
\item
  A: Subjected to change
\item
  B: Unambiguous
\item
  C: Conventional
\item
  D: Context-dependent
\end{itemize}

\hypertarget{question-3}{%
\subsection{Question 3}\label{question-3}}

Which level of linguistic analysis deals with the meaning of morphemes
and words?

\begin{itemize}
\tightlist
\item
  A: Lexical semantics
\item
  B: Syntax
\item
  C: Morphology
\item
  D: Compositional semantics
\end{itemize}

\hypertarget{question-4}{%
\subsection{Question 4}\label{question-4}}

Which of the following is a diachronic corpus?

\begin{itemize}
\tightlist
\item
  A: CHILDES
\item
  B: SubtLex
\item
  C: Corpus of Historical American English
\item
  D: TASA
\end{itemize}

\hypertarget{question-5}{%
\subsection{Question 5}\label{question-5}}

Which of the following resources is not a lexicon?

\begin{itemize}
\tightlist
\item
  A: Words with proportion of native speakers who know the word meaning
\item
  B: Words with concreteness ratings
\item
  C: Words with age of acquisition estimates
\item
  D: Words with their meaning definition
\end{itemize}

\hypertarget{question-6}{%
\subsection{Question 6}\label{question-6}}

What is a valid hyponym of dog in WordNet?

\begin{itemize}
\tightlist
\item
  A: Dalmatian
\item
  B: Animal
\item
  C: Canine
\item
  D: Cat
\end{itemize}

\hypertarget{correct-answers-quiz-1}{%
\subsection{Correct Answers Quiz 1}\label{correct-answers-quiz-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  B: Natural languages are ambiguous.
\item
  B: Unambiguous
\item
  A: Lexical semantics
\item
  C: Corpus of Historical American English
\item
  D: Cat
\end{enumerate}

\hypertarget{quiz-2-nauxefve-bayes-classification}{%
\section{Quiz 2 Na√Øve Bayes
Classification}\label{quiz-2-nauxefve-bayes-classification}}

\hypertarget{question-1-1}{%
\subsection{Question 1}\label{question-1-1}}

Which of the following is not an example of text classification?

\begin{itemize}
\tightlist
\item
  A: Essay grading (pass/fail)
\item
  B: Text simplification
\item
  C: Sentiment analysis
\item
  D: Cyberbullying detection
\end{itemize}

\hypertarget{question-2-1}{%
\subsection{Question 2}\label{question-2-1}}

Which of the following is an advantage of rule systems?

\begin{itemize}
\tightlist
\item
  A: Robust to rare events
\item
  B: Cheap to write
\item
  C: Cannot incorporate domain knowledge
\item
  D: Can deal with ambiguity effortlessly
\end{itemize}

\hypertarget{question-3-1}{%
\subsection{Question 3}\label{question-3-1}}

Which of the following statements about discriminative classifiers is
wrong?

\begin{itemize}
\tightlist
\item
  A: They can only address binary classification problems
\item
  B: They learn the hidden process which yielded the data sample
\item
  C: They can only learn linear boundaries
\item
  D: They are non-deterministic classifiers
\end{itemize}

\hypertarget{question-4-1}{%
\subsection{Question 4}\label{question-4-1}}

Which of the following is an example of extrinsic evaluation?

\begin{itemize}
\tightlist
\item
  A: Run a t-test between precision scores in automatic grading
\item
  B: Compute the difference in accuracy between two classifiers
\item
  C: Measure the customer satisfaction when interacting with two
  different bots
\item
  D: Compare translation quality between two machine translation models
\end{itemize}

\hypertarget{question-5-1}{%
\subsection{Question 5}\label{question-5-1}}

What does the likelihood capture in Bayes Rule?

\begin{itemize}
\tightlist
\item
  A: The probability of the class given the input
\item
  B: The probability of the input
\item
  C: The probability of the input given the class
\item
  D: The probability of the class
\end{itemize}

\hypertarget{question-6-1}{%
\subsection{Question 6}\label{question-6-1}}

What does the conditional independence assumption entail in NBC?

\begin{itemize}
\tightlist
\item
  A: An NBC doesn't track feature co-presence
\item
  B: An NBC doesn't consider the probability of the document given the
  class
\item
  C: An NBC doesn't track sequential information
\item
  D: An NBC doesn't consider all classes when classifying
\end{itemize}

\hypertarget{question-7}{%
\subsection{Question 7}\label{question-7}}

Which of the following is not a stop word?

\begin{itemize}
\tightlist
\item
  A: I
\item
  B: Child
\item
  C: Do
\item
  D: Because
\end{itemize}

\hypertarget{question-8}{%
\subsection{Question 8}\label{question-8}}

In a dataset consisting of 100 tweets, 20 contain instances of
cyberbullying. For the sake of argument, we pretend to be dealing with 2
binary features: whether the tweet contains at least a curse word and
whether the tweet contain non-alphabetic characters. The likelihood that
a tweet containing at least a curse word is an instance of cyberbullying
is 0.8 while the likelihood that a tweet containing non-alphabetic
characters is not an instance of cyberbullying is 0.7.

What is the prior of the cyberbullying class?

\begin{itemize}
\tightlist
\item
  A: 0.1
\item
  B: 0.8
\item
  C: cannot tell
\item
  D: 0.2
\end{itemize}

\hypertarget{question-9}{%
\subsection{Question 9}\label{question-9}}

In a dataset consisting of 100 tweets, 20 contain instances of
cyberbullying. For the sake of argument, we pretend to be dealing with 2
binary features: whether the tweet contains at least a curse word and
whether the tweet contain non-alphabetic characters. The likelihood that
a tweet containing at least a curse word is an instance of cyberbullying
is 0.8 while the likelihood that a tweet containing non-alphabetic
characters is not an instance of cyberbullying is 0.3.

Consider a test tweet with at least a curse word and only alphabetic
characters. What is the probability of the test tweet being an instance
of cyberbullying?

\begin{itemize}
\tightlist
\item
  A: 0.8 \(*\) 0.8 \(*\) 0.3
\item
  B: 0.8 \(*\) 0.2 \(*\) 0.7
\item
  C: 0.2 \(*\) 0.8 \(*\) 0.3
\item
  D: 0.2 \(*\) 0.2 \(*\) 0.7
\end{itemize}

\hypertarget{question-10}{%
\subsection{Question 10}\label{question-10}}

In a dataset consisting of 100 tweets, 20 contain instances of
cyberbullying. For the sake of argument, we pretend to be dealing with 2
binary features: whether the tweet contains at least a curse word and
whether the tweet contain non-alphabetic characters. The likelihood that
a tweet containing at least a curse word is an instance of cyberbullying
is 0.8 while the likelihood that a tweet containing non-alphabetic
characters is not an instance of cyberbullying is 0.3.

Consider a test tweet with at least a curse word and only alphabetic
characters. Would an NBC using these features classify it as an instance
of cyberbullying?

\begin{itemize}
\tightlist
\item
  A: Yes
\item
  B: Not enough information given
\item
  C: It'd be a tie
\item
  D: No
\end{itemize}

\hypertarget{correct-answers-quiz-2}{%
\subsection{Correct Answers Quiz 2}\label{correct-answers-quiz-2}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  B: Text Simplification
\item
  A: Robust to rare events
\item
  B: They learn the hidden process which yielded the data sample
\item
  C: Measure the customer satisfaction when interacting with two
  different bots
\item
  C: The probability of the input given the class
\item
  A: An NBC doesn't track feature co-presence
\item
  B: Child
\item
  D: 0.2
\item
  C: 0.2 \(*\) 0.8 \(*\) 0.3
\item
  D: No
\end{enumerate}

\hypertarget{quiz-3-pre-processing}{%
\section{Quiz 3 Pre-processing}\label{quiz-3-pre-processing}}

\hypertarget{question-1-2}{%
\subsection{Question 1}\label{question-1-2}}

How many lemmas are there in the sentence:

``The children were curious about whether there would be a surprise at
home or whether there had been enough surprises already.''

Punctuation doesn't count.

\begin{itemize}
\tightlist
\item
  A: 21
\item
  B: 19
\item
  C: 18
\item
  D: 16
\end{itemize}

\hypertarget{question-2-2}{%
\subsection{Question 2}\label{question-2-2}}

How many affixes are there in the word untrustworthy?

\begin{itemize}
\tightlist
\item
  A: 3
\item
  B: 2
\item
  C: 0
\item
  D: 1
\end{itemize}

\hypertarget{question-3-2}{%
\subsection{Question 3}\label{question-3-2}}

Which of the following words is inflected?

\begin{itemize}
\tightlist
\item
  A: Touchstone
\item
  B: Colourful
\item
  C: Children
\item
  D: Professor
\end{itemize}

\hypertarget{question-4-2}{%
\subsection{Question 4}\label{question-4-2}}

Which normalisation technique would you use before doing language
identification?

\begin{itemize}
\tightlist
\item
  A: Lemmatisation
\item
  B: Case folding
\item
  C: None of them
\item
  D: Tokenisation
\end{itemize}

\hypertarget{question-5-2}{%
\subsection{Question 5}\label{question-5-2}}

Consider two regular expression /\footnote{a-zA-Z}\{2,6\}\b/. What does
it match?

\begin{itemize}
\tightlist
\item
  A: alphabetic strings between two and six characters at the beginning
  of a line followed by a word boundary Correct!
\item
  B: any string but alphabetic strings between two and six characters
\item
  C: Lines containing alphabetic strings between two and six characters
\item
  D: any string between three and six characters
\end{itemize}

\hypertarget{question-6-2}{%
\subsection{Question 6}\label{question-6-2}}

What is the minimum edit distance between glowing and growling?

\begin{itemize}
\tightlist
\item
  A: 4
\item
  B: 1
\item
  C: 2
\item
  D: 3
\end{itemize}

\hypertarget{correct-answers-quiz-3}{%
\subsection{Correct Answers Quiz 3}\label{correct-answers-quiz-3}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  D: 16
\item
  B: 2
\item
  C: Children
\item
  C: None of them
\item
  A: alphabetic strings between two and six characters at the beginning
  of a line followed by a word boundary
\item
  C: 2
\end{enumerate}

\hypertarget{quiz-4-language-modelling}{%
\section{Quiz 4 Language modelling}\label{quiz-4-language-modelling}}

\hypertarget{question-1-3}{%
\subsection{Question 1}\label{question-1-3}}

How do we use language modelling in machine translation?

\begin{itemize}
\tightlist
\item
  A: To make sure the translation has the same meaning as the source
\item
  B: To predict the next sentence in the translation
\item
  C: To pick the most fluent candidate translation\\
\item
  D: To pick the best word among possible candidate translations for a
  word in the source
\end{itemize}

\hypertarget{question-2-3}{%
\subsection{Question 2}\label{question-2-3}}

Why do we care about the chain rule of probability?

\begin{itemize}
\tightlist
\item
  A: It tells us how to compute the probability of a sentence
\item
  B: It deals with the infinite nature of language
\item
  C: It tells us how to use limited context to approximate larger
  contexts
\item
  D: It tells us how to deal with underflowing problems
\end{itemize}

\hypertarget{question-3-3}{%
\subsection{Question 3}\label{question-3-3}}

How do we get ML estimates for bigram transition probabilities?

\begin{itemize}
\tightlist
\item
  A: Get co-occurrence counts and normalise by row marginals
\item
  B: Get co-occurrence counts and take the log
\item
  C: Get co-occurrence counts and normalise by column marginals
\item
  D: Get co-occurrence counts and normalise by the matrix total
\end{itemize}

\hypertarget{question-4-3}{%
\subsection{Question 4}\label{question-4-3}}

Which of the following is NOT a component of Markov Chains?

\begin{itemize}
\tightlist
\item
  A: Transition counts
\item
  B: Initial probability distribution
\item
  C: Accepting state
\item
  D: History states
\end{itemize}

\hypertarget{question-5-3}{%
\subsection{Question 5}\label{question-5-3}}

If we fit a 4-gram language model, how many BoS symbols do we need to
prepend to the sentences?

\begin{itemize}
\tightlist
\item
  A: 1
\item
  B: 4
\item
  C: 2
\item
  D: 3
\end{itemize}

\hypertarget{question-6-3}{%
\subsection{Question 6}\label{question-6-3}}

In linear interpolation, lambdas have to meet a strict requirement.
Which one?

\begin{itemize}
\tightlist
\item
  A: Their sum must equal 1
\item
  B: They must be lower than 1
\item
  C: The highest value matches the largest n-gram available
\item
  D: Their algebraic sum must be 0
\end{itemize}

\hypertarget{correct-answers-quiz-4}{%
\subsection{Correct Answers Quiz 4}\label{correct-answers-quiz-4}}

\begin{itemize}
\tightlist
\item
  C: To pick the most fluent candidate translation
\item
  A: It tells us how to compute the probability of a sentence
\item
  A: Get co-occurrence counts and normalise by row marginals
\item
  C: Accepting state
\item
  D: 3
\item
  A: Their sum must equal 1
\end{itemize}

\hypertarget{quiz-5-pos-tagging}{%
\section{Quiz 5 PoS Tagging}\label{quiz-5-pos-tagging}}

\hypertarget{question-1-4}{%
\subsection{Question 1}\label{question-1-4}}

Which of the following lexical categories is an example of open class
words?

\begin{itemize}
\tightlist
\item
  A: Auxiliaries
\item
  B: Possessive pronouns
\item
  C: Adverbs
\item
  D: Conjunctions
\end{itemize}

\hypertarget{question-2-4}{%
\subsection{Question 2}\label{question-2-4}}

In terms of PoS tag ambiguity, types tend to be \ldots. tokens?

\begin{itemize}
\tightlist
\item
  A: More ambiguous than
\item
  B: As unambiguous as
\item
  C: Less ambiguous than
\item
  D: As ambiguous as
\end{itemize}

\hypertarget{question-3-4}{%
\subsection{Question 3}\label{question-3-4}}

What does the emission probability matrix encode in a bigram HMM?

\begin{itemize}
\tightlist
\item
  A: The probability of a word given a word
\item
  B: The probability of a tag given a word
\item
  C: The probability of a tag given a tag
\item
  D: The probability of a word given a tag
\end{itemize}

\hypertarget{question-4-4}{%
\subsection{Question 4}\label{question-4-4}}

Which component of the HMM encodes the Markov assumption?

\begin{itemize}
\tightlist
\item
  A: The sequence of observations
\item
  B: The observation likelihood matrix
\item
  C: The initial distribution
\item
  D: The state transition probability matrix
\end{itemize}

\hypertarget{question-5-4}{%
\subsection{Question 5}\label{question-5-4}}

The Viterbi algorithm is an example of?

\begin{itemize}
\tightlist
\item
  A: A classifier
\item
  B: A dynamic programming algorithm
\item
  C: A rule-based system
\item
  D: A vector space
\end{itemize}

\hypertarget{question-6-4}{%
\subsection{Question 6}\label{question-6-4}}

What is the complexity of the Viterbi algorithm? Q is the set of states,
t is the length of the sentence, n is the order of the model, V is the
vocabulary size.

\begin{itemize}
\tightlist
\item
  A: \(O(Q^t)\)
\item
  B: \(O(V^t*n)\)
\item
  C: \(O(Q^n*t)\)
\item
  D: \(O(Q*V)\)
\end{itemize}

\hypertarget{question-7-1}{%
\subsection{Question 7}\label{question-7-1}}

Which of the following quantities does not contribute to the computation
of the new posterior probability of observing each tag given the
sequence of observed events up to that point?

\begin{itemize}
\tightlist
\item
  A: Transition probabilities from state \(q_i\) to state \(q_j\)
\item
  B: Emission probability for observation o\_j given state \(q_j\)
\item
  C: Posterior probability up to the previous observed event
\item
  D: The likelihood of state \(q_j\) given observed event \(o_j\)
\end{itemize}

\hypertarget{question-8-1}{%
\subsection{Question 8}\label{question-8-1}}

In the Viterbi algorithm, we apply the argmax function. What is the
input?

\begin{itemize}
\tightlist
\item
  A: The last column in the trellis
\item
  B: The product of the last column in the trellis, the transition
  probability matrix and the emission probability matrix
\item
  C: The product of the last column in the trellis and the transition
  probability matrix
\item
  D: The product of the transition and emission probability matrices
\end{itemize}

\hypertarget{correct-answers-quiz-5}{%
\subsection{Correct Answers Quiz 5}\label{correct-answers-quiz-5}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  C: Adverbs
\item
  D: Less ambiguous than
\item
  D: The probability of a word given a tag
\item
  D: The state transition probability matrix
\item
  B: A dynamic programming algorithm
\item
  C: \(O(Q^n*t)\)
\item
  D: The likelihood of state \(q_j\) given observed event \(o_j\)
\item
  C: The product of the last column in the trellis and the transition
  probability matrix
\end{enumerate}

\hypertarget{quiz-6-syntax}{%
\section{Quiz 6 Syntax}\label{quiz-6-syntax}}

\hypertarget{question-1-5}{%
\subsection{Question 1}\label{question-1-5}}

Which of the following is not a component of a grammar?

\begin{itemize}
\tightlist
\item
  A: a finite set of non-terminal states
\item
  B: a distinguished start state
\item
  C: a finite set of terminal states
\item
  D: an infinite set of production rules
\end{itemize}

\hypertarget{question-2-5}{%
\subsection{Question 2}\label{question-2-5}}

Which of the following rules is in CNF?

\begin{itemize}
\tightlist
\item
  A: S ‚Üí he ran
\item
  B: S ‚Üí NP VP NP
\item
  C: S ‚Üí you VP
\item
  D: S ‚Üí NP VP
\end{itemize}

\hypertarget{question-3-5}{%
\subsection{Question 3}\label{question-3-5}}

What kind of corpus do we need to estimate a CFG?

\begin{itemize}
\tightlist
\item
  A: Plain corpus
\item
  B: Parallel corpus
\item
  C: Treebank
\item
  D: Corpus with PoS annotations
\end{itemize}

\hypertarget{question-4-5}{%
\subsection{Question 4}\label{question-4-5}}

Which is the defining feature of an S constituent?

\begin{itemize}
\tightlist
\item
  A: Its main verb has all its arguments
\item
  B: Can only occur as the LHS of rules
\item
  C: Cannot be coordinated with other S constituents
\item
  D: Always contains at least one NP
\end{itemize}

\hypertarget{question-5-5}{%
\subsection{Question 5}\label{question-5-5}}

What is the relation between CFGs and dynamic programming?

\begin{itemize}
\tightlist
\item
  A: We can use dynamic programming because context changes our
  sub-parses
\item
  B: We cannot use dynamic programming because context will change our
  sub-parses
\item
  C: We can use dynamic programming because context cannot change
  sub-parses
\item
  D: They're unrelated
\end{itemize}

\hypertarget{question-6-5}{%
\subsection{Question 6}\label{question-6-5}}

In order to initialise the table for the CKY, what do we need to know?
Group of answer choices.

\begin{itemize}
\tightlist
\item
  A: The number of possible rules in the grammar
\item
  B: The number of non-terminals in the grammar
\item
  C: The number of terminals in the grammar
\item
  D: The symbols in the target sentence
\end{itemize}

\hypertarget{question-7-2}{%
\subsection{Question 7}\label{question-7-2}}

How do we use the CKY to know if a string is grammatical? Group of
answer choices.

\begin{itemize}
\tightlist
\item
  A: We check whether the final cell contains the S state
\item
  B: We check whether the final cell is not empty
\item
  C: We check whether the S state happens anywhere in the table
\item
  D: We check whether all terminals appear as the RHS in at least one
  rule
\end{itemize}

\hypertarget{question-8-2}{%
\subsection{Question 8}\label{question-8-2}}

Which of the following is a pre-terminal symbol?

\begin{itemize}
\tightlist
\item
  A: N
\item
  B: NP
\item
  C: PP
\item
  D: do
\end{itemize}

\hypertarget{correct-answers-quiz-6}{%
\subsection{Correct Answers Quiz 6}\label{correct-answers-quiz-6}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A: an infinite set of production rules
\item
  D: S ‚Üí NP VP
\item
  C: Treebank
\item
  A: Its main verb has all its arguments
\item
  C: We can use dynamic programming because context cannot change
  sub-parses
\item
  D: The symbols in the target sentence
\item
  A: We check whether the final cell contains the S state
\item
  A: N
\end{enumerate}

\hypertarget{things-he-said-come-at-the-midterm}{%
\section{Things he said come at the
midterm}\label{things-he-said-come-at-the-midterm}}

\begin{itemize}
\tightlist
\item
  \href{../Classification/Evaluating\%20Classification\%20models.md}{Evaluating
  Classification models} evaluation
\item
  Coming up with your own intrinsic extrinsic evaluation methods
\item
  Filling out part of the
  \href{../Prediction/Viterbi\%20Algorithm.md}{Viterbi Algorithm} square
\item
  \href{../Classification/Native\%20baiyes/Bayes\%20rule.md}{Bayes rule}
\item
  Just look and practice the quizzes
\item
  \href{../Languages/Regular\%20expression.md}{Regular expression}
\end{itemize}

\hypertarget{things-he-said-come-at-the-final}{%
\section{Things he said come at the
final}\label{things-he-said-come-at-the-final}}

\begin{itemize}
\tightlist
\item
  Calculate the \href{../Semantic-Similarity/Co-occurrence.md}{PMI}
  between two words given a
  \href{../Semantic-Similarity/Co-occurrence.md}{Co-occurrence} matrix.
\end{itemize}

\hypertarget{things-he-said}{%
\section{Things he said}\label{things-he-said}}

\begin{itemize}
\tightlist
\item
  Grammar is a descriptive tool, not a normative tool.
\item
  CKY algorithm.
\end{itemize}

\hypertarget{what-is-learning}{%
\section{What is learning}\label{what-is-learning}}

Learning is an experience-driven, long term, relational process which
results in a change in how we interact with the environment due to the
information we have come into contact with.

Learning is when a system can use the information it has seen in the
past to infer general rules about other information in the future.
Learning is not memorizing.

Learning = generalize, infer, analogize (building analogies for
different situations), adaptive (if the environment changes then you
don't break down).

Learning is always a change. If you don't change you probably have not
learned a lot. The change manifests itself in a long-lasting change with
how the system interacts with the environment.

\end{document}
