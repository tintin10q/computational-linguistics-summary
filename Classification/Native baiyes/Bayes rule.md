# Bayes Rule

Bayes rule is one of the most important formulas.

$$p(c|d) = \frac{p(d|c)*p(c)}{p(d)}$$

Read $|$ as "given". 

The formula in English: 

> **Bayes Rule**
> The probability of a certain class given a data point or document is equal to the probability of a class given the data point multiplied by the probability of the class decided by the probability of the data point/document. 

In order to assign a class to a data point we consider 3 things:

1. p(d|c) The probability that a data point is a certain class. Likelihood.
2. p(c) The probability that a certain class appears (regardless of the class). Prior.
3. p(d) The probability that a certain data point appears (regardless of the data point).

- You can also express 2 as what we expect about the class. In some situations some things are expected to occur much more often. This is the **prior**. Your preconceptions.
- You can also express 1 as what the evidence says about how likely a class is based on a data point.  This is the likelihood. This encodes the evidence that an instance has been generated by a given class. 

This serves as model of how to think. You have your bias where you except things. This is p(c) or the prior. You also have the evidence you have. If you are super convinced of something you might discard evidence but if you only look at the evidence (frequentist approach) you discard the important factor of what you believe now. 

So when they tried to prove that the sun is the middle of the solar system the belief that the earth was in the middle was so large that the evidence was almost not enough. 

## Posterior p(c|d)
This is how likely it is that you have an example of class c given the instance you are seeing. This is what we are after. You get this when running the formula. 

# Naïve Bayes
Naïve Bayes than works by calculating like this:

$$C = \text{argmax}_{c \in C} P(c) \prod_{f \in F} p(f|c)$$

So here you take as your prediction the max posterior probability of the top part of the Bayes calculation which is the product of the probability of the features given the class. 

Because you multiply a lot of probabilities computers are bad at this and causes [underflow](https://en.wikipedia.org/wiki/Floating-point_arithmetic)

So instead we do we can move from probability space to log space and when we do that you have to move to sum instead of multiplication:

$$C = \text{argmax}_{c \in C}~log(p(c)) \sum_{f \in F} log(p(f|c))$$

You leave out the bottom division because you assume independence. This way the probability of a point appearing is always 1. X divided by 1 is X.
