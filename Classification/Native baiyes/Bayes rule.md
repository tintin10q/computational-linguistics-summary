# Bayes Rule

Bayes rule is one of the most important formulas.

$$p(c|d) = \frac{p(d|c)*p(c)}{p(d)}$$

Read $|$ as “given”. 

The formula in English: 

> **Bayes Rule**
> The probability of a certain class given a data point or document is equal to the probability of a class given the data point multiplied by the probability of the class decided by the probability of the data point/document. 

In order to assign a class to a data point, we consider 3 things:

1. $p(d|c)$ - The probability that a data point is a certain class. Likelihood.
2. $p(c)$ - The probability that a certain class appears (regardless of the class). Prior.
3. $p(d)$ - The probability that a certain data point appears (regardless of the data point).

- You can also express 2 as what we expect about the class. In some situations, some things are expected to occur much more often. This is the **prior**. Your preconceptions.
- You can also express 1 as what the evidence says about how likely a class is based on a data point.  This is the **likelihood**. This encodes the evidence that an instance has been generated by a given class. 

The Bayes rule serves as a model of how to think. You have your bias where you expect things. This is p(c) or the prior. You also have the current evidence you are presented with. If you are super convinced of something you might discard evidence, but if you only look at the evidence (frequentist approach) you discard the important factor of what you already believe now. 

So when they tried to prove that the sun is the middle of the solar system, the belief that the earth was in the middle was so large that the evidence was almost not enough. 

## Posterior $p(c|d)$
This is how likely it is that you have an example of class c given the instance you are seeing. This is what we are after. You get this when running the formula. 

# Naïve Bayes

Naïve Bayes works by calculating like this:

$$C = \text{argmax}_{c \in C} P(c) \prod_{f \in F} p(f|c)$$

So here you take as your prediction the max posterior probability of the top part of the Bayes calculation, which is the product of the probability of the features given the class. 

Because you multiply a lot of probabilities, computers are bad at this and causes [underflow](https://en.wikipedia.org/wiki/Floating-point_arithmetic)

So instead we do we can move from probability space to log space and when we do that you have to move to sum instead of multiplication:

$$C = \text{argmax}_{c \in C}~log(p(c)) \sum_{f \in F} log(p(f|c))$$

You leave out the bottom division because you assume independence. This way, the probability of a point appearing is always 1. X divided by 1 is X.


# How to do a Bayesian Classification Question 

f3 help: Bayes Rule Bayesian Classification Basian question baysian bayias

1. Likelihood: p(d|c) The probability that a data point is a certain class. Likelihood.
2. Prior: p(c) The probability that a certain class appears (regardless of the class). Prior.
3. p(d) The probability that a certain data point appears (regardless of the data point).


Ask:

1. What is the total datapoints count?
2. What is the count of each class?
    - Remember you have is-class and is-not class
    - From 20 is class you know 80 is-not class if 100 datapoints.
    - So isclass = 20/100 = 0.2
    - So notclass = 80/100 = 0.8
3. Now there will be multiple (boolean) features. A feature can give indication for a class or not the class.  


| x          | has-feature | not feature |
|------------|-------------|-------------|
| is class   |             |             |
| not class  |             |             |

So he will say something like if it has the feature the probability that it has the class is 0.3. With that you can fill in the table. 

| x          | has-feature | not feature |
|------------|-------------|-------------|
| is class   | 0.3         | 0.7         |
| not class  |             |             |

Then you can fill in the other side of the table. You have to do this for all the features he gives you.

| x          | has-feature | not feature |
|------------|-------------|-------------|
| is class   | 0.3         | 0.7         |
| not class  | 0.7         | 0.3         |

4. Then he will ask either for prior or for posterior.
   - Prior is simpel. Chance of the class. 
     - p(c) = Change of class (1)
     - documents in class / total documents 
   - Posterior = class given the document
     - He describes a document and you need to find the probability 
     - Now we do the formula: Prior $\cdot \Pi$ p(d|c) 
     - p(d|c) is chance of datapoint given class 
     - Don't panic just look p(d|c) up in the tables you made.
     - Too hard? Right down the features as one word and look them up with the row of the class and the column on the feature. 
   - He can also ask what is the class?
     - Here you have to make a decision on the class like a rational person
     - Calculate probability of each class based on the features of the document
     - TAKE THE HIGHEST ONE!

### Naïve Bayes full example 

In a dataset consisting of 100 tweets, 20 contain instances of cyberbullying. 

Each tweet has 2 binary features: 

- Tweet contains at least a curse word.
- Tweet contain non-alphabetic characters. 

The likelihood that a tweet containing a curse word is cyberbullying is 0.8 

The likelihood that a tweet containing non-alphabetic characters is not an instance of cyberbullying is 0.3. 

> **What is the prior of the cyberbullying class?**
> 
> Simple, 100 tweets in total. 
> 80 not cyberbully, 20 cyberbully.
> Prior of cyberbully is 20/100 = 0.2

Consider a test tweet with curse words 
and only alphabetic characters. 

> **What is the probability of the test tweet being an instance of cyberbullying?**
> 
> The hard one. Make the tables:
> The one in bold is what they gave us.

| X         | Curse   | Not Curse |
|-----------|---------|-----------|
| Bully     | **0.8** | 0.2       |
| Not Bully | 0.2     | 0.8       |


| X         | Alphabet | Not Alphabet |
|-----------|----------|--------------|
| Bully     | 0.3      | 0.7          |
| Not Bully | 0.7      | **0.3**      |

If you have time double check the tables. (The ones you made)

**Now the probability of cyber bullying**

- Prior * $\Pi$ p(d|c)
- Prior * p(cursing|bully) * p(alpha|bully)
- 0.2 * 0.8 * 0.3

Consider a test tweet **with a curse** word and **only alphabetic characters**. Would an NBC using these features classify it as cyberbullying?

> **Tip:** Use python in the notebooks to calculate things quick!

- Find probability of test tweet being cyberbullying
    - Prior-bully * $\Pi$ p(d|c)
    - Prior-bully * p(cursing|bully) * p(alpha|bully)
    - 0.2 * 0.8 * 0.3
    - 0.048
- Find probability of test tweet not being cyberbullying
    - Prior-not-bully * $\Pi$ p(d|c)
    - Prior-not-bully * p(cursing|not-bully) * p(alpha|not-bully)
    - 0.8 * 0.2 * 0.7
    - 0.111
- What is the highest one?
  - 0.111 so the class is **not bullying**

Good luck
